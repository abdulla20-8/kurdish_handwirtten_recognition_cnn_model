{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HcT3ANdtXmjk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(2)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSboKOp-VO7n",
        "outputId": "6234083c-5dc5-4531-945c-dce2fc8cf5ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp /content/drive/MyDrive/Abdulla_OCR/Character/anotation_28x28/Test.csv .\n",
        "!cp /content/drive/MyDrive/Abdulla_OCR/Character/anotation_28x28/Train.csv .\n",
        "\n",
        "!rm -rf sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "enMBFPZLVRMM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "\n",
        "train = pd.read_csv(\"Train.csv\")\n",
        "test = pd.read_csv(\"Test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1pmGo6loWRIe",
        "outputId": "f059e691-b083-4886-8de4-11435376893b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     6000\n",
              "26    5999\n",
              "20    5999\n",
              "21    5999\n",
              "22    5999\n",
              "23    5999\n",
              "24    5999\n",
              "25    5999\n",
              "27    5999\n",
              "18    5999\n",
              "28    5999\n",
              "29    5999\n",
              "30    5999\n",
              "31    5999\n",
              "32    5999\n",
              "33    5999\n",
              "19    5999\n",
              "17    5999\n",
              "1     5999\n",
              "8     5999\n",
              "2     5999\n",
              "3     5999\n",
              "4     5999\n",
              "5     5999\n",
              "6     5999\n",
              "7     5999\n",
              "9     5999\n",
              "16    5999\n",
              "10    5999\n",
              "11    5999\n",
              "12    5999\n",
              "13    5999\n",
              "14    5999\n",
              "15    5999\n",
              "34    5999\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "Y_train = train[\"label\"]\n",
        "\n",
        "# Drop 'label' column\n",
        "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
        "\n",
        "# free some space\n",
        "del train \n",
        "\n",
        "g = sns.countplot(Y_train)\n",
        "\n",
        "Y_train.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xj_yM6YWOUM",
        "outputId": "f9fa0236-bec7-4d11-c210-e1e79f58edc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count       784\n",
              "unique        1\n",
              "top       False\n",
              "freq        784\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Check the data\n",
        "X_train.isnull().any().describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sliD4w8nWM0P",
        "outputId": "be005858-88f3-47f2-afa3-8125d5b5dcf6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count       784\n",
              "unique        1\n",
              "top       False\n",
              "freq        784\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "test.isnull().any().describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XUcDWzR8WLM9"
      },
      "outputs": [],
      "source": [
        "# Normalize the data\n",
        "X_train = X_train / 255.0\n",
        "test = test / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CDME31IbWJhk"
      },
      "outputs": [],
      "source": [
        "# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n",
        "X_train = X_train.values.reshape(-1,28,28,1)\n",
        "test = test.values.reshape(-1,28,28,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LQiqgLHoWIoG"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
        "Y_train = to_categorical(Y_train, num_classes = 35)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VRDDgdpQWD8f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the train and the validation set for the fitting\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nGY-GLhYVbYP"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(Y_train, num_classes = 35, dtype = 'float32')\n",
        "y_test = to_categorical(Y_val, num_classes = 35, dtype = 'float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M6qt1zIVfLL",
        "outputId": "a235dc84-20ec-442b-8730-605a7dc093d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(188969, 35, 35)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "b-SXYqjmVgd5"
      },
      "outputs": [],
      "source": [
        "# # # Reshaping the array to include the RGB value. Here RGB value is 1 since it is a b&w image\n",
        "\n",
        "# x_train = X_train.values.reshape(-1,28,28,1)\n",
        "# x_test = X_val.values.reshape(-1,28,28,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "caVVu9RgVh5-"
      },
      "outputs": [],
      "source": [
        "# Converting the arrays to float so that we can get decimal points after division\n",
        "x_train = X_train.astype('float32')\n",
        "x_test = X_val.astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WVRBSoPZV0XP"
      },
      "outputs": [],
      "source": [
        "# Normalizing the RGB codes by dividing it to the maximum RGB value.\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_NjSAOXV18y",
        "outputId": "c32f23b7-7f30-4af5-c187-31fa17d3c3ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (188969, 28, 28, 1)\n",
            "y_train shape: (188969, 35, 35)\n",
            "Number of images in x_train 188969\n",
            "Number of images in x_test 20997\n"
          ]
        }
      ],
      "source": [
        "#Checking the shapes of x_train and y_train \n",
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('Number of images in x_train', x_train.shape[0])\n",
        "print('Number of images in x_test', x_test.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lUGM8CYEV32s"
      },
      "outputs": [],
      "source": [
        "#Initializing the input shape\n",
        "input_shape = (28, 28, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU0hRCWXUWb-",
        "outputId": "18b4c18c-295e-4dcd-ba06-47b4c4d7e200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "5906/5906 [==============================] - 44s 6ms/step - loss: 0.4389 - accuracy: 0.8592\n",
            "Epoch 2/400\n",
            "5906/5906 [==============================] - 32s 6ms/step - loss: 0.2005 - accuracy: 0.9366\n",
            "Epoch 3/400\n",
            "5906/5906 [==============================] - 30s 5ms/step - loss: 0.1561 - accuracy: 0.9509\n",
            "Epoch 4/400\n",
            "5906/5906 [==============================] - 32s 5ms/step - loss: 0.1323 - accuracy: 0.9597\n",
            "Epoch 5/400\n",
            "5906/5906 [==============================] - 29s 5ms/step - loss: 0.1247 - accuracy: 0.9627\n",
            "Epoch 6/400\n",
            "5906/5906 [==============================] - 31s 5ms/step - loss: 0.1122 - accuracy: 0.9672\n",
            "Epoch 7/400\n",
            "5906/5906 [==============================] - 29s 5ms/step - loss: 0.1092 - accuracy: 0.9692\n",
            "Epoch 8/400\n",
            "5906/5906 [==============================] - 29s 5ms/step - loss: 0.1087 - accuracy: 0.9703\n",
            "Epoch 9/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.1068 - accuracy: 0.9727\n",
            "Epoch 10/400\n",
            "5906/5906 [==============================] - 29s 5ms/step - loss: 0.1092 - accuracy: 0.9732\n",
            "Epoch 11/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.1059 - accuracy: 0.9751\n",
            "Epoch 12/400\n",
            "5906/5906 [==============================] - 29s 5ms/step - loss: 0.1116 - accuracy: 0.9748\n",
            "Epoch 13/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.1082 - accuracy: 0.9764\n",
            "Epoch 14/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.1155 - accuracy: 0.9765\n",
            "Epoch 15/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.1144 - accuracy: 0.9782\n",
            "Epoch 16/400\n",
            "5906/5906 [==============================] - 29s 5ms/step - loss: 0.1180 - accuracy: 0.9783\n",
            "Epoch 17/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.1175 - accuracy: 0.9787\n",
            "Epoch 18/400\n",
            "5906/5906 [==============================] - 29s 5ms/step - loss: 0.1216 - accuracy: 0.9797\n",
            "Epoch 19/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.1225 - accuracy: 0.9804\n",
            "Epoch 20/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.1289 - accuracy: 0.9804\n",
            "Epoch 21/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.1384 - accuracy: 0.9806\n",
            "Epoch 22/400\n",
            "5906/5906 [==============================] - 29s 5ms/step - loss: 0.1277 - accuracy: 0.9819\n",
            "Epoch 23/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.1371 - accuracy: 0.9824\n",
            "Epoch 24/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.1456 - accuracy: 0.9826\n",
            "Epoch 25/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.1463 - accuracy: 0.9833\n",
            "Epoch 26/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.1551 - accuracy: 0.9828\n",
            "Epoch 27/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.1537 - accuracy: 0.9835\n",
            "Epoch 28/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.1562 - accuracy: 0.9842\n",
            "Epoch 29/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.1574 - accuracy: 0.9848\n",
            "Epoch 30/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.1569 - accuracy: 0.9850\n",
            "Epoch 31/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.1598 - accuracy: 0.9856\n",
            "Epoch 32/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.1612 - accuracy: 0.9863\n",
            "Epoch 33/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.1824 - accuracy: 0.9859\n",
            "Epoch 34/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.1772 - accuracy: 0.9862\n",
            "Epoch 35/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.1811 - accuracy: 0.9861\n",
            "Epoch 36/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.1778 - accuracy: 0.9870\n",
            "Epoch 37/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.1916 - accuracy: 0.9868\n",
            "Epoch 38/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.1977 - accuracy: 0.9874\n",
            "Epoch 39/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.1917 - accuracy: 0.9878\n",
            "Epoch 40/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.1841 - accuracy: 0.9880\n",
            "Epoch 41/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.1962 - accuracy: 0.9885\n",
            "Epoch 42/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.2045 - accuracy: 0.9885\n",
            "Epoch 43/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.1983 - accuracy: 0.9887\n",
            "Epoch 44/400\n",
            "5906/5906 [==============================] - 27s 4ms/step - loss: 0.2151 - accuracy: 0.9894\n",
            "Epoch 45/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.2129 - accuracy: 0.9893\n",
            "Epoch 46/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.2085 - accuracy: 0.9897\n",
            "Epoch 47/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.2293 - accuracy: 0.9893\n",
            "Epoch 48/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.2387 - accuracy: 0.9896\n",
            "Epoch 49/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.2402 - accuracy: 0.9900\n",
            "Epoch 50/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.2350 - accuracy: 0.9903\n",
            "Epoch 51/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.2491 - accuracy: 0.9901\n",
            "Epoch 52/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.2470 - accuracy: 0.9901\n",
            "Epoch 53/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.2425 - accuracy: 0.9906\n",
            "Epoch 54/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.2469 - accuracy: 0.9908\n",
            "Epoch 55/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.2521 - accuracy: 0.9912\n",
            "Epoch 56/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.2636 - accuracy: 0.9909\n",
            "Epoch 57/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.2632 - accuracy: 0.9910\n",
            "Epoch 58/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.2781 - accuracy: 0.9911\n",
            "Epoch 59/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.2583 - accuracy: 0.9912\n",
            "Epoch 60/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.2648 - accuracy: 0.9917\n",
            "Epoch 61/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.2642 - accuracy: 0.9920\n",
            "Epoch 62/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.2795 - accuracy: 0.9919\n",
            "Epoch 63/400\n",
            "5906/5906 [==============================] - 27s 4ms/step - loss: 0.2794 - accuracy: 0.9918\n",
            "Epoch 64/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.2946 - accuracy: 0.9922\n",
            "Epoch 65/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.2801 - accuracy: 0.9922\n",
            "Epoch 66/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.2761 - accuracy: 0.9927\n",
            "Epoch 67/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.2831 - accuracy: 0.9929\n",
            "Epoch 68/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.3262 - accuracy: 0.9924\n",
            "Epoch 69/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.2964 - accuracy: 0.9927\n",
            "Epoch 70/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.3378 - accuracy: 0.9925\n",
            "Epoch 71/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.3120 - accuracy: 0.9928\n",
            "Epoch 72/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.2972 - accuracy: 0.9930\n",
            "Epoch 73/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.3331 - accuracy: 0.9928\n",
            "Epoch 74/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.3219 - accuracy: 0.9930\n",
            "Epoch 75/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.3286 - accuracy: 0.9932\n",
            "Epoch 76/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.3141 - accuracy: 0.9937\n",
            "Epoch 77/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.3348 - accuracy: 0.9933\n",
            "Epoch 78/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.3396 - accuracy: 0.9935\n",
            "Epoch 79/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.3470 - accuracy: 0.9936\n",
            "Epoch 80/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.3315 - accuracy: 0.9936\n",
            "Epoch 81/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.3589 - accuracy: 0.9936\n",
            "Epoch 82/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.3340 - accuracy: 0.9938\n",
            "Epoch 83/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.3314 - accuracy: 0.9939\n",
            "Epoch 84/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.3603 - accuracy: 0.9939\n",
            "Epoch 85/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.3601 - accuracy: 0.9941\n",
            "Epoch 86/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.3614 - accuracy: 0.9940\n",
            "Epoch 87/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.3583 - accuracy: 0.9943\n",
            "Epoch 88/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.3890 - accuracy: 0.9939\n",
            "Epoch 89/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.3770 - accuracy: 0.9942\n",
            "Epoch 90/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.3797 - accuracy: 0.9944\n",
            "Epoch 91/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.3716 - accuracy: 0.9945\n",
            "Epoch 92/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.4299 - accuracy: 0.9941\n",
            "Epoch 93/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.3811 - accuracy: 0.9947\n",
            "Epoch 94/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.3876 - accuracy: 0.9947\n",
            "Epoch 95/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.4044 - accuracy: 0.9949\n",
            "Epoch 96/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.4062 - accuracy: 0.9944\n",
            "Epoch 97/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.3880 - accuracy: 0.9948\n",
            "Epoch 98/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.3989 - accuracy: 0.9951\n",
            "Epoch 99/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.3920 - accuracy: 0.9949\n",
            "Epoch 100/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.4311 - accuracy: 0.9949\n",
            "Epoch 101/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.4164 - accuracy: 0.9950\n",
            "Epoch 102/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.4069 - accuracy: 0.9951\n",
            "Epoch 103/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.3924 - accuracy: 0.9951\n",
            "Epoch 104/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.3629 - accuracy: 0.9956\n",
            "Epoch 105/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.4383 - accuracy: 0.9951\n",
            "Epoch 106/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.3923 - accuracy: 0.9955\n",
            "Epoch 107/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.4480 - accuracy: 0.9952\n",
            "Epoch 108/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.4138 - accuracy: 0.9956\n",
            "Epoch 109/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.4400 - accuracy: 0.9953\n",
            "Epoch 110/400\n",
            "5906/5906 [==============================] - 35s 6ms/step - loss: 0.4425 - accuracy: 0.9951\n",
            "Epoch 111/400\n",
            "5906/5906 [==============================] - 29s 5ms/step - loss: 0.4290 - accuracy: 0.9953\n",
            "Epoch 112/400\n",
            "5906/5906 [==============================] - 32s 5ms/step - loss: 0.4605 - accuracy: 0.9954\n",
            "Epoch 113/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.4196 - accuracy: 0.9956\n",
            "Epoch 114/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.4300 - accuracy: 0.9958\n",
            "Epoch 115/400\n",
            "5906/5906 [==============================] - 34s 6ms/step - loss: 0.4883 - accuracy: 0.9956\n",
            "Epoch 116/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.4150 - accuracy: 0.9959\n",
            "Epoch 117/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.4838 - accuracy: 0.9957\n",
            "Epoch 118/400\n",
            "5906/5906 [==============================] - 27s 4ms/step - loss: 0.4696 - accuracy: 0.9958\n",
            "Epoch 119/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.4873 - accuracy: 0.9957\n",
            "Epoch 120/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.4501 - accuracy: 0.9958\n",
            "Epoch 121/400\n",
            "5906/5906 [==============================] - 29s 5ms/step - loss: 0.4361 - accuracy: 0.9960\n",
            "Epoch 122/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.4890 - accuracy: 0.9959\n",
            "Epoch 123/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.4798 - accuracy: 0.9960\n",
            "Epoch 124/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.5057 - accuracy: 0.9960\n",
            "Epoch 125/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.4863 - accuracy: 0.9961\n",
            "Epoch 126/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.4921 - accuracy: 0.9961\n",
            "Epoch 127/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.5228 - accuracy: 0.9959\n",
            "Epoch 128/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.4612 - accuracy: 0.9961\n",
            "Epoch 129/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.4994 - accuracy: 0.9961\n",
            "Epoch 130/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.4975 - accuracy: 0.9963\n",
            "Epoch 131/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.5348 - accuracy: 0.9960\n",
            "Epoch 132/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.5432 - accuracy: 0.9962\n",
            "Epoch 133/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.4659 - accuracy: 0.9963\n",
            "Epoch 134/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.5247 - accuracy: 0.9962\n",
            "Epoch 135/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.4942 - accuracy: 0.9963\n",
            "Epoch 136/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.5121 - accuracy: 0.9963\n",
            "Epoch 137/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.5208 - accuracy: 0.9963\n",
            "Epoch 138/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.5585 - accuracy: 0.9963\n",
            "Epoch 139/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.5481 - accuracy: 0.9965\n",
            "Epoch 140/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.5309 - accuracy: 0.9964\n",
            "Epoch 141/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.4757 - accuracy: 0.9965\n",
            "Epoch 142/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.5449 - accuracy: 0.9964\n",
            "Epoch 143/400\n",
            "5906/5906 [==============================] - 25s 4ms/step - loss: 0.5308 - accuracy: 0.9964\n",
            "Epoch 144/400\n",
            "5906/5906 [==============================] - 25s 4ms/step - loss: 0.5420 - accuracy: 0.9964\n",
            "Epoch 145/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.5407 - accuracy: 0.9966\n",
            "Epoch 146/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.5999 - accuracy: 0.9963\n",
            "Epoch 147/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.5308 - accuracy: 0.9967\n",
            "Epoch 148/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.5797 - accuracy: 0.9962\n",
            "Epoch 149/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.5449 - accuracy: 0.9967\n",
            "Epoch 150/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.5739 - accuracy: 0.9965\n",
            "Epoch 151/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.5584 - accuracy: 0.9966\n",
            "Epoch 152/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.5338 - accuracy: 0.9968\n",
            "Epoch 153/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.5989 - accuracy: 0.9966\n",
            "Epoch 154/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.5472 - accuracy: 0.9968\n",
            "Epoch 155/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.6846 - accuracy: 0.9965\n",
            "Epoch 156/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6045 - accuracy: 0.9967\n",
            "Epoch 157/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.5518 - accuracy: 0.9968\n",
            "Epoch 158/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.5305 - accuracy: 0.9970\n",
            "Epoch 159/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.6519 - accuracy: 0.9967\n",
            "Epoch 160/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6081 - accuracy: 0.9968\n",
            "Epoch 161/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.5552 - accuracy: 0.9969\n",
            "Epoch 162/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6037 - accuracy: 0.9969\n",
            "Epoch 163/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.4954 - accuracy: 0.9972\n",
            "Epoch 164/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.5354 - accuracy: 0.9969\n",
            "Epoch 165/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.6069 - accuracy: 0.9968\n",
            "Epoch 166/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6243 - accuracy: 0.9970\n",
            "Epoch 167/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.6203 - accuracy: 0.9970\n",
            "Epoch 168/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6775 - accuracy: 0.9967\n",
            "Epoch 169/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.5890 - accuracy: 0.9971\n",
            "Epoch 170/400\n",
            "5906/5906 [==============================] - 27s 4ms/step - loss: 0.6037 - accuracy: 0.9971\n",
            "Epoch 171/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.5748 - accuracy: 0.9970\n",
            "Epoch 172/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.6548 - accuracy: 0.9970\n",
            "Epoch 173/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.6380 - accuracy: 0.9971\n",
            "Epoch 174/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.5493 - accuracy: 0.9973\n",
            "Epoch 175/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.6306 - accuracy: 0.9972\n",
            "Epoch 176/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6529 - accuracy: 0.9972\n",
            "Epoch 177/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.5583 - accuracy: 0.9974\n",
            "Epoch 178/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.5953 - accuracy: 0.9973\n",
            "Epoch 179/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.6657 - accuracy: 0.9971\n",
            "Epoch 180/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6561 - accuracy: 0.9972\n",
            "Epoch 181/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.6256 - accuracy: 0.9973\n",
            "Epoch 182/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6162 - accuracy: 0.9973\n",
            "Epoch 183/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.6655 - accuracy: 0.9972\n",
            "Epoch 184/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6294 - accuracy: 0.9971\n",
            "Epoch 185/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.5907 - accuracy: 0.9974\n",
            "Epoch 186/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6357 - accuracy: 0.9973\n",
            "Epoch 187/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7101 - accuracy: 0.9972\n",
            "Epoch 188/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.6975 - accuracy: 0.9971\n",
            "Epoch 189/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6722 - accuracy: 0.9972\n",
            "Epoch 190/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.5661 - accuracy: 0.9975\n",
            "Epoch 191/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6195 - accuracy: 0.9973\n",
            "Epoch 192/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.6594 - accuracy: 0.9975\n",
            "Epoch 193/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6373 - accuracy: 0.9974\n",
            "Epoch 194/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7045 - accuracy: 0.9973\n",
            "Epoch 195/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7270 - accuracy: 0.9972\n",
            "Epoch 196/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.6307 - accuracy: 0.9976\n",
            "Epoch 197/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6409 - accuracy: 0.9975\n",
            "Epoch 198/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.6774 - accuracy: 0.9975\n",
            "Epoch 199/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.6550 - accuracy: 0.9974\n",
            "Epoch 200/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.6400 - accuracy: 0.9976\n",
            "Epoch 201/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6776 - accuracy: 0.9975\n",
            "Epoch 202/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7308 - accuracy: 0.9975\n",
            "Epoch 203/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6785 - accuracy: 0.9974\n",
            "Epoch 204/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7172 - accuracy: 0.9976\n",
            "Epoch 205/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7097 - accuracy: 0.9976\n",
            "Epoch 206/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.6549 - accuracy: 0.9977\n",
            "Epoch 207/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6377 - accuracy: 0.9977\n",
            "Epoch 208/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.6882 - accuracy: 0.9975\n",
            "Epoch 209/400\n",
            "5906/5906 [==============================] - 27s 4ms/step - loss: 0.7644 - accuracy: 0.9974\n",
            "Epoch 210/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.5787 - accuracy: 0.9977\n",
            "Epoch 211/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6661 - accuracy: 0.9977\n",
            "Epoch 212/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7114 - accuracy: 0.9976\n",
            "Epoch 213/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7491 - accuracy: 0.9976\n",
            "Epoch 214/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.6741 - accuracy: 0.9977\n",
            "Epoch 215/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6501 - accuracy: 0.9978\n",
            "Epoch 216/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7017 - accuracy: 0.9978\n",
            "Epoch 217/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6855 - accuracy: 0.9977\n",
            "Epoch 218/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7488 - accuracy: 0.9977\n",
            "Epoch 219/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7793 - accuracy: 0.9977\n",
            "Epoch 220/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7021 - accuracy: 0.9977\n",
            "Epoch 221/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.6286 - accuracy: 0.9979\n",
            "Epoch 222/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8086 - accuracy: 0.9977\n",
            "Epoch 223/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7714 - accuracy: 0.9976\n",
            "Epoch 224/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7563 - accuracy: 0.9978\n",
            "Epoch 225/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7786 - accuracy: 0.9977\n",
            "Epoch 226/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7628 - accuracy: 0.9979\n",
            "Epoch 227/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8316 - accuracy: 0.9976\n",
            "Epoch 228/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8132 - accuracy: 0.9978\n",
            "Epoch 229/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8424 - accuracy: 0.9977\n",
            "Epoch 230/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.6966 - accuracy: 0.9980\n",
            "Epoch 231/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7659 - accuracy: 0.9978\n",
            "Epoch 232/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8062 - accuracy: 0.9977\n",
            "Epoch 233/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8749 - accuracy: 0.9977\n",
            "Epoch 234/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7318 - accuracy: 0.9979\n",
            "Epoch 235/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7140 - accuracy: 0.9980\n",
            "Epoch 236/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7058 - accuracy: 0.9980\n",
            "Epoch 237/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7803 - accuracy: 0.9978\n",
            "Epoch 238/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7621 - accuracy: 0.9979\n",
            "Epoch 239/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7284 - accuracy: 0.9980\n",
            "Epoch 240/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7010 - accuracy: 0.9981\n",
            "Epoch 241/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8554 - accuracy: 0.9978\n",
            "Epoch 242/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7055 - accuracy: 0.9981\n",
            "Epoch 243/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7344 - accuracy: 0.9980\n",
            "Epoch 244/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7578 - accuracy: 0.9979\n",
            "Epoch 245/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7337 - accuracy: 0.9979\n",
            "Epoch 246/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7020 - accuracy: 0.9981\n",
            "Epoch 247/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8011 - accuracy: 0.9980\n",
            "Epoch 248/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7099 - accuracy: 0.9981\n",
            "Epoch 249/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8193 - accuracy: 0.9980\n",
            "Epoch 250/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7196 - accuracy: 0.9980\n",
            "Epoch 251/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7437 - accuracy: 0.9979\n",
            "Epoch 252/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7618 - accuracy: 0.9982\n",
            "Epoch 253/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.7430 - accuracy: 0.9981\n",
            "Epoch 254/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7841 - accuracy: 0.9980\n",
            "Epoch 255/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7879 - accuracy: 0.9980\n",
            "Epoch 256/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7142 - accuracy: 0.9982\n",
            "Epoch 257/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7717 - accuracy: 0.9980\n",
            "Epoch 258/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8447 - accuracy: 0.9980\n",
            "Epoch 259/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8683 - accuracy: 0.9979\n",
            "Epoch 260/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8303 - accuracy: 0.9980\n",
            "Epoch 261/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8060 - accuracy: 0.9983\n",
            "Epoch 262/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8087 - accuracy: 0.9982\n",
            "Epoch 263/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8192 - accuracy: 0.9982\n",
            "Epoch 264/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7036 - accuracy: 0.9982\n",
            "Epoch 265/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.8531 - accuracy: 0.9980\n",
            "Epoch 266/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9475 - accuracy: 0.9980\n",
            "Epoch 267/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7540 - accuracy: 0.9983\n",
            "Epoch 268/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8132 - accuracy: 0.9981\n",
            "Epoch 269/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8589 - accuracy: 0.9980\n",
            "Epoch 270/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8037 - accuracy: 0.9981\n",
            "Epoch 271/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9460 - accuracy: 0.9981\n",
            "Epoch 272/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7000 - accuracy: 0.9984\n",
            "Epoch 273/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8232 - accuracy: 0.9982\n",
            "Epoch 274/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7225 - accuracy: 0.9982\n",
            "Epoch 275/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7103 - accuracy: 0.9982\n",
            "Epoch 276/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.9499 - accuracy: 0.9982\n",
            "Epoch 277/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.8140 - accuracy: 0.9982\n",
            "Epoch 278/400\n",
            "5906/5906 [==============================] - 27s 4ms/step - loss: 0.8505 - accuracy: 0.9983\n",
            "Epoch 279/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9732 - accuracy: 0.9982\n",
            "Epoch 280/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7401 - accuracy: 0.9982\n",
            "Epoch 281/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9455 - accuracy: 0.9981\n",
            "Epoch 282/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8108 - accuracy: 0.9983\n",
            "Epoch 283/400\n",
            "5906/5906 [==============================] - 27s 4ms/step - loss: 0.7977 - accuracy: 0.9983\n",
            "Epoch 284/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7990 - accuracy: 0.9984\n",
            "Epoch 285/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7951 - accuracy: 0.9984\n",
            "Epoch 286/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7926 - accuracy: 0.9983\n",
            "Epoch 287/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.9597 - accuracy: 0.9983\n",
            "Epoch 288/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7607 - accuracy: 0.9983\n",
            "Epoch 289/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.7410 - accuracy: 0.9984\n",
            "Epoch 290/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9136 - accuracy: 0.9983\n",
            "Epoch 291/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8306 - accuracy: 0.9983\n",
            "Epoch 292/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8322 - accuracy: 0.9983\n",
            "Epoch 293/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8254 - accuracy: 0.9985\n",
            "Epoch 294/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8910 - accuracy: 0.9983\n",
            "Epoch 295/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7946 - accuracy: 0.9984\n",
            "Epoch 296/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8593 - accuracy: 0.9984\n",
            "Epoch 297/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 1.0513 - accuracy: 0.9982\n",
            "Epoch 298/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9605 - accuracy: 0.9983\n",
            "Epoch 299/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8443 - accuracy: 0.9983\n",
            "Epoch 300/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8759 - accuracy: 0.9983\n",
            "Epoch 301/400\n",
            "5906/5906 [==============================] - 27s 4ms/step - loss: 0.8899 - accuracy: 0.9982\n",
            "Epoch 302/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.8965 - accuracy: 0.9984\n",
            "Epoch 303/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8288 - accuracy: 0.9983\n",
            "Epoch 304/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8189 - accuracy: 0.9985\n",
            "Epoch 305/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.9134 - accuracy: 0.9983\n",
            "Epoch 306/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.9690 - accuracy: 0.9984\n",
            "Epoch 307/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8391 - accuracy: 0.9983\n",
            "Epoch 308/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.8595 - accuracy: 0.9985\n",
            "Epoch 309/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8840 - accuracy: 0.9985\n",
            "Epoch 310/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.0361 - accuracy: 0.9984\n",
            "Epoch 311/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.9205 - accuracy: 0.9983\n",
            "Epoch 312/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.8713 - accuracy: 0.9984\n",
            "Epoch 313/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8812 - accuracy: 0.9986\n",
            "Epoch 314/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8310 - accuracy: 0.9985\n",
            "Epoch 315/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8783 - accuracy: 0.9984\n",
            "Epoch 316/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.8666 - accuracy: 0.9985\n",
            "Epoch 317/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9083 - accuracy: 0.9984\n",
            "Epoch 318/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9653 - accuracy: 0.9985\n",
            "Epoch 319/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9780 - accuracy: 0.9985\n",
            "Epoch 320/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.0442 - accuracy: 0.9983\n",
            "Epoch 321/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8642 - accuracy: 0.9985\n",
            "Epoch 322/400\n",
            "5906/5906 [==============================] - 27s 4ms/step - loss: 0.8727 - accuracy: 0.9985\n",
            "Epoch 323/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.0721 - accuracy: 0.9983\n",
            "Epoch 324/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9387 - accuracy: 0.9984\n",
            "Epoch 325/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9170 - accuracy: 0.9985\n",
            "Epoch 326/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8774 - accuracy: 0.9986\n",
            "Epoch 327/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.0317 - accuracy: 0.9984\n",
            "Epoch 328/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.9370 - accuracy: 0.9985\n",
            "Epoch 329/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.8278 - accuracy: 0.9986\n",
            "Epoch 330/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.7965 - accuracy: 0.9986\n",
            "Epoch 331/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.1063 - accuracy: 0.9983\n",
            "Epoch 332/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8803 - accuracy: 0.9985\n",
            "Epoch 333/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8380 - accuracy: 0.9985\n",
            "Epoch 334/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.9572 - accuracy: 0.9985\n",
            "Epoch 335/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.9033 - accuracy: 0.9987\n",
            "Epoch 336/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.8541 - accuracy: 0.9987\n",
            "Epoch 337/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.0336 - accuracy: 0.9985\n",
            "Epoch 338/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.8491 - accuracy: 0.9987\n",
            "Epoch 339/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.0039 - accuracy: 0.9985\n",
            "Epoch 340/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 1.0534 - accuracy: 0.9984\n",
            "Epoch 341/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9727 - accuracy: 0.9986\n",
            "Epoch 342/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.9286 - accuracy: 0.9984\n",
            "Epoch 343/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9729 - accuracy: 0.9984\n",
            "Epoch 344/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.9795 - accuracy: 0.9986\n",
            "Epoch 345/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9041 - accuracy: 0.9987\n",
            "Epoch 346/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 1.1169 - accuracy: 0.9984\n",
            "Epoch 347/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.9542 - accuracy: 0.9986\n",
            "Epoch 348/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 1.0861 - accuracy: 0.9985\n",
            "Epoch 349/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.0165 - accuracy: 0.9985\n",
            "Epoch 350/400\n",
            "5906/5906 [==============================] - 26s 4ms/step - loss: 0.9498 - accuracy: 0.9987\n",
            "Epoch 351/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.0555 - accuracy: 0.9986\n",
            "Epoch 352/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.0327 - accuracy: 0.9985\n",
            "Epoch 353/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8836 - accuracy: 0.9987\n",
            "Epoch 354/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 1.1892 - accuracy: 0.9986\n",
            "Epoch 355/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.0046 - accuracy: 0.9986\n",
            "Epoch 356/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.9858 - accuracy: 0.9986\n",
            "Epoch 357/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9962 - accuracy: 0.9987\n",
            "Epoch 358/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 1.1309 - accuracy: 0.9985\n",
            "Epoch 359/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9977 - accuracy: 0.9986\n",
            "Epoch 360/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.9150 - accuracy: 0.9986\n",
            "Epoch 361/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9888 - accuracy: 0.9986\n",
            "Epoch 362/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 1.0851 - accuracy: 0.9986\n",
            "Epoch 363/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9933 - accuracy: 0.9986\n",
            "Epoch 364/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 1.0296 - accuracy: 0.9986\n",
            "Epoch 365/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9385 - accuracy: 0.9987\n",
            "Epoch 366/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 1.1863 - accuracy: 0.9984\n",
            "Epoch 367/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.1290 - accuracy: 0.9986\n",
            "Epoch 368/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.8780 - accuracy: 0.9986\n",
            "Epoch 369/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 1.0760 - accuracy: 0.9986\n",
            "Epoch 370/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 1.1714 - accuracy: 0.9985\n",
            "Epoch 371/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.0177 - accuracy: 0.9988\n",
            "Epoch 372/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 1.0056 - accuracy: 0.9987\n",
            "Epoch 373/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.2047 - accuracy: 0.9985\n",
            "Epoch 374/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.8815 - accuracy: 0.9987\n",
            "Epoch 375/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9417 - accuracy: 0.9987\n",
            "Epoch 376/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.9880 - accuracy: 0.9988\n",
            "Epoch 377/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9449 - accuracy: 0.9988\n",
            "Epoch 378/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.9461 - accuracy: 0.9988\n",
            "Epoch 379/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.0512 - accuracy: 0.9987\n",
            "Epoch 380/400\n",
            "5906/5906 [==============================] - 29s 5ms/step - loss: 0.8700 - accuracy: 0.9988\n",
            "Epoch 381/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9446 - accuracy: 0.9987\n",
            "Epoch 382/400\n",
            "5906/5906 [==============================] - 29s 5ms/step - loss: 1.1290 - accuracy: 0.9987\n",
            "Epoch 383/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.2989 - accuracy: 0.9985\n",
            "Epoch 384/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 1.1354 - accuracy: 0.9987\n",
            "Epoch 385/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.0696 - accuracy: 0.9987\n",
            "Epoch 386/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 1.1217 - accuracy: 0.9987\n",
            "Epoch 387/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.0120 - accuracy: 0.9987\n",
            "Epoch 388/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 1.0390 - accuracy: 0.9988\n",
            "Epoch 389/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.7998 - accuracy: 0.9988\n",
            "Epoch 390/400\n",
            "5906/5906 [==============================] - 29s 5ms/step - loss: 1.1874 - accuracy: 0.9987\n",
            "Epoch 391/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.9083 - accuracy: 0.9988\n",
            "Epoch 392/400\n",
            "5906/5906 [==============================] - 29s 5ms/step - loss: 0.9890 - accuracy: 0.9987\n",
            "Epoch 393/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.9286 - accuracy: 0.9988\n",
            "Epoch 394/400\n",
            "5906/5906 [==============================] - 29s 5ms/step - loss: 1.0998 - accuracy: 0.9988\n",
            "Epoch 395/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 1.1003 - accuracy: 0.9988\n",
            "Epoch 396/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 1.1214 - accuracy: 0.9986\n",
            "Epoch 397/400\n",
            "5906/5906 [==============================] - 27s 5ms/step - loss: 0.8061 - accuracy: 0.9989\n",
            "Epoch 398/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.8706 - accuracy: 0.9989\n",
            "Epoch 399/400\n",
            "5906/5906 [==============================] - 28s 5ms/step - loss: 0.9597 - accuracy: 0.9987\n",
            "Epoch 400/400\n",
            "5906/5906 [==============================] - 29s 5ms/step - loss: 0.9692 - accuracy: 0.9987\n",
            "657/657 [==============================] - 3s 3ms/step - loss: 81.8168 - accuracy: 0.9681\n",
            "5906/5906 [==============================] - 21s 4ms/step - loss: 0.0579 - accuracy: 0.9999\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05789969116449356, 0.9998518228530884]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# load the data\n",
        "X_train, X_val, Y_train, Y_val\n",
        "\n",
        "# (train_img,train_label),(test_img,test_label) = keras.datasets.mnist.load_data()\n",
        "# train_img = train_img.reshape([-1, 28, 28, 1])\n",
        "# test_img = test_img.reshape([-1, 28, 28, 1])\n",
        "# train_img = train_img/255.0\n",
        "# test_img = test_img/255.0\n",
        "# train_label = keras.utils.to_categorical(train_label)\n",
        "# test_label = keras.utils.to_categorical(test_label)\n",
        "\n",
        "# define the model architecture\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (5, 5), padding=\"same\", input_shape=[28, 28, 1]),\n",
        "    keras.layers.MaxPool2D((2,2)),\n",
        "    keras.layers.Conv2D(64, (5, 5), padding=\"same\"),\n",
        "    keras.layers.MaxPool2D((2,2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(1024, activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(35, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "model.fit(x=X_train,y=Y_train, epochs=400)\n",
        "model.evaluate(X_val, Y_val)\n",
        "model.evaluate(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkc451Cag84U",
        "outputId": "b5243505-5776-4f6d-8c9b-9274531694a8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 32)        832       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 14, 64)        51264     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              3212288   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 35)                35875     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,300,259\n",
            "Trainable params: 3,300,259\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_val, Y_val)\n",
        "model.evaluate(X_train, Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKv4P_lEhgT8",
        "outputId": "158696ca-c032-4fc2-88cf-c839d7972e1c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "657/657 [==============================] - 2s 3ms/step - loss: 81.8168 - accuracy: 0.9681\n",
            "5906/5906 [==============================] - 19s 3ms/step - loss: 0.0579 - accuracy: 0.9999\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05789969116449356, 0.9998518228530884]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lDmeKhy1UzMQ"
      },
      "outputs": [],
      "source": [
        "model.save(\"model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.1082 - accuracy: 0.9764"
      ],
      "metadata": {
        "id": "eq186R_Wg91H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}