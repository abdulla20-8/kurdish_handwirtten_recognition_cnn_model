{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSVP1wbtMpn8"
   },
   "source": [
    "# Biggest question?-- Why CNN?\n",
    "\n",
    "The MNIST problem, which is an image based problem. I am able to achieve an accuracy of 96.5% using MLP model. So, I Generalized that MLP can be used for any image based problem and solve it in similar way. So, I had a question---Why CNN?\n",
    "\n",
    "I read a wonderful blog on CNN by [Adit Deshpande](https://adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/). I would re-iterate the same understanding here. \n",
    "\n",
    "**What's the actual problem?**\n",
    "\n",
    "Image classification is the task of taking an input image and outputting a class (a cat, dog, etc) or a probability of classes that best describes the image. For humans, this task of recognition is one of the first skills we learn from the moment we are born and is one that comes naturally and effortlessly as adults. Without even thinking twice, we are able to quickly and seamlessly identify the environment we are in as well as the objects that surround us. When we see an image or just when we look at the world around us, most of the time we are able to immediately characterize the scene and give each object a label, all without even consciously noticing. The skills of being able to quickly recognize patterns, generalize from prior knowledge and adapt to different image environments. Machines don't have this skill set.\n",
    "\n",
    "**Inputs and Outputs**\n",
    "\n",
    "When a computer sees an image (takes an image as input), it will see an array of pixel values, depending on the resolution and size of the image. Assume, if the image size is a 32 x 32 x 3 array of numbers (The 3 refers to RGB values). Assume, we have a color image in JPG form and its size is 480 x 480. The representative array will be 480 x 480 x 3 (within the computer, as 3 represents the RGB channel). Each of these numbers will be of a value between 0 to 255 which describes the pixel intensity at that point. Even, though these numbers seem meaningless to us. But, the computer can only understand the language of numbers and in fact, this array representation will only be the inputs available to the computer. The idea is that when this input array of numbers is given to the computer, after applying the CNN algorithm, the output will be numbers that describe the probability of the image being a certain class (0.80 for cat, 0.15 for dog, 0.05 for bird, etc).\n",
    "What is expected from the Computer?\n",
    "So, after understanding the problem and how the input and output would be, the next step is to understand How to approach the problem. We expect the computer to differentiate between all the images it is given and then figure out the unique features that make a dog or that make a cat (means we are expecting the computer or our model to find the patterns, which will recognize the dog as a dog and the cat as a cat), here the pattern finding means to find the best weights to our model. Even in our mind subconsciously, we would do the same process we process. When we look at a picture of a dog with our eyes, we can classify it as a dog if the picture has identifiable features such as paws and its facial pattern and other dog-related features. In a similar way, the computer is able to perform the task of image classification in a step-by-step process by looking for low-level features such as edges and curves initially and then building more abstract concepts through a series of convolutional layers.\n",
    "\n",
    "**Biological Connection**\n",
    "\n",
    "Before stepping into further details, let's understand biological inspiration from where CNN has been designed. CNNs do take biological inspiration from the visual cortex. The visual cortex has small regions of cells that are sensitive to specific regions of the visual field. This idea was expanded upon by a fascinating experiment by Hubel and Wiesel in 1962 (Video) where they showed that some individual neuron cells in the brain responded (or fired) only in the presence of edges of a certain orientation. For example, some neurons fired when exposed to vertical edges and some when shown horizontal or diagonal edges. Hubel and Wiesel found out that all of these neurons were organized in a columnar architecture and that together, were able to produce visual perception. \n",
    "This idea of specialized components inside of a system having specific tasks (the neuron cells in the visual cortex looking for specific characteristics) is where models are designed with multiple layers with different layers doing different tasks (1st layer doing edge detection, 2nd layer doing slightly more than edge detection, etc..). This article explains in-depth biological inspiration.\n",
    "\n",
    "**Structure**\n",
    "\n",
    "A more detailed overview of what CNNs do would be that - the image is passed as an input, then- pass it through a series of convolutional layers along with non-linear activation functions, then- pooling (downsampling), then - fully connected layers, then - flatten layer, and then - get an output. The output can be a single class or a probability of classes that best describes the image. Now, the further analysis would be on understanding each of these layers.\n",
    "\n",
    "\n",
    "![image.png](attachment:4dd72296-a126-4964-905c-315ad83d188f.png)\n",
    "\n",
    "\n",
    "After learning about CNN, I got to understand that there is no hard and fast rule that only CNNs to be used for image classification.Both MLP and CNN can be used for Image classification however MLP takes vector as input and CNN takes tensor as input so CNN can understand spatial relation(relation between nearby pixels of image)between pixels of images better thus for complicated images CNN will perform better than MLP.CNN is designed to work for Images or Videos classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTS8ZNYwMpoF"
   },
   "source": [
    "# About the MNIST Dataset\n",
    "\n",
    "The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. This dataset is considered to be the \"hello world\" dataset for Computer Vision.\n",
    "\n",
    "But, I am using it for MLP to experiment all the state of the art concepts (batch normalization and dropouts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfv55KRVMpoF"
   },
   "source": [
    "# Import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:15:18.766366Z",
     "iopub.status.busy": "2023-04-02T05:15:18.765913Z",
     "iopub.status.idle": "2023-04-02T05:15:46.488619Z",
     "shell.execute_reply": "2023-04-02T05:15:46.486985Z",
     "shell.execute_reply.started": "2023-04-02T05:15:18.766331Z"
    },
    "id": "Ton9LTPuMpoF",
    "outputId": "dd22866e-84f9-4501-dfa5-42058c5baec6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting keras_sequential_ascii\n",
      "  Downloading keras_sequential_ascii-0.1.1.tar.gz (3.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (from keras_sequential_ascii) (2.12.0)\n",
      "Building wheels for collected packages: keras_sequential_ascii\n",
      "  Building wheel for keras_sequential_ascii (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras_sequential_ascii: filename=keras_sequential_ascii-0.1.1-py3-none-any.whl size=3060 sha256=c16a7606ab5a4a5842aa16fa4462775c7dc486be4eb283927365c5798d74efac\n",
      "  Stored in directory: /root/.cache/pip/wheels/67/8d/db/f44305967500e96c1c57b4e42151af8d0b81d5d60d35220a7c\n",
      "Successfully built keras_sequential_ascii\n",
      "Installing collected packages: keras_sequential_ascii\n",
      "Successfully installed keras_sequential_ascii-0.1.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting visualkeras\n",
      "  Downloading visualkeras-0.0.2-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from visualkeras) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.9/dist-packages (from visualkeras) (1.22.4)\n",
      "Collecting aggdraw>=1.3.11\n",
      "  Downloading aggdraw-1.3.16-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (992 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m992.0/992.0 KB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: aggdraw, visualkeras\n",
      "Successfully installed aggdraw-1.3.16 visualkeras-0.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install keras_sequential_ascii\n",
    "!pip install visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T05:15:46.4919Z",
     "iopub.status.busy": "2023-04-02T05:15:46.491308Z",
     "iopub.status.idle": "2023-04-02T05:15:57.972196Z",
     "shell.execute_reply": "2023-04-02T05:15:57.970968Z",
     "shell.execute_reply.started": "2023-04-02T05:15:46.491856Z"
    },
    "id": "F07tLymhMpoG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import all the keras related libraries\n",
    "\n",
    "from keras.models import Sequential  \n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import BatchNormalization, Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "#model analyzing libraries\n",
    "from keras_sequential_ascii import keras2ascii\n",
    "import visualkeras\n",
    "\n",
    "#import plotting libraries \n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly as py\n",
    "\n",
    "# Ignore warnings while installing as well as in the below cells\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#to pick random seed and random points from dataset\n",
    "import random                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDv3b0iYMpoG"
   },
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6JZx9ZkdMyhk",
    "outputId": "3163c5e9-ddc6-4c34-f61e-c1a39dcece63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!cp /content/drive/MyDrive/Abdulla_OCR/Character/anotation_28x28/Test.csv .\n",
    "!cp /content/drive/MyDrive/Abdulla_OCR/Character/anotation_28x28/Train.csv .\n",
    "\n",
    "!rm -rf sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:15:57.974659Z",
     "iopub.status.busy": "2023-04-02T05:15:57.974189Z",
     "iopub.status.idle": "2023-04-02T05:16:04.62533Z",
     "shell.execute_reply": "2023-04-02T05:16:04.624025Z",
     "shell.execute_reply.started": "2023-04-02T05:15:57.974608Z"
    },
    "id": "jiajD_CZMpoG",
    "outputId": "9f4f9e5b-b5c3-4ead-d111-36d97cfcf9a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of TRAIN DATA is (209966, 785)\n",
      "The shape of TEST DATA is (34966, 784)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('Train.csv')\n",
    "test_data = pd.read_csv(\"Test.csv\")\n",
    "\n",
    "print(f\"The shape of TRAIN DATA is {train_data.shape}\\nThe shape of TEST DATA is {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ylCaLZDMpoG"
   },
   "source": [
    "In TRAIN DATA, there is one column additional to the TEST DATA, that's because of the target variable/feature for the TRAIN DATA. The best part is, MNIST data is actually an image dataset, which is of 28x28 image. But, it has been flattened to 784 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRdus2qAMpoH"
   },
   "source": [
    "# Distrubution of target variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:16:04.628528Z",
     "iopub.status.busy": "2023-04-02T05:16:04.628143Z",
     "iopub.status.idle": "2023-04-02T05:16:06.339013Z",
     "shell.execute_reply": "2023-04-02T05:16:06.337539Z",
     "shell.execute_reply.started": "2023-04-02T05:16:04.628493Z"
    },
    "id": "yI5VrywTMpoH",
    "outputId": "57f2de87-d341-40b4-d38f-f5cb699e1cb9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"84c210e9-5caa-4530-9c18-84923f0a373c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"84c210e9-5caa-4530-9c18-84923f0a373c\")) {                    Plotly.newPlot(                        \"84c210e9-5caa-4530-9c18-84923f0a373c\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"0\",\"offsetgroup\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"0\"],\"xaxis\":\"x\",\"y\":[6000],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"26\",\"marker\":{\"color\":\"#EF553B\",\"pattern\":{\"shape\":\"\"}},\"name\":\"26\",\"offsetgroup\":\"26\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"26\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"20\",\"marker\":{\"color\":\"#00cc96\",\"pattern\":{\"shape\":\"\"}},\"name\":\"20\",\"offsetgroup\":\"20\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"20\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"21\",\"marker\":{\"color\":\"#ab63fa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"21\",\"offsetgroup\":\"21\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"21\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"22\",\"marker\":{\"color\":\"#FFA15A\",\"pattern\":{\"shape\":\"\"}},\"name\":\"22\",\"offsetgroup\":\"22\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"22\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"23\",\"marker\":{\"color\":\"#19d3f3\",\"pattern\":{\"shape\":\"\"}},\"name\":\"23\",\"offsetgroup\":\"23\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"23\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"24\",\"marker\":{\"color\":\"#FF6692\",\"pattern\":{\"shape\":\"\"}},\"name\":\"24\",\"offsetgroup\":\"24\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"24\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"25\",\"marker\":{\"color\":\"#B6E880\",\"pattern\":{\"shape\":\"\"}},\"name\":\"25\",\"offsetgroup\":\"25\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"25\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"27\",\"marker\":{\"color\":\"#FF97FF\",\"pattern\":{\"shape\":\"\"}},\"name\":\"27\",\"offsetgroup\":\"27\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"27\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"18\",\"marker\":{\"color\":\"#FECB52\",\"pattern\":{\"shape\":\"\"}},\"name\":\"18\",\"offsetgroup\":\"18\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"18\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"28\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"28\",\"offsetgroup\":\"28\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"28\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"29\",\"marker\":{\"color\":\"#EF553B\",\"pattern\":{\"shape\":\"\"}},\"name\":\"29\",\"offsetgroup\":\"29\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"29\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"30\",\"marker\":{\"color\":\"#00cc96\",\"pattern\":{\"shape\":\"\"}},\"name\":\"30\",\"offsetgroup\":\"30\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"30\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"31\",\"marker\":{\"color\":\"#ab63fa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"31\",\"offsetgroup\":\"31\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"31\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"32\",\"marker\":{\"color\":\"#FFA15A\",\"pattern\":{\"shape\":\"\"}},\"name\":\"32\",\"offsetgroup\":\"32\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"32\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"33\",\"marker\":{\"color\":\"#19d3f3\",\"pattern\":{\"shape\":\"\"}},\"name\":\"33\",\"offsetgroup\":\"33\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"33\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"19\",\"marker\":{\"color\":\"#FF6692\",\"pattern\":{\"shape\":\"\"}},\"name\":\"19\",\"offsetgroup\":\"19\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"19\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"17\",\"marker\":{\"color\":\"#B6E880\",\"pattern\":{\"shape\":\"\"}},\"name\":\"17\",\"offsetgroup\":\"17\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"17\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"1\",\"marker\":{\"color\":\"#FF97FF\",\"pattern\":{\"shape\":\"\"}},\"name\":\"1\",\"offsetgroup\":\"1\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"1\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"8\",\"marker\":{\"color\":\"#FECB52\",\"pattern\":{\"shape\":\"\"}},\"name\":\"8\",\"offsetgroup\":\"8\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"8\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"2\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"2\",\"offsetgroup\":\"2\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"2\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"3\",\"marker\":{\"color\":\"#EF553B\",\"pattern\":{\"shape\":\"\"}},\"name\":\"3\",\"offsetgroup\":\"3\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"3\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"4\",\"marker\":{\"color\":\"#00cc96\",\"pattern\":{\"shape\":\"\"}},\"name\":\"4\",\"offsetgroup\":\"4\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"4\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"5\",\"marker\":{\"color\":\"#ab63fa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"5\",\"offsetgroup\":\"5\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"5\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"6\",\"marker\":{\"color\":\"#FFA15A\",\"pattern\":{\"shape\":\"\"}},\"name\":\"6\",\"offsetgroup\":\"6\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"6\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"7\",\"marker\":{\"color\":\"#19d3f3\",\"pattern\":{\"shape\":\"\"}},\"name\":\"7\",\"offsetgroup\":\"7\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"7\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"9\",\"marker\":{\"color\":\"#FF6692\",\"pattern\":{\"shape\":\"\"}},\"name\":\"9\",\"offsetgroup\":\"9\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"9\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"16\",\"marker\":{\"color\":\"#B6E880\",\"pattern\":{\"shape\":\"\"}},\"name\":\"16\",\"offsetgroup\":\"16\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"16\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"10\",\"marker\":{\"color\":\"#FF97FF\",\"pattern\":{\"shape\":\"\"}},\"name\":\"10\",\"offsetgroup\":\"10\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"10\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"11\",\"marker\":{\"color\":\"#FECB52\",\"pattern\":{\"shape\":\"\"}},\"name\":\"11\",\"offsetgroup\":\"11\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"11\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"12\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"12\",\"offsetgroup\":\"12\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"12\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"13\",\"marker\":{\"color\":\"#EF553B\",\"pattern\":{\"shape\":\"\"}},\"name\":\"13\",\"offsetgroup\":\"13\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"13\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"14\",\"marker\":{\"color\":\"#00cc96\",\"pattern\":{\"shape\":\"\"}},\"name\":\"14\",\"offsetgroup\":\"14\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"14\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"15\",\"marker\":{\"color\":\"#ab63fa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"15\",\"offsetgroup\":\"15\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"15\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class label=%{x}<br>Count=%{y}<extra></extra>\",\"legendgroup\":\"34\",\"marker\":{\"color\":\"#FFA15A\",\"pattern\":{\"shape\":\"\"}},\"name\":\"34\",\"offsetgroup\":\"34\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"34\"],\"xaxis\":\"x\",\"y\":[5999],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Class label\"},\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"26\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"27\",\"18\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"19\",\"17\",\"1\",\"8\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"9\",\"16\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"34\"]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Count\"}},\"legend\":{\"title\":{\"text\":\"Class label\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('84c210e9-5caa-4530-9c18-84923f0a373c');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_label = list(train_data['label'].value_counts().index)\n",
    "class_label = [str(each_label) for each_label in class_label]\n",
    "count = list(train_data['label'].value_counts().values)\n",
    "\n",
    "target_var_dist = pd.DataFrame({\"Class label\":class_label,\"Count\":count})\n",
    "px.bar(target_var_dist,target_var_dist['Class label'],target_var_dist['Count'],color=target_var_dist['Class label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkmpOTRSMpoH"
   },
   "source": [
    "It's very rare to find such kind of distribution in real world (where the data is close to equal/uniformly distributed). If the data is uniformly distributed between all the classes(target variable) then the chance of biasedness reduces a lot---which is one of the ideal condition in building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWTQUV19MpoH"
   },
   "source": [
    "Splitting and Arranging the Data accordingly as per to fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:16:06.342163Z",
     "iopub.status.busy": "2023-04-02T05:16:06.341262Z",
     "iopub.status.idle": "2023-04-02T05:16:06.527595Z",
     "shell.execute_reply": "2023-04-02T05:16:06.525921Z",
     "shell.execute_reply.started": "2023-04-02T05:16:06.342108Z"
    },
    "id": "3EHaoAXAMpoH",
    "outputId": "a1e9fa7b-cc06-4c97-b435-54a458789088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of TARGET VARIABLE is (209966,)\n",
      "The shape of ACTUAL TRAIN DATA is (209966, 784)\n"
     ]
    }
   ],
   "source": [
    "target_variable   = train_data['label']\n",
    "actual_train_data = train_data.drop(columns='label')\n",
    "\n",
    "print(f\"The shape of TARGET VARIABLE is {target_variable.shape}\\nThe shape of ACTUAL TRAIN DATA is {actual_train_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQ_ZUjFrMpoH"
   },
   "source": [
    "Splitting the TRAIN DATA further into validation(X_val) and train(X_train) | and their respective class labels too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:16:06.531299Z",
     "iopub.status.busy": "2023-04-02T05:16:06.529857Z",
     "iopub.status.idle": "2023-04-02T05:16:06.907119Z",
     "shell.execute_reply": "2023-04-02T05:16:06.905577Z",
     "shell.execute_reply.started": "2023-04-02T05:16:06.531259Z"
    },
    "id": "WdoN5mX2MpoI",
    "outputId": "b972d079-d0e3-4cf6-eec4-31c87c09f7cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The shape of X_train variable is (167972, 784) \n",
      " The shape of y_train variable is (167972,) \n",
      " The shape of X_val variable is (41994, 784) \n",
      " The shape of y_val variable is (41994,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(actual_train_data, target_variable, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\" The shape of X_train variable is {X_train.shape} \\n The shape of y_train variable is {y_train.shape} \\n The shape of X_val variable is {X_val.shape} \\n The shape of y_val variable is {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a37AaUnwMpoI"
   },
   "source": [
    "Visualizing the images in-detail and pre-processing them is explained in detail in the below mentioned notebook. Please refer there. \n",
    "\n",
    "https://www.kaggle.com/code/mvschamanth/mlp-model-in-keras-with-high-accuracy-of-97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gy1VyWAMMpoI"
   },
   "source": [
    "## Pre-processing the data\n",
    "\n",
    "It is also called as Scaling (or) Normalizing the data\n",
    " \n",
    "The pixel values range from 0 to 255: the background majority close to 0, and those close to 255 representing the digit.\n",
    "\n",
    "Although raw pixel values can be used directly, it is better to normalize the input data as to avoid large gradient values that could make training difficult.\n",
    "\n",
    "As,stochastic gradient descent is being used to train the model,the chance of getting stuck in local optima will not be an issue when SCALING is being done and it also speeds up the training.\n",
    "\n",
    "Neural network activations need the inputs to be scaled because scaling can also helps in preventing vanishing and exploding gradient problems.\n",
    "\n",
    "Normalizing the data to the interval (0,1)--it is not really required because all the pixel values already lie in the range of (0,255)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T05:16:06.909542Z",
     "iopub.status.busy": "2023-04-02T05:16:06.909018Z",
     "iopub.status.idle": "2023-04-02T05:16:07.028448Z",
     "shell.execute_reply": "2023-04-02T05:16:07.027413Z",
     "shell.execute_reply.started": "2023-04-02T05:16:06.909477Z"
    },
    "id": "KeCMhgDkMpoI"
   },
   "outputs": [],
   "source": [
    "# The values are ranging between 0 and 255. So, Xmin will be 0 and Xmax will be 255. \n",
    "# Actual formula -- X => (X - Xmin)/(Xmax-Xmin) \n",
    "\n",
    "X_train = X_train/255\n",
    "X_val = X_val/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNGAhCZyMpoI"
   },
   "source": [
    "### Reshaping the data\n",
    "The data is to reshaped to the (28x28) from 784\n",
    "\n",
    "Since the training examples are 1D vectors but the convolutions should be happening on the 2D images,the data should be reshaped to the input data from (n_train x 784) to (n_train x 28 x 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:16:07.029948Z",
     "iopub.status.busy": "2023-04-02T05:16:07.029553Z",
     "iopub.status.idle": "2023-04-02T05:16:07.037251Z",
     "shell.execute_reply": "2023-04-02T05:16:07.035914Z",
     "shell.execute_reply.started": "2023-04-02T05:16:07.02991Z"
    },
    "id": "-V9TQR_TMpoI",
    "outputId": "19b020d4-41c1-4972-9fc8-15475faeeea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The shape of X_train variable is (167972, 28, 28, 1) \n",
      " The shape of y_train variable is (167972,) \n",
      " The shape of X_val variable is (41994, 28, 28, 1) \n",
      " The shape of y_val variable is (41994,)\n"
     ]
    }
   ],
   "source": [
    "X_train_converted = X_train.values.reshape(-1,28,28,1)\n",
    "X_val_converted = X_val.values.reshape(-1,28,28,1)\n",
    "print(f\" The shape of X_train variable is {X_train_converted.shape} \\n The shape of y_train variable is {y_train.shape} \\n The shape of X_val variable is {X_val_converted.shape} \\n The shape of y_val variable is {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:16:07.039656Z",
     "iopub.status.busy": "2023-04-02T05:16:07.038891Z",
     "iopub.status.idle": "2023-04-02T05:16:07.052203Z",
     "shell.execute_reply": "2023-04-02T05:16:07.050759Z",
     "shell.execute_reply.started": "2023-04-02T05:16:07.039617Z"
    },
    "id": "YwchN7pTMpoJ",
    "outputId": "68a0c487-72b7-4504-fe52-87a50eb04383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before upsampling --- (167972, 28, 28, 1)\n",
      "Before upsampling --- (28, 28, 1)\n",
      "Before upsampling --- (28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before upsampling --- {X_train_converted.shape}\")\n",
    "print(f\"Before upsampling --- {X_train_converted[0].shape}\")\n",
    "print(f\"Before upsampling --- {X_train_converted[0][1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPMciei0MpoJ"
   },
   "source": [
    "Converting data into 32x32 as the LeNet model architecture is defined for the model, where the input would be of 32x32 dimensions. So, 28x28 dimension data is converted into 32x32 dimension data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:16:07.058781Z",
     "iopub.status.busy": "2023-04-02T05:16:07.057833Z",
     "iopub.status.idle": "2023-04-02T05:16:07.432892Z",
     "shell.execute_reply": "2023-04-02T05:16:07.431363Z",
     "shell.execute_reply.started": "2023-04-02T05:16:07.058736Z"
    },
    "id": "FJoApvHFMpoJ",
    "outputId": "7e28756c-86ca-4491-9556-cf813fc7b1ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The shape of X_train variable is (167972, 32, 32, 1) \n",
      " The shape of y_train variable is (167972,) \n",
      " The shape of X_val variable is (41994, 32, 32, 1) \n",
      " The shape of y_val variable is (41994,)\n"
     ]
    }
   ],
   "source": [
    "X_train_converted = np.pad(X_train_converted,((0,0),(2,2),(2,2),(0,0)),'constant')\n",
    "X_val_converted = np.pad(X_val_converted,((0,0),(2,2),(2,2),(0,0)),'constant')\n",
    "print(f\" The shape of X_train variable is {X_train_converted.shape} \\n The shape of y_train variable is {y_train.shape} \\n The shape of X_val variable is {X_val_converted.shape} \\n The shape of y_val variable is {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:16:07.435499Z",
     "iopub.status.busy": "2023-04-02T05:16:07.434791Z",
     "iopub.status.idle": "2023-04-02T05:16:07.442715Z",
     "shell.execute_reply": "2023-04-02T05:16:07.441495Z",
     "shell.execute_reply.started": "2023-04-02T05:16:07.435451Z"
    },
    "id": "F7rWPqPrMpoJ",
    "outputId": "c2a56ccf-2a80-4900-8e76-ace5a1659760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After upsampling --- (167972, 32, 32, 1)\n",
      "After upsampling --- (32, 32, 1)\n",
      "After upsampling --- (32, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"After upsampling --- {X_train_converted.shape}\")\n",
    "print(f\"After upsampling --- {X_train_converted[0].shape}\")\n",
    "print(f\"After upsampling --- {X_train_converted[0][1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewieKYgvMpoJ"
   },
   "source": [
    "#### Target-variable update\n",
    "\n",
    "Target_variable for each image is single value. This should be converted into 10-dim vector. For instance, If the class label for an image is 5, then it should be represented as [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T05:16:07.444869Z",
     "iopub.status.busy": "2023-04-02T05:16:07.444182Z",
     "iopub.status.idle": "2023-04-02T05:16:07.455721Z",
     "shell.execute_reply": "2023-04-02T05:16:07.454538Z",
     "shell.execute_reply.started": "2023-04-02T05:16:07.444833Z"
    },
    "id": "DDr38razMpoJ"
   },
   "outputs": [],
   "source": [
    "Y_train = keras.utils.to_categorical(y_train,num_classes=35) \n",
    "Y_val = keras.utils.to_categorical(y_val,num_classes=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zc4yGfLNMpoK"
   },
   "source": [
    "# Convolutional Neural Network\n",
    "CNN is used for image classification, object detection and Image Segmentation\n",
    "\n",
    "#### Building a LeNet model (which is a basic CNN algorithm)\n",
    "LeNet was introduced in the research paper “Gradient-Based Learning Applied To Document Recognition” in the year 1998 by Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner.\n",
    "\n",
    "#### LeNet-5 CNN architecture\n",
    "![image.png](attachment:aae213ea-440e-4bf8-8177-f714ffb098ee.png)\n",
    "LeNet-5 Total seven layer , does not comprise an input, each containing a trainable parameters; each layer has a plurality of the Map the Feature , a characteristic of each of the input FeatureMap extracted by means of a convolution filter, and then each FeatureMap There are multiple neurons\n",
    "![image.png](attachment:3d957938-f8b4-40b3-8cec-1fcffe602308.png)\n",
    "[Much more detailed explanation is over here](https://www.kaggle.com/code/blurredmachine/lenet-architecture-a-complete-guide)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMePRWVpMpoK"
   },
   "source": [
    "There are many blogs, which explain about the convolution, stride, pooling and padding. Here are the few references\n",
    "\n",
    "[Easy understanding of Stride is here](https://medium.com/machine-learning-algorithms/what-is-stride-in-convolutional-neural-network-e3b4ae9baedb) \n",
    "\n",
    "[Better understanding about Pooling layers](https://towardsai.net/p/l/introduction-to-pooling-layers-in-cnn)\n",
    "\n",
    "[About Padding layers](https://medium.com/codex/why-padding-is-important-in-convolutional-neural-network-cnn-8cf7e9e38ca8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiEHk-5dMpoK"
   },
   "source": [
    "# Building the Model\n",
    "\n",
    "[This article gives so much help on understanding of the CNN ] to Visualize how the CNN works (https://poloclub.github.io/cnn-explainer/#article-convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T05:16:07.458341Z",
     "iopub.status.busy": "2023-04-02T05:16:07.457973Z",
     "iopub.status.idle": "2023-04-02T05:16:07.738126Z",
     "shell.execute_reply": "2023-04-02T05:16:07.736786Z",
     "shell.execute_reply.started": "2023-04-02T05:16:07.458308Z"
    },
    "id": "PyX_lohmMpoK"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=(32, 32, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), activation='tanh'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation='tanh'))\n",
    "model.add(Dense(84, activation='tanh'))\n",
    "model.add(Dense(35, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:16:07.740733Z",
     "iopub.status.busy": "2023-04-02T05:16:07.739896Z",
     "iopub.status.idle": "2023-04-02T05:16:07.773989Z",
     "shell.execute_reply": "2023-04-02T05:16:07.772776Z",
     "shell.execute_reply.started": "2023-04-02T05:16:07.740663Z"
    },
    "id": "UL8WdZZbMpoK",
    "outputId": "563c9d64-1c15-4acb-c1be-2c685a36c4f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 6)         156       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 6)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 10, 10, 16)        2416      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5, 5, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 120)               48120     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 35)                2975      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63,831\n",
      "Trainable params: 63,831\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:16:07.776566Z",
     "iopub.status.busy": "2023-04-02T05:16:07.77597Z",
     "iopub.status.idle": "2023-04-02T05:16:07.785313Z",
     "shell.execute_reply": "2023-04-02T05:16:07.784005Z",
     "shell.execute_reply.started": "2023-04-02T05:16:07.776516Z"
    },
    "id": "ipSQhrpCMpoK",
    "outputId": "66b700ff-8d19-4a44-81ba-aeede2d9f5e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####     32   32    1\n",
      "              Conv2D    \\|/  -------------------       156     0.2%\n",
      "                tanh   #####     28   28    6\n",
      "        MaxPooling2D   Y max -------------------         0     0.0%\n",
      "                       #####     14   14    6\n",
      "              Conv2D    \\|/  -------------------      2416     3.8%\n",
      "                tanh   #####     10   10   16\n",
      "        MaxPooling2D   Y max -------------------         0     0.0%\n",
      "                       #####      5    5   16\n",
      "             Flatten   ||||| -------------------         0     0.0%\n",
      "                       #####         400\n",
      "               Dense   XXXXX -------------------     48120    75.4%\n",
      "                tanh   #####         120\n",
      "               Dense   XXXXX -------------------     10164    15.9%\n",
      "                tanh   #####          84\n",
      "               Dense   XXXXX -------------------      2975     4.7%\n",
      "             softmax   #####          35\n"
     ]
    }
   ],
   "source": [
    "keras2ascii(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ru8BmUVMpoL"
   },
   "source": [
    "It is always necessary to understand the shapes and number of parameters of the model before training it and getting final predictions. \n",
    "\n",
    "Sizes of each layer can be calculated using below formulas\n",
    "\n",
    "An image of dimensions W_in x H_in and a filter of dimensions K x K with stride S and a padding P. Then, the output activation map would be of dimensions: \n",
    "\n",
    "W_out = {(W_in - K + 2P)/S} + 1\n",
    "\n",
    "H_out = {(H_in - K + 2P)/S} + 1\n",
    "\n",
    "After applying this formula, if found out that the output dimensions are not integers--then it means that stride S has been set incorrectly.\n",
    "\n",
    "* When there is no padding at all, the output dimensions are ( ({W_in - K}/S) + 1,({H_in - K}/S) + 1).\n",
    "* If the size of the input is unchanged after the convolution layer , then same padding can be applied. So, then W_out = W_in and H_out = H_in ==> If Stride s=1, then p = (k-1)/2\n",
    "\n",
    "\n",
    "If there is a pooling kernel of dimensions r x r and a Stride s then size of the output volumne would be:\n",
    "\n",
    "W_out = ((W_in - r)/s)+1\n",
    "\n",
    "H_out = ((H_in -r)/s)+1\n",
    "\n",
    "\n",
    "=====================================================================\n",
    "\n",
    "Input image is of size = 32x32x1 (the image is of size 32x32 with only 1 channel--it is also called depth at times).\n",
    "\n",
    "So, W_in = 32 , H_in = 32 and c = 1 (where c is number of channels)\n",
    "\n",
    "1. In the first layer, there are 6 (5x5) kernels, which implement Conv2D operation with tanh as activation function. So, kernel k = 5 and if Stride s is not mentioned, then s = 1 by default and also default padding is valid--which means **no zero-padding** so, p = 0 \n",
    "\n",
    "So, the output dimension of first layer would be\n",
    "\n",
    "W_out = ((32-5)/1)+1\n",
    "\n",
    "H_out = ((32-5)/1)+1\n",
    "\n",
    "W_out x H_out = 28 x 28\n",
    "\n",
    "W_out x H_out x channels = 28 x 28 x 6\n",
    "\n",
    "Then, what about weights?--trainable parameters?\n",
    "\n",
    "In Convolutional layers, the objective is to find best kernels---which are weights. So, that our model works better for the task we are solving on\n",
    "\n",
    "We have 6 kernels, each of size 5x5 => 25 weights in each kernel => there are 6 such kernels => 25x6 = 150 + bias term to each kernel (6 bias terms altogether) ==> 150+6 = 156 weights in first layer\n",
    "\n",
    "2. The second layer is max pooling layer---which doesn't have any weights (trainable parameters) because it is just considering the maximum value from 2x2 grid of the output from previous layer and the stride of 2 (it is default for pooling layers)\n",
    "\n",
    "W_out = ((28-2)/2)+1\n",
    "\n",
    "H_out = ((28-2)/2)+1\n",
    "\n",
    "W_out x H_out = 14 x 14\n",
    "\n",
    "W_out x H_out x channels = 14 x 14 x 6\n",
    "\n",
    "3. In the third layer, there are 16 (5x5) kernels, which implement Conv2D operation with tanh as activation function. So, kernel k = 5 and if Stride s is not mentioned, then s = 1 by default and also default padding is valid--which means **no zero-padding** so, p = 0 \n",
    "\n",
    "So, the output dimension of third layer would be\n",
    "\n",
    "W_out = ((14-5)/1)+1\n",
    "\n",
    "H_out = ((14-5)/1)+1\n",
    "\n",
    "W_out x H_out = 10 x 10\n",
    "\n",
    "W_out x H_out x channels = 10 x 10 x 16\n",
    "\n",
    "Then, what about weights?--trainable parameters?\n",
    "\n",
    "In Convolutional layers, the objective is to find best kernels---which are weights. So, that our model works better for the task we are solving on\n",
    "\n",
    "Each kernel is of size 5x5 => 25 weights in each kernel and the input to this layer is of 14x14x6 (6 channels) => there are 25x6 = 150 weights and there are 16 Conv2D layers here. So, 150x16 = 2400 and bias term to each kernel (16 bias terms altogether) ==> 2400+16 = 2416 weights in third layer\n",
    "\n",
    "4. The fourth layer is max pooling layer---which doesn't have any weights (trainable parameters) because it is just considering the maximum value from 2x2 grid of the output from previous layer and the stride of 2 (it is default for pooling layers)\n",
    "\n",
    "W_out = ((10-2)/2)+1\n",
    "\n",
    "H_out = ((10-2)/2)+1\n",
    "\n",
    "W_out x H_out = 5 x 5\n",
    "\n",
    "W_out x H_out x channels = 5 x 5 x 16\n",
    "\n",
    "5. The fifth layer is flatten --- which will convert the array/matrix into the vector\n",
    "\n",
    "5 X 5 X 16 = 400 ---- This will be the dimensions of the new vector\n",
    "\n",
    "From, here on----it would be treated as an MLP network\n",
    "\n",
    "6. Dense connections with 120 neurons and the input is 400 dimensions. So, number of weights will be 120 x 400 = 48000 + bias term connecting to 120 neurons. In total, 48000+120 = 48120 weights\n",
    "\n",
    "7. Dense connections with 84 neurons and the previous layer is of 120 neurons, as this is fully connected layer---the number of weights is 120 x 84 = 10080 + bias term connecting to 84 neurons. In total, 10080 + 84 = 10164 weights\n",
    "\n",
    "8. From the previous layer, it is again connected to 10 neurons (softmax layer). So, the number of weights is 84 x 10 = 840 + bias term connecting to 10 neurons. In total, 840 + 10 = 850 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:16:07.786781Z",
     "iopub.status.busy": "2023-04-02T05:16:07.786388Z",
     "iopub.status.idle": "2023-04-02T05:16:07.82065Z",
     "shell.execute_reply": "2023-04-02T05:16:07.819745Z",
     "shell.execute_reply.started": "2023-04-02T05:16:07.786745Z"
    },
    "id": "lmC_7jllMpoL",
    "outputId": "50a9af6d-3080-4c67-9306-aed29ef469e0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=292x212 at 0x7F6271786880>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualkeras.layered_view(model,legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T05:16:07.822965Z",
     "iopub.status.busy": "2023-04-02T05:16:07.822279Z",
     "iopub.status.idle": "2023-04-02T05:16:07.842988Z",
     "shell.execute_reply": "2023-04-02T05:16:07.841975Z",
     "shell.execute_reply.started": "2023-04-02T05:16:07.822928Z"
    },
    "id": "TY5W-jmhMpoL"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEJj4kt4MpoL"
   },
   "source": [
    "In depth explanation of compile function parameters and fit function parameters is explained in the below notebook. Please refer it for more detail\n",
    "\n",
    "https://www.kaggle.com/code/mvschamanth/mlp-model-in-keras-with-high-accuracy-of-97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T05:16:07.845003Z",
     "iopub.status.busy": "2023-04-02T05:16:07.844586Z",
     "iopub.status.idle": "2023-04-02T05:16:07.849919Z",
     "shell.execute_reply": "2023-04-02T05:16:07.848793Z",
     "shell.execute_reply.started": "2023-04-02T05:16:07.844966Z"
    },
    "id": "KjpZJsK2MpoL"
   },
   "outputs": [],
   "source": [
    "number_of_epoch = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:16:07.852219Z",
     "iopub.status.busy": "2023-04-02T05:16:07.851198Z",
     "iopub.status.idle": "2023-04-02T05:20:51.537423Z",
     "shell.execute_reply": "2023-04-02T05:20:51.535728Z",
     "shell.execute_reply.started": "2023-04-02T05:16:07.852163Z"
    },
    "id": "_ZIe9IrFMpoL",
    "outputId": "5f6a8a8b-bad6-48fb-f950-72973e0efbb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "500/500 [==============================] - 11s 17ms/step - loss: 3.4703 - accuracy: 0.0798 - val_loss: 3.2633 - val_accuracy: 0.1415\n",
      "Epoch 2/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 2.8878 - accuracy: 0.2136 - val_loss: 2.6002 - val_accuracy: 0.2795\n",
      "Epoch 3/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 2.4576 - accuracy: 0.3135 - val_loss: 2.3355 - val_accuracy: 0.3399\n",
      "Epoch 4/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 2.2557 - accuracy: 0.3608 - val_loss: 2.1743 - val_accuracy: 0.3786\n",
      "Epoch 5/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 2.1097 - accuracy: 0.3986 - val_loss: 2.0380 - val_accuracy: 0.4148\n",
      "Epoch 6/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9769 - accuracy: 0.4336 - val_loss: 1.9070 - val_accuracy: 0.4486\n",
      "Epoch 7/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8456 - accuracy: 0.4683 - val_loss: 1.7759 - val_accuracy: 0.4856\n",
      "Epoch 8/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 1.7163 - accuracy: 0.5050 - val_loss: 1.6502 - val_accuracy: 0.5168\n",
      "Epoch 9/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5955 - accuracy: 0.5371 - val_loss: 1.5360 - val_accuracy: 0.5528\n",
      "Epoch 10/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 1.4875 - accuracy: 0.5669 - val_loss: 1.4350 - val_accuracy: 0.5767\n",
      "Epoch 11/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 1.3928 - accuracy: 0.5923 - val_loss: 1.3462 - val_accuracy: 0.6028\n",
      "Epoch 12/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 1.3097 - accuracy: 0.6138 - val_loss: 1.2682 - val_accuracy: 0.6240\n",
      "Epoch 13/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 1.2363 - accuracy: 0.6340 - val_loss: 1.2006 - val_accuracy: 0.6413\n",
      "Epoch 14/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.1706 - accuracy: 0.6512 - val_loss: 1.1377 - val_accuracy: 0.6587\n",
      "Epoch 15/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1118 - accuracy: 0.6680 - val_loss: 1.0838 - val_accuracy: 0.6740\n",
      "Epoch 16/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.0591 - accuracy: 0.6827 - val_loss: 1.0336 - val_accuracy: 0.6906\n",
      "Epoch 17/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0117 - accuracy: 0.6951 - val_loss: 0.9889 - val_accuracy: 0.7031\n",
      "Epoch 18/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.9687 - accuracy: 0.7074 - val_loss: 0.9493 - val_accuracy: 0.7133\n",
      "Epoch 19/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.9299 - accuracy: 0.7186 - val_loss: 0.9152 - val_accuracy: 0.7239\n",
      "Epoch 20/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.8950 - accuracy: 0.7288 - val_loss: 0.8811 - val_accuracy: 0.7325\n",
      "Epoch 21/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.8632 - accuracy: 0.7383 - val_loss: 0.8530 - val_accuracy: 0.7417\n",
      "Epoch 22/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.8342 - accuracy: 0.7467 - val_loss: 0.8234 - val_accuracy: 0.7497\n",
      "Epoch 23/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.8081 - accuracy: 0.7541 - val_loss: 0.8016 - val_accuracy: 0.7558\n",
      "Epoch 24/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.7841 - accuracy: 0.7612 - val_loss: 0.7772 - val_accuracy: 0.7632\n",
      "Epoch 25/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.7620 - accuracy: 0.7678 - val_loss: 0.7572 - val_accuracy: 0.7677\n",
      "Epoch 26/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.7419 - accuracy: 0.7736 - val_loss: 0.7394 - val_accuracy: 0.7720\n",
      "Epoch 27/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.7228 - accuracy: 0.7783 - val_loss: 0.7223 - val_accuracy: 0.7765\n",
      "Epoch 28/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.7056 - accuracy: 0.7834 - val_loss: 0.7069 - val_accuracy: 0.7808\n",
      "Epoch 29/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.6896 - accuracy: 0.7878 - val_loss: 0.6899 - val_accuracy: 0.7860\n",
      "Epoch 30/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.6741 - accuracy: 0.7927 - val_loss: 0.6763 - val_accuracy: 0.7901\n",
      "Epoch 31/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.6602 - accuracy: 0.7957 - val_loss: 0.6638 - val_accuracy: 0.7929\n",
      "Epoch 32/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.6469 - accuracy: 0.8003 - val_loss: 0.6489 - val_accuracy: 0.7981\n",
      "Epoch 33/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.6340 - accuracy: 0.8039 - val_loss: 0.6360 - val_accuracy: 0.8001\n",
      "Epoch 34/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.6219 - accuracy: 0.8072 - val_loss: 0.6271 - val_accuracy: 0.8043\n",
      "Epoch 35/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.6101 - accuracy: 0.8104 - val_loss: 0.6143 - val_accuracy: 0.8074\n",
      "Epoch 36/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.5991 - accuracy: 0.8141 - val_loss: 0.6123 - val_accuracy: 0.8069\n",
      "Epoch 37/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.5891 - accuracy: 0.8167 - val_loss: 0.5980 - val_accuracy: 0.8119\n",
      "Epoch 38/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.5795 - accuracy: 0.8199 - val_loss: 0.5839 - val_accuracy: 0.8160\n",
      "Epoch 39/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5704 - accuracy: 0.8217 - val_loss: 0.5823 - val_accuracy: 0.8168\n",
      "Epoch 40/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.5611 - accuracy: 0.8249 - val_loss: 0.5671 - val_accuracy: 0.8222\n",
      "Epoch 41/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5513 - accuracy: 0.8285 - val_loss: 0.5589 - val_accuracy: 0.8244\n",
      "Epoch 42/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.5437 - accuracy: 0.8302 - val_loss: 0.5527 - val_accuracy: 0.8260\n",
      "Epoch 43/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5353 - accuracy: 0.8326 - val_loss: 0.5456 - val_accuracy: 0.8284\n",
      "Epoch 44/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.5277 - accuracy: 0.8353 - val_loss: 0.5354 - val_accuracy: 0.8309\n",
      "Epoch 45/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.5195 - accuracy: 0.8375 - val_loss: 0.5334 - val_accuracy: 0.8310\n",
      "Epoch 46/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.5122 - accuracy: 0.8399 - val_loss: 0.5217 - val_accuracy: 0.8357\n",
      "Epoch 47/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.5053 - accuracy: 0.8425 - val_loss: 0.5160 - val_accuracy: 0.8362\n",
      "Epoch 48/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4979 - accuracy: 0.8443 - val_loss: 0.5085 - val_accuracy: 0.8393\n",
      "Epoch 49/350\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4910 - accuracy: 0.8468 - val_loss: 0.5024 - val_accuracy: 0.8404\n",
      "Epoch 50/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.4844 - accuracy: 0.8490 - val_loss: 0.4949 - val_accuracy: 0.8443\n",
      "Epoch 51/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.4785 - accuracy: 0.8507 - val_loss: 0.4896 - val_accuracy: 0.8453\n",
      "Epoch 52/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.4720 - accuracy: 0.8523 - val_loss: 0.4866 - val_accuracy: 0.8453\n",
      "Epoch 53/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4656 - accuracy: 0.8555 - val_loss: 0.4793 - val_accuracy: 0.8478\n",
      "Epoch 54/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.4600 - accuracy: 0.8568 - val_loss: 0.4781 - val_accuracy: 0.8498\n",
      "Epoch 55/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4537 - accuracy: 0.8583 - val_loss: 0.4678 - val_accuracy: 0.8522\n",
      "Epoch 56/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.4487 - accuracy: 0.8598 - val_loss: 0.4627 - val_accuracy: 0.8545\n",
      "Epoch 57/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.4429 - accuracy: 0.8620 - val_loss: 0.4636 - val_accuracy: 0.8546\n",
      "Epoch 58/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.4376 - accuracy: 0.8635 - val_loss: 0.4514 - val_accuracy: 0.8582\n",
      "Epoch 59/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.4320 - accuracy: 0.8658 - val_loss: 0.4517 - val_accuracy: 0.8556\n",
      "Epoch 60/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.4272 - accuracy: 0.8669 - val_loss: 0.4413 - val_accuracy: 0.8603\n",
      "Epoch 61/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.4214 - accuracy: 0.8684 - val_loss: 0.4358 - val_accuracy: 0.8629\n",
      "Epoch 62/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.4163 - accuracy: 0.8706 - val_loss: 0.4338 - val_accuracy: 0.8626\n",
      "Epoch 63/350\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.4107 - accuracy: 0.8722 - val_loss: 0.4352 - val_accuracy: 0.8617\n",
      "Epoch 64/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.4065 - accuracy: 0.8738 - val_loss: 0.4226 - val_accuracy: 0.8665\n",
      "Epoch 65/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.4025 - accuracy: 0.8741 - val_loss: 0.4192 - val_accuracy: 0.8684\n",
      "Epoch 66/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.3962 - accuracy: 0.8768 - val_loss: 0.4142 - val_accuracy: 0.8695\n",
      "Epoch 67/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.3923 - accuracy: 0.8785 - val_loss: 0.4086 - val_accuracy: 0.8715\n",
      "Epoch 68/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.3875 - accuracy: 0.8789 - val_loss: 0.4062 - val_accuracy: 0.8729\n",
      "Epoch 69/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.3830 - accuracy: 0.8810 - val_loss: 0.4051 - val_accuracy: 0.8727\n",
      "Epoch 70/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.3801 - accuracy: 0.8817 - val_loss: 0.4007 - val_accuracy: 0.8740\n",
      "Epoch 71/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3751 - accuracy: 0.8830 - val_loss: 0.3945 - val_accuracy: 0.8761\n",
      "Epoch 72/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.3711 - accuracy: 0.8843 - val_loss: 0.3918 - val_accuracy: 0.8759\n",
      "Epoch 73/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3673 - accuracy: 0.8861 - val_loss: 0.3858 - val_accuracy: 0.8787\n",
      "Epoch 74/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3638 - accuracy: 0.8870 - val_loss: 0.3867 - val_accuracy: 0.8780\n",
      "Epoch 75/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.3591 - accuracy: 0.8885 - val_loss: 0.3801 - val_accuracy: 0.8802\n",
      "Epoch 76/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3554 - accuracy: 0.8893 - val_loss: 0.3793 - val_accuracy: 0.8809\n",
      "Epoch 77/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.3525 - accuracy: 0.8907 - val_loss: 0.3739 - val_accuracy: 0.8828\n",
      "Epoch 78/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.3480 - accuracy: 0.8918 - val_loss: 0.3721 - val_accuracy: 0.8825\n",
      "Epoch 79/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3458 - accuracy: 0.8932 - val_loss: 0.3671 - val_accuracy: 0.8841\n",
      "Epoch 80/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3422 - accuracy: 0.8938 - val_loss: 0.3720 - val_accuracy: 0.8810\n",
      "Epoch 81/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.3378 - accuracy: 0.8953 - val_loss: 0.3674 - val_accuracy: 0.8844\n",
      "Epoch 82/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3351 - accuracy: 0.8961 - val_loss: 0.3719 - val_accuracy: 0.8792\n",
      "Epoch 83/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.3312 - accuracy: 0.8973 - val_loss: 0.3758 - val_accuracy: 0.8807\n",
      "Epoch 84/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.3288 - accuracy: 0.8983 - val_loss: 0.3594 - val_accuracy: 0.8849\n",
      "Epoch 85/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.3254 - accuracy: 0.8994 - val_loss: 0.3522 - val_accuracy: 0.8887\n",
      "Epoch 86/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3235 - accuracy: 0.8998 - val_loss: 0.3488 - val_accuracy: 0.8897\n",
      "Epoch 87/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.3198 - accuracy: 0.9010 - val_loss: 0.3452 - val_accuracy: 0.8910\n",
      "Epoch 88/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3167 - accuracy: 0.9020 - val_loss: 0.3485 - val_accuracy: 0.8892\n",
      "Epoch 89/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3145 - accuracy: 0.9028 - val_loss: 0.3395 - val_accuracy: 0.8933\n",
      "Epoch 90/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3121 - accuracy: 0.9033 - val_loss: 0.3382 - val_accuracy: 0.8932\n",
      "Epoch 91/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.3084 - accuracy: 0.9046 - val_loss: 0.3380 - val_accuracy: 0.8933\n",
      "Epoch 92/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.3068 - accuracy: 0.9049 - val_loss: 0.3317 - val_accuracy: 0.8954\n",
      "Epoch 93/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.3044 - accuracy: 0.9057 - val_loss: 0.3336 - val_accuracy: 0.8938\n",
      "Epoch 94/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3014 - accuracy: 0.9065 - val_loss: 0.3282 - val_accuracy: 0.8965\n",
      "Epoch 95/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2987 - accuracy: 0.9074 - val_loss: 0.3405 - val_accuracy: 0.8892\n",
      "Epoch 96/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.2963 - accuracy: 0.9084 - val_loss: 0.3264 - val_accuracy: 0.8978\n",
      "Epoch 97/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.2939 - accuracy: 0.9092 - val_loss: 0.3222 - val_accuracy: 0.8980\n",
      "Epoch 98/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2912 - accuracy: 0.9098 - val_loss: 0.3288 - val_accuracy: 0.8947\n",
      "Epoch 99/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.2888 - accuracy: 0.9110 - val_loss: 0.3179 - val_accuracy: 0.8995\n",
      "Epoch 100/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2872 - accuracy: 0.9114 - val_loss: 0.3175 - val_accuracy: 0.9003\n",
      "Epoch 101/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2857 - accuracy: 0.9117 - val_loss: 0.3136 - val_accuracy: 0.9008\n",
      "Epoch 102/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2828 - accuracy: 0.9126 - val_loss: 0.3118 - val_accuracy: 0.9009\n",
      "Epoch 103/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.2809 - accuracy: 0.9130 - val_loss: 0.3166 - val_accuracy: 0.8999\n",
      "Epoch 104/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2782 - accuracy: 0.9143 - val_loss: 0.3086 - val_accuracy: 0.9018\n",
      "Epoch 105/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.2768 - accuracy: 0.9142 - val_loss: 0.3111 - val_accuracy: 0.9013\n",
      "Epoch 106/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2740 - accuracy: 0.9153 - val_loss: 0.3067 - val_accuracy: 0.9029\n",
      "Epoch 107/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2730 - accuracy: 0.9162 - val_loss: 0.3023 - val_accuracy: 0.9039\n",
      "Epoch 108/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2711 - accuracy: 0.9164 - val_loss: 0.3071 - val_accuracy: 0.9015\n",
      "Epoch 109/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2689 - accuracy: 0.9168 - val_loss: 0.3087 - val_accuracy: 0.9010\n",
      "Epoch 110/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2671 - accuracy: 0.9180 - val_loss: 0.2998 - val_accuracy: 0.9061\n",
      "Epoch 111/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.2656 - accuracy: 0.9181 - val_loss: 0.2956 - val_accuracy: 0.9061\n",
      "Epoch 112/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2628 - accuracy: 0.9189 - val_loss: 0.2959 - val_accuracy: 0.9062\n",
      "Epoch 113/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2613 - accuracy: 0.9197 - val_loss: 0.2978 - val_accuracy: 0.9063\n",
      "Epoch 114/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2596 - accuracy: 0.9201 - val_loss: 0.3052 - val_accuracy: 0.9022\n",
      "Epoch 115/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.2572 - accuracy: 0.9206 - val_loss: 0.2909 - val_accuracy: 0.9081\n",
      "Epoch 116/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2558 - accuracy: 0.9211 - val_loss: 0.2901 - val_accuracy: 0.9079\n",
      "Epoch 117/350\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.2547 - accuracy: 0.9215 - val_loss: 0.2905 - val_accuracy: 0.9079\n",
      "Epoch 118/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2534 - accuracy: 0.9218 - val_loss: 0.2919 - val_accuracy: 0.9065\n",
      "Epoch 119/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.2515 - accuracy: 0.9227 - val_loss: 0.2854 - val_accuracy: 0.9091\n",
      "Epoch 120/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.2499 - accuracy: 0.9228 - val_loss: 0.2869 - val_accuracy: 0.9086\n",
      "Epoch 121/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2485 - accuracy: 0.9234 - val_loss: 0.2840 - val_accuracy: 0.9102\n",
      "Epoch 122/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2469 - accuracy: 0.9237 - val_loss: 0.2889 - val_accuracy: 0.9068\n",
      "Epoch 123/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2452 - accuracy: 0.9243 - val_loss: 0.2822 - val_accuracy: 0.9110\n",
      "Epoch 124/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.2434 - accuracy: 0.9249 - val_loss: 0.2804 - val_accuracy: 0.9109\n",
      "Epoch 125/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2425 - accuracy: 0.9252 - val_loss: 0.2820 - val_accuracy: 0.9093\n",
      "Epoch 126/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.2405 - accuracy: 0.9261 - val_loss: 0.2771 - val_accuracy: 0.9112\n",
      "Epoch 127/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2396 - accuracy: 0.9257 - val_loss: 0.2840 - val_accuracy: 0.9102\n",
      "Epoch 128/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2388 - accuracy: 0.9262 - val_loss: 0.2791 - val_accuracy: 0.9119\n",
      "Epoch 129/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2362 - accuracy: 0.9272 - val_loss: 0.2725 - val_accuracy: 0.9131\n",
      "Epoch 130/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2354 - accuracy: 0.9275 - val_loss: 0.2800 - val_accuracy: 0.9101\n",
      "Epoch 131/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2339 - accuracy: 0.9279 - val_loss: 0.2699 - val_accuracy: 0.9147\n",
      "Epoch 132/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2328 - accuracy: 0.9282 - val_loss: 0.2710 - val_accuracy: 0.9131\n",
      "Epoch 133/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.2314 - accuracy: 0.9287 - val_loss: 0.2696 - val_accuracy: 0.9142\n",
      "Epoch 134/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2304 - accuracy: 0.9286 - val_loss: 0.2679 - val_accuracy: 0.9143\n",
      "Epoch 135/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.2289 - accuracy: 0.9291 - val_loss: 0.2752 - val_accuracy: 0.9120\n",
      "Epoch 136/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.2275 - accuracy: 0.9295 - val_loss: 0.2687 - val_accuracy: 0.9138\n",
      "Epoch 137/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2261 - accuracy: 0.9303 - val_loss: 0.2730 - val_accuracy: 0.9127\n",
      "Epoch 138/350\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.2252 - accuracy: 0.9308 - val_loss: 0.2641 - val_accuracy: 0.9158\n",
      "Epoch 139/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.2240 - accuracy: 0.9309 - val_loss: 0.2715 - val_accuracy: 0.9141\n",
      "Epoch 140/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2228 - accuracy: 0.9314 - val_loss: 0.2619 - val_accuracy: 0.9166\n",
      "Epoch 141/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2217 - accuracy: 0.9313 - val_loss: 0.2704 - val_accuracy: 0.9140\n",
      "Epoch 142/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2204 - accuracy: 0.9320 - val_loss: 0.2625 - val_accuracy: 0.9155\n",
      "Epoch 143/350\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.2195 - accuracy: 0.9318 - val_loss: 0.2646 - val_accuracy: 0.9161\n",
      "Epoch 144/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2180 - accuracy: 0.9326 - val_loss: 0.2660 - val_accuracy: 0.9141\n",
      "Epoch 145/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.2175 - accuracy: 0.9327 - val_loss: 0.2568 - val_accuracy: 0.9183\n",
      "Epoch 146/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.2155 - accuracy: 0.9335 - val_loss: 0.2550 - val_accuracy: 0.9188\n",
      "Epoch 147/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.2145 - accuracy: 0.9338 - val_loss: 0.2564 - val_accuracy: 0.9181\n",
      "Epoch 148/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.2139 - accuracy: 0.9343 - val_loss: 0.2598 - val_accuracy: 0.9177\n",
      "Epoch 149/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2127 - accuracy: 0.9343 - val_loss: 0.2556 - val_accuracy: 0.9187\n",
      "Epoch 150/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2117 - accuracy: 0.9346 - val_loss: 0.2547 - val_accuracy: 0.9190\n",
      "Epoch 151/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2107 - accuracy: 0.9349 - val_loss: 0.2525 - val_accuracy: 0.9205\n",
      "Epoch 152/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.2090 - accuracy: 0.9358 - val_loss: 0.2552 - val_accuracy: 0.9189\n",
      "Epoch 153/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2085 - accuracy: 0.9360 - val_loss: 0.2499 - val_accuracy: 0.9204\n",
      "Epoch 154/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.2073 - accuracy: 0.9365 - val_loss: 0.2585 - val_accuracy: 0.9179\n",
      "Epoch 155/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2060 - accuracy: 0.9368 - val_loss: 0.2483 - val_accuracy: 0.9217\n",
      "Epoch 156/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.2058 - accuracy: 0.9367 - val_loss: 0.2490 - val_accuracy: 0.9216\n",
      "Epoch 157/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2038 - accuracy: 0.9376 - val_loss: 0.2477 - val_accuracy: 0.9221\n",
      "Epoch 158/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.2030 - accuracy: 0.9376 - val_loss: 0.2503 - val_accuracy: 0.9204\n",
      "Epoch 159/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2029 - accuracy: 0.9379 - val_loss: 0.2470 - val_accuracy: 0.9216\n",
      "Epoch 160/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.2012 - accuracy: 0.9381 - val_loss: 0.2463 - val_accuracy: 0.9225\n",
      "Epoch 161/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2007 - accuracy: 0.9383 - val_loss: 0.2487 - val_accuracy: 0.9212\n",
      "Epoch 162/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.2002 - accuracy: 0.9384 - val_loss: 0.2436 - val_accuracy: 0.9233\n",
      "Epoch 163/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1990 - accuracy: 0.9389 - val_loss: 0.2454 - val_accuracy: 0.9225\n",
      "Epoch 164/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1977 - accuracy: 0.9392 - val_loss: 0.2441 - val_accuracy: 0.9223\n",
      "Epoch 165/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1968 - accuracy: 0.9395 - val_loss: 0.2427 - val_accuracy: 0.9237\n",
      "Epoch 166/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1968 - accuracy: 0.9393 - val_loss: 0.2427 - val_accuracy: 0.9237\n",
      "Epoch 167/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1953 - accuracy: 0.9401 - val_loss: 0.2404 - val_accuracy: 0.9237\n",
      "Epoch 168/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1944 - accuracy: 0.9403 - val_loss: 0.2397 - val_accuracy: 0.9242\n",
      "Epoch 169/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1937 - accuracy: 0.9410 - val_loss: 0.2392 - val_accuracy: 0.9241\n",
      "Epoch 170/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1924 - accuracy: 0.9412 - val_loss: 0.2425 - val_accuracy: 0.9235\n",
      "Epoch 171/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.1913 - accuracy: 0.9416 - val_loss: 0.2378 - val_accuracy: 0.9248\n",
      "Epoch 172/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1912 - accuracy: 0.9413 - val_loss: 0.2394 - val_accuracy: 0.9245\n",
      "Epoch 173/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1902 - accuracy: 0.9416 - val_loss: 0.2449 - val_accuracy: 0.9230\n",
      "Epoch 174/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1891 - accuracy: 0.9419 - val_loss: 0.2515 - val_accuracy: 0.9209\n",
      "Epoch 175/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1883 - accuracy: 0.9427 - val_loss: 0.2377 - val_accuracy: 0.9247\n",
      "Epoch 176/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1876 - accuracy: 0.9431 - val_loss: 0.2351 - val_accuracy: 0.9263\n",
      "Epoch 177/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1872 - accuracy: 0.9428 - val_loss: 0.2350 - val_accuracy: 0.9265\n",
      "Epoch 178/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1860 - accuracy: 0.9434 - val_loss: 0.2328 - val_accuracy: 0.9262\n",
      "Epoch 179/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1857 - accuracy: 0.9429 - val_loss: 0.2350 - val_accuracy: 0.9258\n",
      "Epoch 180/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1844 - accuracy: 0.9436 - val_loss: 0.2341 - val_accuracy: 0.9255\n",
      "Epoch 181/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1840 - accuracy: 0.9439 - val_loss: 0.2322 - val_accuracy: 0.9273\n",
      "Epoch 182/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1837 - accuracy: 0.9435 - val_loss: 0.2396 - val_accuracy: 0.9239\n",
      "Epoch 183/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1828 - accuracy: 0.9442 - val_loss: 0.2318 - val_accuracy: 0.9271\n",
      "Epoch 184/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1812 - accuracy: 0.9452 - val_loss: 0.2358 - val_accuracy: 0.9256\n",
      "Epoch 185/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1809 - accuracy: 0.9447 - val_loss: 0.2311 - val_accuracy: 0.9267\n",
      "Epoch 186/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1798 - accuracy: 0.9450 - val_loss: 0.2340 - val_accuracy: 0.9265\n",
      "Epoch 187/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1793 - accuracy: 0.9452 - val_loss: 0.2390 - val_accuracy: 0.9233\n",
      "Epoch 188/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1788 - accuracy: 0.9454 - val_loss: 0.2279 - val_accuracy: 0.9277\n",
      "Epoch 189/350\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1779 - accuracy: 0.9458 - val_loss: 0.2302 - val_accuracy: 0.9278\n",
      "Epoch 190/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1772 - accuracy: 0.9461 - val_loss: 0.2306 - val_accuracy: 0.9270\n",
      "Epoch 191/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1764 - accuracy: 0.9458 - val_loss: 0.2269 - val_accuracy: 0.9280\n",
      "Epoch 192/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1763 - accuracy: 0.9464 - val_loss: 0.2265 - val_accuracy: 0.9283\n",
      "Epoch 193/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1749 - accuracy: 0.9471 - val_loss: 0.2304 - val_accuracy: 0.9270\n",
      "Epoch 194/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1747 - accuracy: 0.9468 - val_loss: 0.2267 - val_accuracy: 0.9283\n",
      "Epoch 195/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1746 - accuracy: 0.9466 - val_loss: 0.2260 - val_accuracy: 0.9289\n",
      "Epoch 196/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1732 - accuracy: 0.9475 - val_loss: 0.2257 - val_accuracy: 0.9296\n",
      "Epoch 197/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1728 - accuracy: 0.9471 - val_loss: 0.2282 - val_accuracy: 0.9286\n",
      "Epoch 198/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1716 - accuracy: 0.9478 - val_loss: 0.2248 - val_accuracy: 0.9290\n",
      "Epoch 199/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1714 - accuracy: 0.9478 - val_loss: 0.2241 - val_accuracy: 0.9299\n",
      "Epoch 200/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.1705 - accuracy: 0.9481 - val_loss: 0.2273 - val_accuracy: 0.9293\n",
      "Epoch 201/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1698 - accuracy: 0.9485 - val_loss: 0.2266 - val_accuracy: 0.9281\n",
      "Epoch 202/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1694 - accuracy: 0.9486 - val_loss: 0.2251 - val_accuracy: 0.9292\n",
      "Epoch 203/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1684 - accuracy: 0.9488 - val_loss: 0.2224 - val_accuracy: 0.9303\n",
      "Epoch 204/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1678 - accuracy: 0.9491 - val_loss: 0.2237 - val_accuracy: 0.9295\n",
      "Epoch 205/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1676 - accuracy: 0.9492 - val_loss: 0.2236 - val_accuracy: 0.9300\n",
      "Epoch 206/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1671 - accuracy: 0.9495 - val_loss: 0.2229 - val_accuracy: 0.9294\n",
      "Epoch 207/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1660 - accuracy: 0.9495 - val_loss: 0.2202 - val_accuracy: 0.9304\n",
      "Epoch 208/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1656 - accuracy: 0.9498 - val_loss: 0.2219 - val_accuracy: 0.9303\n",
      "Epoch 209/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1653 - accuracy: 0.9499 - val_loss: 0.2210 - val_accuracy: 0.9300\n",
      "Epoch 210/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1644 - accuracy: 0.9504 - val_loss: 0.2243 - val_accuracy: 0.9293\n",
      "Epoch 211/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.1633 - accuracy: 0.9506 - val_loss: 0.2235 - val_accuracy: 0.9289\n",
      "Epoch 212/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1633 - accuracy: 0.9501 - val_loss: 0.2206 - val_accuracy: 0.9308\n",
      "Epoch 213/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.1629 - accuracy: 0.9509 - val_loss: 0.2184 - val_accuracy: 0.9301\n",
      "Epoch 214/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1620 - accuracy: 0.9509 - val_loss: 0.2195 - val_accuracy: 0.9305\n",
      "Epoch 215/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1608 - accuracy: 0.9513 - val_loss: 0.2197 - val_accuracy: 0.9300\n",
      "Epoch 216/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1611 - accuracy: 0.9513 - val_loss: 0.2215 - val_accuracy: 0.9298\n",
      "Epoch 217/350\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1603 - accuracy: 0.9517 - val_loss: 0.2239 - val_accuracy: 0.9281\n",
      "Epoch 218/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1596 - accuracy: 0.9515 - val_loss: 0.2171 - val_accuracy: 0.9314\n",
      "Epoch 219/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1594 - accuracy: 0.9517 - val_loss: 0.2210 - val_accuracy: 0.9314\n",
      "Epoch 220/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1586 - accuracy: 0.9519 - val_loss: 0.2169 - val_accuracy: 0.9324\n",
      "Epoch 221/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1584 - accuracy: 0.9524 - val_loss: 0.2162 - val_accuracy: 0.9314\n",
      "Epoch 222/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1575 - accuracy: 0.9524 - val_loss: 0.2184 - val_accuracy: 0.9310\n",
      "Epoch 223/350\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1569 - accuracy: 0.9527 - val_loss: 0.2187 - val_accuracy: 0.9298\n",
      "Epoch 224/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1560 - accuracy: 0.9528 - val_loss: 0.2148 - val_accuracy: 0.9326\n",
      "Epoch 225/350\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1557 - accuracy: 0.9529 - val_loss: 0.2197 - val_accuracy: 0.9311\n",
      "Epoch 226/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1552 - accuracy: 0.9531 - val_loss: 0.2157 - val_accuracy: 0.9317\n",
      "Epoch 227/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1548 - accuracy: 0.9536 - val_loss: 0.2158 - val_accuracy: 0.9321\n",
      "Epoch 228/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.1542 - accuracy: 0.9533 - val_loss: 0.2140 - val_accuracy: 0.9325\n",
      "Epoch 229/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1539 - accuracy: 0.9536 - val_loss: 0.2226 - val_accuracy: 0.9295\n",
      "Epoch 230/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1534 - accuracy: 0.9537 - val_loss: 0.2157 - val_accuracy: 0.9321\n",
      "Epoch 231/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1527 - accuracy: 0.9539 - val_loss: 0.2131 - val_accuracy: 0.9316\n",
      "Epoch 232/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.1525 - accuracy: 0.9542 - val_loss: 0.2115 - val_accuracy: 0.9329\n",
      "Epoch 233/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1515 - accuracy: 0.9543 - val_loss: 0.2151 - val_accuracy: 0.9325\n",
      "Epoch 234/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.1516 - accuracy: 0.9544 - val_loss: 0.2115 - val_accuracy: 0.9326\n",
      "Epoch 235/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1505 - accuracy: 0.9549 - val_loss: 0.2127 - val_accuracy: 0.9329\n",
      "Epoch 236/350\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1505 - accuracy: 0.9546 - val_loss: 0.2118 - val_accuracy: 0.9329\n",
      "Epoch 237/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1497 - accuracy: 0.9547 - val_loss: 0.2115 - val_accuracy: 0.9326\n",
      "Epoch 238/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1496 - accuracy: 0.9548 - val_loss: 0.2141 - val_accuracy: 0.9323\n",
      "Epoch 239/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1486 - accuracy: 0.9552 - val_loss: 0.2121 - val_accuracy: 0.9325\n",
      "Epoch 240/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1483 - accuracy: 0.9552 - val_loss: 0.2108 - val_accuracy: 0.9335\n",
      "Epoch 241/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1479 - accuracy: 0.9554 - val_loss: 0.2104 - val_accuracy: 0.9337\n",
      "Epoch 242/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1474 - accuracy: 0.9554 - val_loss: 0.2101 - val_accuracy: 0.9329\n",
      "Epoch 243/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1468 - accuracy: 0.9561 - val_loss: 0.2091 - val_accuracy: 0.9343\n",
      "Epoch 244/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1463 - accuracy: 0.9566 - val_loss: 0.2114 - val_accuracy: 0.9337\n",
      "Epoch 245/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1458 - accuracy: 0.9560 - val_loss: 0.2085 - val_accuracy: 0.9349\n",
      "Epoch 246/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1458 - accuracy: 0.9562 - val_loss: 0.2140 - val_accuracy: 0.9313\n",
      "Epoch 247/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1451 - accuracy: 0.9565 - val_loss: 0.2103 - val_accuracy: 0.9340\n",
      "Epoch 248/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1443 - accuracy: 0.9571 - val_loss: 0.2103 - val_accuracy: 0.9341\n",
      "Epoch 249/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1442 - accuracy: 0.9568 - val_loss: 0.2090 - val_accuracy: 0.9342\n",
      "Epoch 250/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1435 - accuracy: 0.9569 - val_loss: 0.2071 - val_accuracy: 0.9348\n",
      "Epoch 251/350\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1436 - accuracy: 0.9571 - val_loss: 0.2278 - val_accuracy: 0.9265\n",
      "Epoch 252/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1427 - accuracy: 0.9572 - val_loss: 0.2148 - val_accuracy: 0.9316\n",
      "Epoch 253/350\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1423 - accuracy: 0.9572 - val_loss: 0.2080 - val_accuracy: 0.9339\n",
      "Epoch 254/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1416 - accuracy: 0.9577 - val_loss: 0.2094 - val_accuracy: 0.9328\n",
      "Epoch 255/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1409 - accuracy: 0.9578 - val_loss: 0.2112 - val_accuracy: 0.9323\n",
      "Epoch 256/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1406 - accuracy: 0.9580 - val_loss: 0.2091 - val_accuracy: 0.9333\n",
      "Epoch 257/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1408 - accuracy: 0.9576 - val_loss: 0.2063 - val_accuracy: 0.9352\n",
      "Epoch 258/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1400 - accuracy: 0.9583 - val_loss: 0.2065 - val_accuracy: 0.9346\n",
      "Epoch 259/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1395 - accuracy: 0.9584 - val_loss: 0.2082 - val_accuracy: 0.9340\n",
      "Epoch 260/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1392 - accuracy: 0.9585 - val_loss: 0.2092 - val_accuracy: 0.9341\n",
      "Epoch 261/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1387 - accuracy: 0.9587 - val_loss: 0.2092 - val_accuracy: 0.9345\n",
      "Epoch 262/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1384 - accuracy: 0.9591 - val_loss: 0.2078 - val_accuracy: 0.9349\n",
      "Epoch 263/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1380 - accuracy: 0.9588 - val_loss: 0.2138 - val_accuracy: 0.9320\n",
      "Epoch 264/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1372 - accuracy: 0.9593 - val_loss: 0.2087 - val_accuracy: 0.9328\n",
      "Epoch 265/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1371 - accuracy: 0.9590 - val_loss: 0.2054 - val_accuracy: 0.9351\n",
      "Epoch 266/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1364 - accuracy: 0.9596 - val_loss: 0.2055 - val_accuracy: 0.9352\n",
      "Epoch 267/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1363 - accuracy: 0.9593 - val_loss: 0.2037 - val_accuracy: 0.9354\n",
      "Epoch 268/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1360 - accuracy: 0.9596 - val_loss: 0.2038 - val_accuracy: 0.9356\n",
      "Epoch 269/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1354 - accuracy: 0.9599 - val_loss: 0.2061 - val_accuracy: 0.9347\n",
      "Epoch 270/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1351 - accuracy: 0.9597 - val_loss: 0.2040 - val_accuracy: 0.9354\n",
      "Epoch 271/350\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1347 - accuracy: 0.9598 - val_loss: 0.2025 - val_accuracy: 0.9366\n",
      "Epoch 272/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1344 - accuracy: 0.9602 - val_loss: 0.2050 - val_accuracy: 0.9347\n",
      "Epoch 273/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1338 - accuracy: 0.9604 - val_loss: 0.2029 - val_accuracy: 0.9361\n",
      "Epoch 274/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1332 - accuracy: 0.9602 - val_loss: 0.2039 - val_accuracy: 0.9355\n",
      "Epoch 275/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1331 - accuracy: 0.9606 - val_loss: 0.2084 - val_accuracy: 0.9334\n",
      "Epoch 276/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.1325 - accuracy: 0.9609 - val_loss: 0.2014 - val_accuracy: 0.9363\n",
      "Epoch 277/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1327 - accuracy: 0.9605 - val_loss: 0.2039 - val_accuracy: 0.9356\n",
      "Epoch 278/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1320 - accuracy: 0.9610 - val_loss: 0.2027 - val_accuracy: 0.9357\n",
      "Epoch 279/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1317 - accuracy: 0.9610 - val_loss: 0.2053 - val_accuracy: 0.9356\n",
      "Epoch 280/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1308 - accuracy: 0.9611 - val_loss: 0.2013 - val_accuracy: 0.9360\n",
      "Epoch 281/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1307 - accuracy: 0.9614 - val_loss: 0.2027 - val_accuracy: 0.9360\n",
      "Epoch 282/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1304 - accuracy: 0.9613 - val_loss: 0.2045 - val_accuracy: 0.9357\n",
      "Epoch 283/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1298 - accuracy: 0.9615 - val_loss: 0.2037 - val_accuracy: 0.9362\n",
      "Epoch 284/350\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1297 - accuracy: 0.9616 - val_loss: 0.2046 - val_accuracy: 0.9352\n",
      "Epoch 285/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1294 - accuracy: 0.9618 - val_loss: 0.2014 - val_accuracy: 0.9368\n",
      "Epoch 286/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1285 - accuracy: 0.9619 - val_loss: 0.1999 - val_accuracy: 0.9373\n",
      "Epoch 287/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1285 - accuracy: 0.9623 - val_loss: 0.2033 - val_accuracy: 0.9361\n",
      "Epoch 288/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.1282 - accuracy: 0.9622 - val_loss: 0.2057 - val_accuracy: 0.9343\n",
      "Epoch 289/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1277 - accuracy: 0.9626 - val_loss: 0.2010 - val_accuracy: 0.9365\n",
      "Epoch 290/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1274 - accuracy: 0.9623 - val_loss: 0.2029 - val_accuracy: 0.9363\n",
      "Epoch 291/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1268 - accuracy: 0.9627 - val_loss: 0.2012 - val_accuracy: 0.9365\n",
      "Epoch 292/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.1265 - accuracy: 0.9630 - val_loss: 0.2010 - val_accuracy: 0.9360\n",
      "Epoch 293/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1263 - accuracy: 0.9630 - val_loss: 0.2015 - val_accuracy: 0.9368\n",
      "Epoch 294/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1258 - accuracy: 0.9632 - val_loss: 0.2018 - val_accuracy: 0.9353\n",
      "Epoch 295/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1253 - accuracy: 0.9633 - val_loss: 0.1993 - val_accuracy: 0.9376\n",
      "Epoch 296/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1253 - accuracy: 0.9630 - val_loss: 0.2012 - val_accuracy: 0.9362\n",
      "Epoch 297/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1248 - accuracy: 0.9635 - val_loss: 0.2019 - val_accuracy: 0.9349\n",
      "Epoch 298/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1243 - accuracy: 0.9635 - val_loss: 0.1991 - val_accuracy: 0.9366\n",
      "Epoch 299/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1241 - accuracy: 0.9632 - val_loss: 0.1992 - val_accuracy: 0.9370\n",
      "Epoch 300/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1235 - accuracy: 0.9639 - val_loss: 0.1998 - val_accuracy: 0.9366\n",
      "Epoch 301/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1232 - accuracy: 0.9636 - val_loss: 0.1990 - val_accuracy: 0.9371\n",
      "Epoch 302/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1231 - accuracy: 0.9640 - val_loss: 0.1999 - val_accuracy: 0.9366\n",
      "Epoch 303/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1226 - accuracy: 0.9642 - val_loss: 0.2018 - val_accuracy: 0.9357\n",
      "Epoch 304/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.1224 - accuracy: 0.9641 - val_loss: 0.2039 - val_accuracy: 0.9356\n",
      "Epoch 305/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1221 - accuracy: 0.9644 - val_loss: 0.1989 - val_accuracy: 0.9367\n",
      "Epoch 306/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1219 - accuracy: 0.9643 - val_loss: 0.1992 - val_accuracy: 0.9373\n",
      "Epoch 307/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1215 - accuracy: 0.9646 - val_loss: 0.2007 - val_accuracy: 0.9367\n",
      "Epoch 308/350\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1210 - accuracy: 0.9645 - val_loss: 0.2039 - val_accuracy: 0.9348\n",
      "Epoch 309/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1209 - accuracy: 0.9645 - val_loss: 0.1973 - val_accuracy: 0.9378\n",
      "Epoch 310/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1202 - accuracy: 0.9648 - val_loss: 0.1977 - val_accuracy: 0.9373\n",
      "Epoch 311/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1201 - accuracy: 0.9653 - val_loss: 0.1979 - val_accuracy: 0.9373\n",
      "Epoch 312/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1194 - accuracy: 0.9650 - val_loss: 0.2001 - val_accuracy: 0.9367\n",
      "Epoch 313/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1193 - accuracy: 0.9651 - val_loss: 0.1984 - val_accuracy: 0.9378\n",
      "Epoch 314/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1186 - accuracy: 0.9656 - val_loss: 0.1998 - val_accuracy: 0.9361\n",
      "Epoch 315/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1190 - accuracy: 0.9654 - val_loss: 0.1969 - val_accuracy: 0.9382\n",
      "Epoch 316/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1185 - accuracy: 0.9656 - val_loss: 0.1976 - val_accuracy: 0.9376\n",
      "Epoch 317/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1182 - accuracy: 0.9654 - val_loss: 0.1984 - val_accuracy: 0.9369\n",
      "Epoch 318/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.1178 - accuracy: 0.9659 - val_loss: 0.2007 - val_accuracy: 0.9362\n",
      "Epoch 319/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1174 - accuracy: 0.9660 - val_loss: 0.1964 - val_accuracy: 0.9380\n",
      "Epoch 320/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1170 - accuracy: 0.9661 - val_loss: 0.1975 - val_accuracy: 0.9376\n",
      "Epoch 321/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1170 - accuracy: 0.9661 - val_loss: 0.1963 - val_accuracy: 0.9381\n",
      "Epoch 322/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1166 - accuracy: 0.9660 - val_loss: 0.1973 - val_accuracy: 0.9368\n",
      "Epoch 323/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1165 - accuracy: 0.9663 - val_loss: 0.1988 - val_accuracy: 0.9376\n",
      "Epoch 324/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.1158 - accuracy: 0.9665 - val_loss: 0.1984 - val_accuracy: 0.9364\n",
      "Epoch 325/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1155 - accuracy: 0.9667 - val_loss: 0.1960 - val_accuracy: 0.9381\n",
      "Epoch 326/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.1154 - accuracy: 0.9664 - val_loss: 0.1968 - val_accuracy: 0.9373\n",
      "Epoch 327/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1148 - accuracy: 0.9665 - val_loss: 0.1993 - val_accuracy: 0.9378\n",
      "Epoch 328/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1147 - accuracy: 0.9667 - val_loss: 0.1966 - val_accuracy: 0.9376\n",
      "Epoch 329/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1145 - accuracy: 0.9669 - val_loss: 0.1984 - val_accuracy: 0.9367\n",
      "Epoch 330/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1140 - accuracy: 0.9671 - val_loss: 0.1973 - val_accuracy: 0.9376\n",
      "Epoch 331/350\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1138 - accuracy: 0.9670 - val_loss: 0.1974 - val_accuracy: 0.9374\n",
      "Epoch 332/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1138 - accuracy: 0.9670 - val_loss: 0.2013 - val_accuracy: 0.9372\n",
      "Epoch 333/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1132 - accuracy: 0.9677 - val_loss: 0.1969 - val_accuracy: 0.9376\n",
      "Epoch 334/350\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1127 - accuracy: 0.9675 - val_loss: 0.1981 - val_accuracy: 0.9377\n",
      "Epoch 335/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1122 - accuracy: 0.9673 - val_loss: 0.1953 - val_accuracy: 0.9380\n",
      "Epoch 336/350\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1123 - accuracy: 0.9675 - val_loss: 0.1964 - val_accuracy: 0.9384\n",
      "Epoch 337/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1120 - accuracy: 0.9676 - val_loss: 0.1967 - val_accuracy: 0.9375\n",
      "Epoch 338/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1113 - accuracy: 0.9681 - val_loss: 0.1966 - val_accuracy: 0.9378\n",
      "Epoch 339/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1114 - accuracy: 0.9681 - val_loss: 0.1964 - val_accuracy: 0.9385\n",
      "Epoch 340/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1109 - accuracy: 0.9680 - val_loss: 0.1972 - val_accuracy: 0.9374\n",
      "Epoch 341/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1112 - accuracy: 0.9678 - val_loss: 0.1955 - val_accuracy: 0.9383\n",
      "Epoch 342/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1104 - accuracy: 0.9683 - val_loss: 0.2040 - val_accuracy: 0.9358\n",
      "Epoch 343/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.1102 - accuracy: 0.9682 - val_loss: 0.1964 - val_accuracy: 0.9375\n",
      "Epoch 344/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1098 - accuracy: 0.9687 - val_loss: 0.1966 - val_accuracy: 0.9379\n",
      "Epoch 345/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1093 - accuracy: 0.9686 - val_loss: 0.1963 - val_accuracy: 0.9378\n",
      "Epoch 346/350\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1094 - accuracy: 0.9686 - val_loss: 0.1958 - val_accuracy: 0.9383\n",
      "Epoch 347/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.1091 - accuracy: 0.9685 - val_loss: 0.1998 - val_accuracy: 0.9370\n",
      "Epoch 348/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1086 - accuracy: 0.9688 - val_loss: 0.1949 - val_accuracy: 0.9384\n",
      "Epoch 349/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1084 - accuracy: 0.9692 - val_loss: 0.1945 - val_accuracy: 0.9388\n",
      "Epoch 350/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1081 - accuracy: 0.9695 - val_loss: 0.1959 - val_accuracy: 0.9380\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_converted, Y_train, steps_per_epoch=500, epochs=number_of_epoch, verbose=1, validation_data=(X_val_converted, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ZiP3T6FFXd7d"
   },
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znTJrSIDMpoM"
   },
   "source": [
    "## Loss_plots and Accuracy_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T05:20:51.539387Z",
     "iopub.status.busy": "2023-04-02T05:20:51.539042Z",
     "iopub.status.idle": "2023-04-02T05:20:51.547217Z",
     "shell.execute_reply": "2023-04-02T05:20:51.545792Z",
     "shell.execute_reply.started": "2023-04-02T05:20:51.539355Z"
    },
    "id": "I0EySZwGMpoM"
   },
   "outputs": [],
   "source": [
    "#creating a list for no of epochs\n",
    "n_epochs = list(range(1,number_of_epoch+1))\n",
    "\n",
    "#creating a dataframe of loss and accuracy values\n",
    "history_details_df = pd.DataFrame({\"Number of epochs\":n_epochs,\"Train_Accuracy\":history.history['accuracy'],\"Validation_accuracy\":history.history['val_accuracy'],\"Train_Loss\":history.history['loss'],\"Validation_loss\":history.history['val_loss']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:20:51.549581Z",
     "iopub.status.busy": "2023-04-02T05:20:51.549167Z",
     "iopub.status.idle": "2023-04-02T05:20:51.682828Z",
     "shell.execute_reply": "2023-04-02T05:20:51.681378Z",
     "shell.execute_reply.started": "2023-04-02T05:20:51.549545Z"
    },
    "id": "K9csbdpEMpoM",
    "outputId": "88b6686e-5871-449e-81c5-1e221930fc62"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"148c6940-6538-49b6-85a2-b2f9d85db889\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"148c6940-6538-49b6-85a2-b2f9d85db889\")) {                    Plotly.newPlot(                        \"148c6940-6538-49b6-85a2-b2f9d85db889\",                        [{\"hovertemplate\":\"variable=Train_Accuracy<br>Number of epochs=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"Train_Accuracy\",\"line\":{\"color\":\"rgb(27,158,119)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Train_Accuracy\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350],\"xaxis\":\"x\",\"y\":[0.07983473688364029,0.21363084018230438,0.31345701217651367,0.360816091299057,0.3985842764377594,0.4335603415966034,0.468328058719635,0.5050246715545654,0.5371490716934204,0.5668920874595642,0.5922534465789795,0.6138165593147278,0.6339985132217407,0.6511918902397156,0.6680101156234741,0.6826554536819458,0.6950682401657104,0.7073976397514343,0.7185781002044678,0.7288060188293457,0.7383075952529907,0.7466601729393005,0.7541078329086304,0.7612459063529968,0.7678065299987793,0.773581326007843,0.7782964110374451,0.783440113067627,0.7878039479255676,0.7927213907241821,0.7957278490066528,0.8002643585205078,0.8039435148239136,0.8072357177734375,0.8103850483894348,0.8141416311264038,0.8167372941970825,0.8198747634887695,0.8216726779937744,0.8249112963676453,0.828465461730957,0.830233633518219,0.8326089978218079,0.8352880477905273,0.8374788761138916,0.8398780822753906,0.8424856662750244,0.8443252444267273,0.8467839956283569,0.8489927053451538,0.8506953716278076,0.8522908687591553,0.8554878234863281,0.8567975759506226,0.8583394885063171,0.8597623705863953,0.8620246052742004,0.8635486960411072,0.8658407330513,0.8668766021728516,0.8684304356575012,0.8706153631210327,0.8722108602523804,0.87376469373703,0.8740861415863037,0.8768485188484192,0.8785274028778076,0.8788548111915588,0.8809563517570496,0.8816588521003723,0.8829864263534546,0.8843140602111816,0.8861000537872314,0.8869990110397339,0.8885350227355957,0.8893386721611023,0.8906722664833069,0.8918153047561646,0.8931905031204224,0.893756091594696,0.8952503800392151,0.896101713180542,0.8972566723823547,0.8982627987861633,0.8994296789169312,0.8997511267662048,0.9009954333305359,0.9019598364830017,0.9027635455131531,0.9033231735229492,0.9046388864517212,0.9049365520477295,0.9057283401489258,0.9064963459968567,0.9073535799980164,0.9084430932998657,0.909151554107666,0.9098361730575562,0.9109851717948914,0.9113721251487732,0.9116697907447815,0.9126045107841492,0.9130390882492065,0.9142833352088928,0.9142357110977173,0.9152596592903137,0.9162420034408569,0.9164265394210815,0.9168134927749634,0.9179982542991638,0.9180875420570374,0.9189448356628418,0.9196592569351196,0.9201295375823975,0.9205760359764099,0.921123743057251,0.921534538269043,0.9217666983604431,0.9227132797241211,0.9228383302688599,0.9234157800674438,0.9237372875213623,0.9242849946022034,0.9248625040054321,0.9252256155014038,0.9260769486427307,0.9256959557533264,0.926249623298645,0.9271783232688904,0.9274819493293762,0.9278689026832581,0.9282082915306091,0.928678572177887,0.9286131262779236,0.929089367389679,0.929512083530426,0.9303038716316223,0.9307682514190674,0.9308575391769409,0.9314349889755249,0.9312801957130432,0.9320065379142761,0.9317684173583984,0.9326018691062927,0.9326614141464233,0.9334591627120972,0.9337806105613708,0.9342509508132935,0.9343461990356445,0.9346379041671753,0.9349415302276611,0.9357631206512451,0.9359595775604248,0.9364596605300903,0.9368109107017517,0.9366739988327026,0.9375967383384705,0.9375788569450378,0.9379242062568665,0.938055157661438,0.9382635354995728,0.9384361505508423,0.9389481544494629,0.9392458200454712,0.939549446105957,0.9393351078033447,0.9400733709335327,0.9403293132781982,0.9410259127616882,0.9411628246307373,0.9416093230247498,0.9413116574287415,0.9416450262069702,0.9418593645095825,0.942728579044342,0.9431214928627014,0.9428178668022156,0.9433774948120117,0.942936897277832,0.9435977339744568,0.9439073204994202,0.9434905648231506,0.9441930651664734,0.9452230334281921,0.944663405418396,0.9449967741966248,0.9451634883880615,0.9453896880149841,0.9457528591156006,0.94614577293396,0.9458064436912537,0.9464017748832703,0.9470506906509399,0.9467887282371521,0.9466220736503601,0.9474912285804749,0.9471399784088135,0.9478127360343933,0.9478246569633484,0.9480865597724915,0.9484795331954956,0.948586642742157,0.9488486051559448,0.9491046071052551,0.9492177367210388,0.9494855999946594,0.9494796991348267,0.9498368501663208,0.9498785734176636,0.9503964781761169,0.9505751132965088,0.9501166939735413,0.9509144425392151,0.9509263634681702,0.9512597322463989,0.9512597322463989,0.9516883492469788,0.9514681100845337,0.9516526460647583,0.9519205689430237,0.9523849487304688,0.9524027705192566,0.9527361989021301,0.9528433084487915,0.9529147744178772,0.9530933499336243,0.953629195690155,0.9533315300941467,0.9535517692565918,0.9537363648414612,0.9538792371749878,0.954164981842041,0.9543078541755676,0.9544150233268738,0.9549091458320618,0.9545817375183105,0.9547305703163147,0.9548079371452332,0.9552425146102905,0.9551770687103271,0.9554330706596375,0.9554449319839478,0.956099808216095,0.9566415548324585,0.9560402631759644,0.9562426805496216,0.9564749002456665,0.9571059346199036,0.9567546844482422,0.9569094777107239,0.957141637802124,0.9571952223777771,0.9572131037712097,0.9577310681343079,0.9578441381454468,0.957963228225708,0.9576238989830017,0.9583263993263245,0.9583978056907654,0.9585466384887695,0.9587073922157288,0.9591122269630432,0.9588324427604675,0.9593086838722229,0.9590229392051697,0.9596182703971863,0.9592908620834351,0.9596301913261414,0.9598861932754517,0.9597373604774475,0.9597551822662354,0.9601719379425049,0.9603922367095947,0.9602314829826355,0.9606184363365173,0.9608803987503052,0.9604517221450806,0.9610292315483093,0.9609637260437012,0.9611125588417053,0.9613626003265381,0.9613030552864075,0.9615114331245422,0.9615530967712402,0.9617555141448975,0.9618746042251587,0.9622794389724731,0.9622496366500854,0.962583065032959,0.9622913599014282,0.9626961350440979,0.9629580974578857,0.9629878997802734,0.9631962180137634,0.9633391499519348,0.9629759788513184,0.963541567325592,0.9635058045387268,0.963160514831543,0.9638510942459106,0.9636308550834656,0.9639642238616943,0.9642202258110046,0.964065432548523,0.964392900466919,0.9643035531044006,0.9646250605583191,0.9644583463668823,0.9645417332649231,0.9648155570030212,0.9652620553970337,0.9650000929832458,0.9650596380233765,0.965559720993042,0.9654466509819031,0.9656371474266052,0.9653573036193848,0.9658514261245728,0.9660062193870544,0.9661193490028381,0.9660717248916626,0.9659883975982666,0.9662681818008423,0.966518223285675,0.9666610956192017,0.9663693904876709,0.9665480256080627,0.9666968584060669,0.9668635129928589,0.9671373963356018,0.9670302271842957,0.9669826030731201,0.967697024345398,0.967464804649353,0.9673219323158264,0.9674707651138306,0.9675660133361816,0.9681017994880676,0.9681017994880676,0.9679827690124512,0.9677505493164062,0.9682506322860718,0.9682089686393738,0.9686912298202515,0.9686018824577332,0.9685900211334229,0.9684590101242065,0.9688042998313904,0.9692448973655701,0.9694830179214478],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=Validation_accuracy<br>Number of epochs=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"Validation_accuracy\",\"line\":{\"color\":\"rgb(217,95,2)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Validation_accuracy\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350],\"xaxis\":\"x\",\"y\":[0.14149640500545502,0.27946850657463074,0.339905709028244,0.3786255121231079,0.414773553609848,0.44861170649528503,0.4856407940387726,0.5168119072914124,0.5528408885002136,0.576653778553009,0.6027765870094299,0.6240177154541016,0.6413059234619141,0.6586655378341675,0.6740010380744934,0.6906462907791138,0.703052818775177,0.7133162021636963,0.7239367365837097,0.7324855923652649,0.7417250275611877,0.7496547102928162,0.7558460831642151,0.7631566524505615,0.7676572799682617,0.7719912528991699,0.77646803855896,0.7807543873786926,0.7860170602798462,0.7900652289390564,0.7929227948188782,0.7980663776397705,0.8001381158828735,0.8043292164802551,0.8073772192001343,0.8069486021995544,0.8118540644645691,0.8160213232040405,0.8168309926986694,0.8222126960754395,0.8243558406829834,0.826022744178772,0.8284278512001038,0.8309282064437866,0.8310472965240479,0.8357384204864502,0.8362385034561157,0.8392627239227295,0.8404057621955872,0.84426349401474,0.8453112244606018,0.845335066318512,0.8477877974510193,0.849811851978302,0.8522169589996338,0.854479193687439,0.85457444190979,0.8582178354263306,0.8555983901023865,0.8602895736694336,0.8628613352775574,0.8626470565795898,0.8617183566093445,0.8664571046829224,0.8683621287345886,0.8695051670074463,0.8715292811393738,0.8729342222213745,0.8727198839187622,0.8740057945251465,0.8760537505149841,0.8758631944656372,0.8786969780921936,0.8779587745666504,0.88017338514328,0.8808639049530029,0.882768988609314,0.8825070261955261,0.884102463722229,0.8809829950332642,0.884388267993927,0.8791970014572144,0.8807210326194763,0.8849121332168579,0.8887221813201904,0.8896985054016113,0.8909606337547302,0.889222264289856,0.8932942748069763,0.8931514024734497,0.8933418989181519,0.8954374194145203,0.8937943577766418,0.8964614272117615,0.889222264289856,0.8978187441825867,0.8980092406272888,0.8947230577468872,0.8995094299316406,0.9003428816795349,0.9007953405380249,0.9009144306182861,0.8999381065368652,0.901795506477356,0.9012715816497803,0.9028908610343933,0.9039386510848999,0.9015335440635681,0.9009620547294617,0.9060817956924438,0.9060817956924438,0.9062247276306152,0.9063437581062317,0.902247965335846,0.9081059098243713,0.9078677892684937,0.9078677892684937,0.9064866304397583,0.9090822339057922,0.9086059927940369,0.9101776480674744,0.9067723751068115,0.9109634757041931,0.910939633846283,0.9093441963195801,0.911225438117981,0.9101776480674744,0.9118921756744385,0.9130828380584717,0.9101061820983887,0.9146782755851746,0.9131304621696472,0.9142258167266846,0.9142734408378601,0.9119874238967896,0.9138448238372803,0.9127256274223328,0.9158450961112976,0.9140591621398926,0.9165832996368408,0.9139876961708069,0.9154641032218933,0.9161070585250854,0.914082944393158,0.9182740449905396,0.9187502861022949,0.9181311726570129,0.9176549315452576,0.9187026619911194,0.9189884066581726,0.9205362796783447,0.9189407825469971,0.9204171895980835,0.9178930521011353,0.9216792583465576,0.9216316342353821,0.9220603108406067,0.920369565486908,0.9215602278709412,0.9225127696990967,0.9212030172348022,0.923346221446991,0.9225127696990967,0.9223222136497498,0.9237033724784851,0.9236795902252197,0.9237033724784851,0.9242272973060608,0.9241082072257996,0.9234890937805176,0.9247749447822571,0.9244654178619385,0.9230366349220276,0.9209411144256592,0.924679696559906,0.9263466000556946,0.9265370965003967,0.9262275695800781,0.9257751107215881,0.9255369901657104,0.9273467659950256,0.9239177107810974,0.9270848035812378,0.9256083965301514,0.9267038106918335,0.9265133142471313,0.9232509136199951,0.9277039766311646,0.9277516007423401,0.9270133972167969,0.9279897212982178,0.9282993078231812,0.9270133972167969,0.9283230900764465,0.928894579410553,0.9296327829360962,0.9286088347434998,0.9290136694908142,0.9298709630966187,0.929275631904602,0.9281325936317444,0.9291565418243408,0.9302757382392883,0.9294661283493042,0.9299662113189697,0.9294185042381287,0.9304424524307251,0.9302995800971985,0.9299662113189697,0.9292994141578674,0.9289184212684631,0.9307758212089539,0.9301328659057617,0.9304662346839905,0.9300138354301453,0.9297518730163574,0.9281087517738342,0.931418776512146,0.9313949346542358,0.932418942451477,0.931418776512146,0.930966317653656,0.9298471212387085,0.9326094388961792,0.9311330318450928,0.9317045211791992,0.9321093559265137,0.9325141906738281,0.9295375347137451,0.9320617318153381,0.9316331148147583,0.932942807674408,0.9325141906738281,0.9326094388961792,0.9329189658164978,0.932942807674408,0.932585597038269,0.9323474764823914,0.9325379729270935,0.9335381388664246,0.9336810111999512,0.9328713417053223,0.9343477487564087,0.9336810111999512,0.9348716735839844,0.9312759041786194,0.9339667558670044,0.934109628200531,0.9341572523117065,0.9348478317260742,0.9264894723892212,0.9315616488456726,0.9339191317558289,0.9327523112297058,0.932323694229126,0.9333476424217224,0.9352288246154785,0.9346097111701965,0.9340143799781799,0.9340858459472656,0.9344906210899353,0.9349192976951599,0.9320141077041626,0.9327760934829712,0.9350621700286865,0.935181200504303,0.9354431629180908,0.9355860352516174,0.9346573352813721,0.9353716969490051,0.936633825302124,0.934728741645813,0.936086118221283,0.9355145692825317,0.9334190487861633,0.936348021030426,0.9355860352516174,0.9356575012207031,0.935633659362793,0.9360384941101074,0.9359670281410217,0.9356812834739685,0.936181366443634,0.9352050423622131,0.936800479888916,0.9373481869697571,0.9361337423324585,0.9343477487564087,0.9364671111106873,0.9362766146659851,0.9364909529685974,0.9360146522521973,0.9367528557777405,0.935276448726654,0.9376339316368103,0.936181366443634,0.9349192976951599,0.936633825302124,0.9370148181915283,0.9366099834442139,0.9371338486671448,0.9365862011909485,0.9357051253318787,0.9355860352516174,0.9366576075553894,0.9372767806053162,0.9367290735244751,0.9347764253616333,0.9378482699394226,0.9373244047164917,0.9373481869697571,0.9366576075553894,0.9378244280815125,0.9361099004745483,0.9382292628288269,0.9375863075256348,0.9368719458580017,0.936181366443634,0.9379673004150391,0.9375863075256348,0.9381340146064758,0.9367528557777405,0.9375863075256348,0.9364194869995117,0.9381102323532104,0.9373005628585815,0.9377530217170715,0.9375625252723694,0.9367290735244751,0.9375625252723694,0.9373958110809326,0.9372291564941406,0.9375625252723694,0.9376815557479858,0.9379911422729492,0.9384436011314392,0.9375386834144592,0.9377530217170715,0.9384912252426147,0.9373958110809326,0.938324511051178,0.9358241558074951,0.9375386834144592,0.937872052192688,0.9378244280815125,0.938324511051178,0.9370148181915283,0.938419759273529,0.9388007521629333,0.9380387663841248],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Number of epochs\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Accuracy plot\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('148c6940-6538-49b6-85a2-b2f9d85db889');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.line(history_details_df,x='Number of epochs',y=history_details_df.columns[1:3],color_discrete_sequence=px.colors.qualitative.Dark2,title='Accuracy plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:20:51.685052Z",
     "iopub.status.busy": "2023-04-02T05:20:51.684655Z",
     "iopub.status.idle": "2023-04-02T05:20:51.76061Z",
     "shell.execute_reply": "2023-04-02T05:20:51.759213Z",
     "shell.execute_reply.started": "2023-04-02T05:20:51.685017Z"
    },
    "id": "tY8VY4xwMpoM",
    "outputId": "239806ee-5611-48f7-abf9-f7de41fd30b6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"aac7b923-fe0d-4f3f-ac7c-b1a0a5776373\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"aac7b923-fe0d-4f3f-ac7c-b1a0a5776373\")) {                    Plotly.newPlot(                        \"aac7b923-fe0d-4f3f-ac7c-b1a0a5776373\",                        [{\"hovertemplate\":\"variable=Train_Loss<br>Number of epochs=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"Train_Loss\",\"line\":{\"color\":\"rgb(133, 92, 117)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Train_Loss\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350],\"xaxis\":\"x\",\"y\":[3.4703426361083984,2.8878307342529297,2.457624673843384,2.2557356357574463,2.1096601486206055,1.9768871068954468,1.8455946445465088,1.716345191001892,1.5954954624176025,1.4874811172485352,1.3927708864212036,1.3097187280654907,1.2362667322158813,1.1706230640411377,1.1118011474609375,1.0590741634368896,1.0116647481918335,0.9686728715896606,0.9298692345619202,0.8950104117393494,0.8631702661514282,0.8342262506484985,0.8081048130989075,0.7841328978538513,0.762031614780426,0.7418618202209473,0.7227592468261719,0.7056122422218323,0.6895970106124878,0.6740935444831848,0.6602308750152588,0.6469296813011169,0.6340413689613342,0.6219314336776733,0.6101243495941162,0.5990549325942993,0.589129626750946,0.5795255303382874,0.5703701376914978,0.5610503554344177,0.5513206124305725,0.5436660647392273,0.5353488326072693,0.5276885628700256,0.5194635391235352,0.5122232437133789,0.5053175687789917,0.49790260195732117,0.49103933572769165,0.4843868613243103,0.47850269079208374,0.4719886779785156,0.46555405855178833,0.4599701166152954,0.4537300169467926,0.4486578404903412,0.44286325573921204,0.43759965896606445,0.43199944496154785,0.4271853268146515,0.4213695228099823,0.4163435697555542,0.4107331335544586,0.40648195147514343,0.4024570882320404,0.3961670398712158,0.39227500557899475,0.387545108795166,0.38300344347953796,0.38007742166519165,0.3751259744167328,0.37107130885124207,0.3672555088996887,0.3637513518333435,0.35906553268432617,0.3553794026374817,0.3524525463581085,0.3480248749256134,0.34579789638519287,0.3422354459762573,0.33781588077545166,0.33505338430404663,0.3311694264411926,0.3288455307483673,0.32537686824798584,0.32346293330192566,0.3197605013847351,0.31674906611442566,0.31445345282554626,0.3121053874492645,0.30843088030815125,0.30681878328323364,0.30440831184387207,0.3014209568500519,0.2986783981323242,0.2962524890899658,0.293946236371994,0.2912437915802002,0.28883111476898193,0.28723499178886414,0.28574660420417786,0.2827986776828766,0.28090929985046387,0.2781614065170288,0.2767547070980072,0.2739916145801544,0.2730032205581665,0.2711278796195984,0.26886144280433655,0.2671295404434204,0.2655566930770874,0.26279252767562866,0.26129579544067383,0.259568989276886,0.25719308853149414,0.25581294298171997,0.2546967566013336,0.25344446301460266,0.2514925003051758,0.24988336861133575,0.24852339923381805,0.24685657024383545,0.24524551630020142,0.2433730810880661,0.24249373376369476,0.24052949249744415,0.23959608376026154,0.23884114623069763,0.2362406849861145,0.23538944125175476,0.2338901311159134,0.2327588051557541,0.23140203952789307,0.23043954372406006,0.2289109230041504,0.2274802327156067,0.22606873512268066,0.22519871592521667,0.22401387989521027,0.22275742888450623,0.2217143177986145,0.22044959664344788,0.2194776087999344,0.21804027259349823,0.2174522876739502,0.21554048359394073,0.21449603140354156,0.21392443776130676,0.21272507309913635,0.21174059808254242,0.21071134507656097,0.20895177125930786,0.20854073762893677,0.20729245245456696,0.2059738039970398,0.20583297312259674,0.2037842720746994,0.20299218595027924,0.20287160575389862,0.20117290318012238,0.20066328346729279,0.20021529495716095,0.1990198940038681,0.1977282613515854,0.1967516541481018,0.19682875275611877,0.1952853798866272,0.1943918615579605,0.19365867972373962,0.1924435943365097,0.191330224275589,0.19123928248882294,0.1902225911617279,0.1891351342201233,0.18825863301753998,0.18758514523506165,0.18718251585960388,0.18602098524570465,0.18571363389492035,0.18436439335346222,0.1839560866355896,0.18367019295692444,0.18283629417419434,0.18115995824337006,0.18091782927513123,0.17980705201625824,0.1792854219675064,0.17879219353199005,0.17789040505886078,0.17724451422691345,0.17644380033016205,0.17626093327999115,0.17486818134784698,0.17465190589427948,0.1745772808790207,0.17316552996635437,0.17275787889957428,0.17163389921188354,0.1713961809873581,0.17049844563007355,0.16977831721305847,0.16936099529266357,0.168425053358078,0.1678178310394287,0.16755259037017822,0.16711702942848206,0.16601485013961792,0.16555289924144745,0.16527771949768066,0.16442394256591797,0.16333484649658203,0.16327103972434998,0.16286538541316986,0.16196991503238678,0.16079486906528473,0.16113024950027466,0.16031338274478912,0.15962305665016174,0.15938174724578857,0.1585875004529953,0.15843886137008667,0.15753090381622314,0.1568775177001953,0.15601269900798798,0.15572743117809296,0.15520192682743073,0.15482769906520844,0.1541990488767624,0.15393202006816864,0.15341609716415405,0.15270882844924927,0.15250055491924286,0.15150579810142517,0.15161839127540588,0.15053685009479523,0.150451198220253,0.1496991664171219,0.14962422847747803,0.14864473044872284,0.1482917070388794,0.14786969125270844,0.14736980199813843,0.1468007117509842,0.14628532528877258,0.14580489695072174,0.14581981301307678,0.1451234221458435,0.14431357383728027,0.14423570036888123,0.1435430645942688,0.14361701905727386,0.14273011684417725,0.14234523475170135,0.14161361753940582,0.14091260731220245,0.1405860036611557,0.1407867670059204,0.1399698406457901,0.13949905335903168,0.13924306631088257,0.13868512213230133,0.13842494785785675,0.1380012333393097,0.1372271031141281,0.13712982833385468,0.13642756640911102,0.1363036036491394,0.1360393613576889,0.13536593317985535,0.13511846959590912,0.1347380429506302,0.1343824416399002,0.13380268216133118,0.13321293890476227,0.13307249546051025,0.1325368732213974,0.13274608552455902,0.13204535841941833,0.13169004023075104,0.13083001971244812,0.13070325553417206,0.1304396390914917,0.12977401912212372,0.12970171868801117,0.1293892115354538,0.12851254642009735,0.12849119305610657,0.12819308042526245,0.12768425047397614,0.127371683716774,0.12684109807014465,0.12646198272705078,0.12625856697559357,0.12581174075603485,0.12530730664730072,0.12533855438232422,0.12476709485054016,0.12425444275140762,0.12405367195606232,0.12351533025503159,0.12324974685907364,0.12312446534633636,0.12261395156383514,0.12244465202093124,0.12214792519807816,0.12193033844232559,0.12146841734647751,0.12103258818387985,0.12090300768613815,0.12018521130084991,0.12005428969860077,0.11936728656291962,0.11931141465902328,0.11856108158826828,0.11900301277637482,0.11849414557218552,0.1182057335972786,0.11779086291790009,0.11743757873773575,0.11695291101932526,0.11695396900177002,0.11655930429697037,0.11645106971263885,0.11583231389522552,0.11545585095882416,0.11542012542486191,0.11482878029346466,0.11474496871232986,0.11454629898071289,0.11397732049226761,0.11376406252384186,0.11379177868366241,0.11317794024944305,0.1126864030957222,0.11223716288805008,0.11231622099876404,0.11202576011419296,0.11129933595657349,0.11142012476921082,0.11092886328697205,0.11123741418123245,0.11035583913326263,0.11015722900629044,0.10976944863796234,0.10932696610689163,0.10935023427009583,0.10910943895578384,0.10860658437013626,0.10836880654096603,0.10805795341730118],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=Validation_loss<br>Number of epochs=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"Validation_loss\",\"line\":{\"color\":\"rgb(217, 175, 107)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Validation_loss\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350],\"xaxis\":\"x\",\"y\":[3.2633426189422607,2.600216865539551,2.335538625717163,2.174294948577881,2.037991523742676,1.9069969654083252,1.7758760452270508,1.650233507156372,1.5359795093536377,1.4350093603134155,1.3461942672729492,1.268165111541748,1.2005678415298462,1.1376750469207764,1.0837785005569458,1.0335679054260254,0.988894522190094,0.9492751955986023,0.9151712656021118,0.8811183571815491,0.8530488610267639,0.823388934135437,0.8016342520713806,0.7771672010421753,0.757160484790802,0.7394341826438904,0.7222990989685059,0.7068681120872498,0.689858615398407,0.6762564182281494,0.6638378500938416,0.6488730907440186,0.6360043287277222,0.6271462440490723,0.6143273115158081,0.6123170852661133,0.5979743003845215,0.5839459896087646,0.5823299288749695,0.5671233534812927,0.5589125156402588,0.5526798963546753,0.5456172227859497,0.5354422330856323,0.5333609580993652,0.5216667056083679,0.5159744620323181,0.5085121989250183,0.5023886561393738,0.4949282705783844,0.48964858055114746,0.4866145849227905,0.47926148772239685,0.4781084656715393,0.4677855372428894,0.4627111554145813,0.4636121392250061,0.451445996761322,0.4516627788543701,0.44126033782958984,0.43584850430488586,0.4337666630744934,0.4351935386657715,0.42261090874671936,0.41923072934150696,0.4142285883426666,0.4086248278617859,0.40624329447746277,0.4051479995250702,0.40071532130241394,0.39453595876693726,0.3918152153491974,0.385763019323349,0.38672930002212524,0.3800794780254364,0.3793303966522217,0.37390536069869995,0.3720739781856537,0.3670787513256073,0.37204599380493164,0.3673691749572754,0.37189802527427673,0.3758186399936676,0.35940849781036377,0.35218527913093567,0.34879767894744873,0.3452375531196594,0.3484516143798828,0.3394833505153656,0.33823341131210327,0.3380015790462494,0.33168360590934753,0.3336303234100342,0.32824623584747314,0.3404909372329712,0.32641109824180603,0.3221675455570221,0.3288486897945404,0.31790900230407715,0.31747883558273315,0.31357505917549133,0.31181076169013977,0.3165504038333893,0.3086080253124237,0.3111473619937897,0.3067426085472107,0.30230453610420227,0.30708402395248413,0.30867934226989746,0.2998124957084656,0.2955939471721649,0.29593804478645325,0.29775387048721313,0.30516842007637024,0.29085129499435425,0.29009711742401123,0.2905358672142029,0.29187139868736267,0.2854427695274353,0.28685107827186584,0.2840442657470703,0.28888818621635437,0.282156765460968,0.28035080432891846,0.28199756145477295,0.2770686149597168,0.2840105891227722,0.27911248803138733,0.2724848687648773,0.2800312042236328,0.2698758542537689,0.27103498578071594,0.26963964104652405,0.2678753733634949,0.27516722679138184,0.2686917185783386,0.2729519307613373,0.26411664485931396,0.27146458625793457,0.26185932755470276,0.2704011797904968,0.2625465393066406,0.26457494497299194,0.26601600646972656,0.2568378150463104,0.254995197057724,0.2563948631286621,0.25980356335639954,0.25564509630203247,0.25468334555625916,0.25247564911842346,0.25523388385772705,0.24989433586597443,0.25850820541381836,0.24832592904567719,0.2489527463912964,0.24773581326007843,0.25033119320869446,0.24704404175281525,0.24632638692855835,0.24874672293663025,0.24359667301177979,0.2453552633523941,0.24413509666919708,0.24266189336776733,0.2426595687866211,0.24042008817195892,0.2396996021270752,0.23924267292022705,0.24253490567207336,0.23778575658798218,0.23942901194095612,0.2449275255203247,0.25151193141937256,0.23767311871051788,0.23506878316402435,0.23496578633785248,0.23275385797023773,0.234956294298172,0.23408901691436768,0.23222151398658752,0.23963865637779236,0.2317635715007782,0.23583751916885376,0.231106698513031,0.23400887846946716,0.2390103042125702,0.22786518931388855,0.23019173741340637,0.23060153424739838,0.2268799990415573,0.2264801561832428,0.23040267825126648,0.2266920953989029,0.22596654295921326,0.22565928101539612,0.22820612788200378,0.2248227447271347,0.22408856451511383,0.22725661098957062,0.22661294043064117,0.22506392002105713,0.22241602838039398,0.22370345890522003,0.22362767159938812,0.22290721535682678,0.22024355828762054,0.22187843918800354,0.2209651619195938,0.22425583004951477,0.2235262095928192,0.22057798504829407,0.21839508414268494,0.21945352852344513,0.21973825991153717,0.22150367498397827,0.2238992601633072,0.2170979380607605,0.22096943855285645,0.21685203909873962,0.21617646515369415,0.21836219727993011,0.2187494933605194,0.2147844433784485,0.21969282627105713,0.21570147573947906,0.21583843231201172,0.21398569643497467,0.2226148396730423,0.2156727910041809,0.21307344734668732,0.2115446776151657,0.21514661610126495,0.21145278215408325,0.21272814273834229,0.21182085573673248,0.2115216851234436,0.2141103595495224,0.21205274760723114,0.2108447402715683,0.21038177609443665,0.21014602482318878,0.20907393097877502,0.21143515408039093,0.20848137140274048,0.2139512300491333,0.21031275391578674,0.21034903824329376,0.20903901755809784,0.2071121484041214,0.22784949839115143,0.21483035385608673,0.2080242782831192,0.20944760739803314,0.21115021407604218,0.20908775925636292,0.20634588599205017,0.20654284954071045,0.2081865519285202,0.20917917788028717,0.20922774076461792,0.20779480040073395,0.2138199806213379,0.20871517062187195,0.2053835541009903,0.20546866953372955,0.20372682809829712,0.20380699634552002,0.20610322058200836,0.20399130880832672,0.2024591714143753,0.20496663451194763,0.20287373661994934,0.20388159155845642,0.2083885222673416,0.20139899849891663,0.20386724174022675,0.2027149200439453,0.20526708662509918,0.20128971338272095,0.20266036689281464,0.20446908473968506,0.20366421341896057,0.20464523136615753,0.2014428675174713,0.1999436914920807,0.20329396426677704,0.2057424634695053,0.20097437500953674,0.20288898050785065,0.20123733580112457,0.20101769268512726,0.2014637440443039,0.2018376886844635,0.19932830333709717,0.20116835832595825,0.20189984142780304,0.19906684756278992,0.19924698770046234,0.1998450756072998,0.19895824790000916,0.1999184638261795,0.20183727145195007,0.20392921566963196,0.19886620342731476,0.19915087521076202,0.20067748427391052,0.2039177119731903,0.1972903460264206,0.197664275765419,0.19790154695510864,0.2000807821750641,0.19839003682136536,0.19981180131435394,0.19688433408737183,0.19764141738414764,0.1984276920557022,0.20066425204277039,0.1964191496372223,0.19748789072036743,0.19625400006771088,0.19732601940631866,0.19875262677669525,0.1984328329563141,0.19595670700073242,0.19683320820331573,0.19931653141975403,0.1966322362422943,0.19842268526554108,0.1973453015089035,0.19740881025791168,0.2013074904680252,0.19687607884407043,0.1981360763311386,0.19530291855335236,0.1963765174150467,0.19668173789978027,0.196573868393898,0.1964244693517685,0.1972021609544754,0.19553716480731964,0.2039797008037567,0.19643855094909668,0.19661465287208557,0.1962609440088272,0.19581907987594604,0.1997777223587036,0.19494421780109406,0.19452029466629028,0.1958526223897934],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Number of epochs\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Loss plot\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('aac7b923-fe0d-4f3f-ac7c-b1a0a5776373');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.line(history_details_df,x='Number of epochs',y=history_details_df.columns[3:],color_discrete_sequence=px.colors.qualitative.Antique,title='Loss plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:20:51.764175Z",
     "iopub.status.busy": "2023-04-02T05:20:51.762815Z",
     "iopub.status.idle": "2023-04-02T05:20:51.847672Z",
     "shell.execute_reply": "2023-04-02T05:20:51.846261Z",
     "shell.execute_reply.started": "2023-04-02T05:20:51.76412Z"
    },
    "id": "K8c0L-mHMpoM",
    "outputId": "1cb12f7a-1053-4733-a0a7-1a2e1f872a30"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"5c2ed564-3e87-4c52-8bb2-b02df0d55ccf\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5c2ed564-3e87-4c52-8bb2-b02df0d55ccf\")) {                    Plotly.newPlot(                        \"5c2ed564-3e87-4c52-8bb2-b02df0d55ccf\",                        [{\"hovertemplate\":\"variable=Train_Accuracy<br>Number of epochs=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"Train_Accuracy\",\"line\":{\"color\":\"rgb(27,158,119)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Train_Accuracy\",\"showlegend\":true,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350],\"xaxis\":\"x\",\"y\":[0.07983473688364029,0.21363084018230438,0.31345701217651367,0.360816091299057,0.3985842764377594,0.4335603415966034,0.468328058719635,0.5050246715545654,0.5371490716934204,0.5668920874595642,0.5922534465789795,0.6138165593147278,0.6339985132217407,0.6511918902397156,0.6680101156234741,0.6826554536819458,0.6950682401657104,0.7073976397514343,0.7185781002044678,0.7288060188293457,0.7383075952529907,0.7466601729393005,0.7541078329086304,0.7612459063529968,0.7678065299987793,0.773581326007843,0.7782964110374451,0.783440113067627,0.7878039479255676,0.7927213907241821,0.7957278490066528,0.8002643585205078,0.8039435148239136,0.8072357177734375,0.8103850483894348,0.8141416311264038,0.8167372941970825,0.8198747634887695,0.8216726779937744,0.8249112963676453,0.828465461730957,0.830233633518219,0.8326089978218079,0.8352880477905273,0.8374788761138916,0.8398780822753906,0.8424856662750244,0.8443252444267273,0.8467839956283569,0.8489927053451538,0.8506953716278076,0.8522908687591553,0.8554878234863281,0.8567975759506226,0.8583394885063171,0.8597623705863953,0.8620246052742004,0.8635486960411072,0.8658407330513,0.8668766021728516,0.8684304356575012,0.8706153631210327,0.8722108602523804,0.87376469373703,0.8740861415863037,0.8768485188484192,0.8785274028778076,0.8788548111915588,0.8809563517570496,0.8816588521003723,0.8829864263534546,0.8843140602111816,0.8861000537872314,0.8869990110397339,0.8885350227355957,0.8893386721611023,0.8906722664833069,0.8918153047561646,0.8931905031204224,0.893756091594696,0.8952503800392151,0.896101713180542,0.8972566723823547,0.8982627987861633,0.8994296789169312,0.8997511267662048,0.9009954333305359,0.9019598364830017,0.9027635455131531,0.9033231735229492,0.9046388864517212,0.9049365520477295,0.9057283401489258,0.9064963459968567,0.9073535799980164,0.9084430932998657,0.909151554107666,0.9098361730575562,0.9109851717948914,0.9113721251487732,0.9116697907447815,0.9126045107841492,0.9130390882492065,0.9142833352088928,0.9142357110977173,0.9152596592903137,0.9162420034408569,0.9164265394210815,0.9168134927749634,0.9179982542991638,0.9180875420570374,0.9189448356628418,0.9196592569351196,0.9201295375823975,0.9205760359764099,0.921123743057251,0.921534538269043,0.9217666983604431,0.9227132797241211,0.9228383302688599,0.9234157800674438,0.9237372875213623,0.9242849946022034,0.9248625040054321,0.9252256155014038,0.9260769486427307,0.9256959557533264,0.926249623298645,0.9271783232688904,0.9274819493293762,0.9278689026832581,0.9282082915306091,0.928678572177887,0.9286131262779236,0.929089367389679,0.929512083530426,0.9303038716316223,0.9307682514190674,0.9308575391769409,0.9314349889755249,0.9312801957130432,0.9320065379142761,0.9317684173583984,0.9326018691062927,0.9326614141464233,0.9334591627120972,0.9337806105613708,0.9342509508132935,0.9343461990356445,0.9346379041671753,0.9349415302276611,0.9357631206512451,0.9359595775604248,0.9364596605300903,0.9368109107017517,0.9366739988327026,0.9375967383384705,0.9375788569450378,0.9379242062568665,0.938055157661438,0.9382635354995728,0.9384361505508423,0.9389481544494629,0.9392458200454712,0.939549446105957,0.9393351078033447,0.9400733709335327,0.9403293132781982,0.9410259127616882,0.9411628246307373,0.9416093230247498,0.9413116574287415,0.9416450262069702,0.9418593645095825,0.942728579044342,0.9431214928627014,0.9428178668022156,0.9433774948120117,0.942936897277832,0.9435977339744568,0.9439073204994202,0.9434905648231506,0.9441930651664734,0.9452230334281921,0.944663405418396,0.9449967741966248,0.9451634883880615,0.9453896880149841,0.9457528591156006,0.94614577293396,0.9458064436912537,0.9464017748832703,0.9470506906509399,0.9467887282371521,0.9466220736503601,0.9474912285804749,0.9471399784088135,0.9478127360343933,0.9478246569633484,0.9480865597724915,0.9484795331954956,0.948586642742157,0.9488486051559448,0.9491046071052551,0.9492177367210388,0.9494855999946594,0.9494796991348267,0.9498368501663208,0.9498785734176636,0.9503964781761169,0.9505751132965088,0.9501166939735413,0.9509144425392151,0.9509263634681702,0.9512597322463989,0.9512597322463989,0.9516883492469788,0.9514681100845337,0.9516526460647583,0.9519205689430237,0.9523849487304688,0.9524027705192566,0.9527361989021301,0.9528433084487915,0.9529147744178772,0.9530933499336243,0.953629195690155,0.9533315300941467,0.9535517692565918,0.9537363648414612,0.9538792371749878,0.954164981842041,0.9543078541755676,0.9544150233268738,0.9549091458320618,0.9545817375183105,0.9547305703163147,0.9548079371452332,0.9552425146102905,0.9551770687103271,0.9554330706596375,0.9554449319839478,0.956099808216095,0.9566415548324585,0.9560402631759644,0.9562426805496216,0.9564749002456665,0.9571059346199036,0.9567546844482422,0.9569094777107239,0.957141637802124,0.9571952223777771,0.9572131037712097,0.9577310681343079,0.9578441381454468,0.957963228225708,0.9576238989830017,0.9583263993263245,0.9583978056907654,0.9585466384887695,0.9587073922157288,0.9591122269630432,0.9588324427604675,0.9593086838722229,0.9590229392051697,0.9596182703971863,0.9592908620834351,0.9596301913261414,0.9598861932754517,0.9597373604774475,0.9597551822662354,0.9601719379425049,0.9603922367095947,0.9602314829826355,0.9606184363365173,0.9608803987503052,0.9604517221450806,0.9610292315483093,0.9609637260437012,0.9611125588417053,0.9613626003265381,0.9613030552864075,0.9615114331245422,0.9615530967712402,0.9617555141448975,0.9618746042251587,0.9622794389724731,0.9622496366500854,0.962583065032959,0.9622913599014282,0.9626961350440979,0.9629580974578857,0.9629878997802734,0.9631962180137634,0.9633391499519348,0.9629759788513184,0.963541567325592,0.9635058045387268,0.963160514831543,0.9638510942459106,0.9636308550834656,0.9639642238616943,0.9642202258110046,0.964065432548523,0.964392900466919,0.9643035531044006,0.9646250605583191,0.9644583463668823,0.9645417332649231,0.9648155570030212,0.9652620553970337,0.9650000929832458,0.9650596380233765,0.965559720993042,0.9654466509819031,0.9656371474266052,0.9653573036193848,0.9658514261245728,0.9660062193870544,0.9661193490028381,0.9660717248916626,0.9659883975982666,0.9662681818008423,0.966518223285675,0.9666610956192017,0.9663693904876709,0.9665480256080627,0.9666968584060669,0.9668635129928589,0.9671373963356018,0.9670302271842957,0.9669826030731201,0.967697024345398,0.967464804649353,0.9673219323158264,0.9674707651138306,0.9675660133361816,0.9681017994880676,0.9681017994880676,0.9679827690124512,0.9677505493164062,0.9682506322860718,0.9682089686393738,0.9686912298202515,0.9686018824577332,0.9685900211334229,0.9684590101242065,0.9688042998313904,0.9692448973655701,0.9694830179214478],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"variable=Validation_accuracy<br>Number of epochs=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"Validation_accuracy\",\"line\":{\"color\":\"rgb(217,95,2)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Validation_accuracy\",\"showlegend\":true,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350],\"xaxis\":\"x\",\"y\":[0.14149640500545502,0.27946850657463074,0.339905709028244,0.3786255121231079,0.414773553609848,0.44861170649528503,0.4856407940387726,0.5168119072914124,0.5528408885002136,0.576653778553009,0.6027765870094299,0.6240177154541016,0.6413059234619141,0.6586655378341675,0.6740010380744934,0.6906462907791138,0.703052818775177,0.7133162021636963,0.7239367365837097,0.7324855923652649,0.7417250275611877,0.7496547102928162,0.7558460831642151,0.7631566524505615,0.7676572799682617,0.7719912528991699,0.77646803855896,0.7807543873786926,0.7860170602798462,0.7900652289390564,0.7929227948188782,0.7980663776397705,0.8001381158828735,0.8043292164802551,0.8073772192001343,0.8069486021995544,0.8118540644645691,0.8160213232040405,0.8168309926986694,0.8222126960754395,0.8243558406829834,0.826022744178772,0.8284278512001038,0.8309282064437866,0.8310472965240479,0.8357384204864502,0.8362385034561157,0.8392627239227295,0.8404057621955872,0.84426349401474,0.8453112244606018,0.845335066318512,0.8477877974510193,0.849811851978302,0.8522169589996338,0.854479193687439,0.85457444190979,0.8582178354263306,0.8555983901023865,0.8602895736694336,0.8628613352775574,0.8626470565795898,0.8617183566093445,0.8664571046829224,0.8683621287345886,0.8695051670074463,0.8715292811393738,0.8729342222213745,0.8727198839187622,0.8740057945251465,0.8760537505149841,0.8758631944656372,0.8786969780921936,0.8779587745666504,0.88017338514328,0.8808639049530029,0.882768988609314,0.8825070261955261,0.884102463722229,0.8809829950332642,0.884388267993927,0.8791970014572144,0.8807210326194763,0.8849121332168579,0.8887221813201904,0.8896985054016113,0.8909606337547302,0.889222264289856,0.8932942748069763,0.8931514024734497,0.8933418989181519,0.8954374194145203,0.8937943577766418,0.8964614272117615,0.889222264289856,0.8978187441825867,0.8980092406272888,0.8947230577468872,0.8995094299316406,0.9003428816795349,0.9007953405380249,0.9009144306182861,0.8999381065368652,0.901795506477356,0.9012715816497803,0.9028908610343933,0.9039386510848999,0.9015335440635681,0.9009620547294617,0.9060817956924438,0.9060817956924438,0.9062247276306152,0.9063437581062317,0.902247965335846,0.9081059098243713,0.9078677892684937,0.9078677892684937,0.9064866304397583,0.9090822339057922,0.9086059927940369,0.9101776480674744,0.9067723751068115,0.9109634757041931,0.910939633846283,0.9093441963195801,0.911225438117981,0.9101776480674744,0.9118921756744385,0.9130828380584717,0.9101061820983887,0.9146782755851746,0.9131304621696472,0.9142258167266846,0.9142734408378601,0.9119874238967896,0.9138448238372803,0.9127256274223328,0.9158450961112976,0.9140591621398926,0.9165832996368408,0.9139876961708069,0.9154641032218933,0.9161070585250854,0.914082944393158,0.9182740449905396,0.9187502861022949,0.9181311726570129,0.9176549315452576,0.9187026619911194,0.9189884066581726,0.9205362796783447,0.9189407825469971,0.9204171895980835,0.9178930521011353,0.9216792583465576,0.9216316342353821,0.9220603108406067,0.920369565486908,0.9215602278709412,0.9225127696990967,0.9212030172348022,0.923346221446991,0.9225127696990967,0.9223222136497498,0.9237033724784851,0.9236795902252197,0.9237033724784851,0.9242272973060608,0.9241082072257996,0.9234890937805176,0.9247749447822571,0.9244654178619385,0.9230366349220276,0.9209411144256592,0.924679696559906,0.9263466000556946,0.9265370965003967,0.9262275695800781,0.9257751107215881,0.9255369901657104,0.9273467659950256,0.9239177107810974,0.9270848035812378,0.9256083965301514,0.9267038106918335,0.9265133142471313,0.9232509136199951,0.9277039766311646,0.9277516007423401,0.9270133972167969,0.9279897212982178,0.9282993078231812,0.9270133972167969,0.9283230900764465,0.928894579410553,0.9296327829360962,0.9286088347434998,0.9290136694908142,0.9298709630966187,0.929275631904602,0.9281325936317444,0.9291565418243408,0.9302757382392883,0.9294661283493042,0.9299662113189697,0.9294185042381287,0.9304424524307251,0.9302995800971985,0.9299662113189697,0.9292994141578674,0.9289184212684631,0.9307758212089539,0.9301328659057617,0.9304662346839905,0.9300138354301453,0.9297518730163574,0.9281087517738342,0.931418776512146,0.9313949346542358,0.932418942451477,0.931418776512146,0.930966317653656,0.9298471212387085,0.9326094388961792,0.9311330318450928,0.9317045211791992,0.9321093559265137,0.9325141906738281,0.9295375347137451,0.9320617318153381,0.9316331148147583,0.932942807674408,0.9325141906738281,0.9326094388961792,0.9329189658164978,0.932942807674408,0.932585597038269,0.9323474764823914,0.9325379729270935,0.9335381388664246,0.9336810111999512,0.9328713417053223,0.9343477487564087,0.9336810111999512,0.9348716735839844,0.9312759041786194,0.9339667558670044,0.934109628200531,0.9341572523117065,0.9348478317260742,0.9264894723892212,0.9315616488456726,0.9339191317558289,0.9327523112297058,0.932323694229126,0.9333476424217224,0.9352288246154785,0.9346097111701965,0.9340143799781799,0.9340858459472656,0.9344906210899353,0.9349192976951599,0.9320141077041626,0.9327760934829712,0.9350621700286865,0.935181200504303,0.9354431629180908,0.9355860352516174,0.9346573352813721,0.9353716969490051,0.936633825302124,0.934728741645813,0.936086118221283,0.9355145692825317,0.9334190487861633,0.936348021030426,0.9355860352516174,0.9356575012207031,0.935633659362793,0.9360384941101074,0.9359670281410217,0.9356812834739685,0.936181366443634,0.9352050423622131,0.936800479888916,0.9373481869697571,0.9361337423324585,0.9343477487564087,0.9364671111106873,0.9362766146659851,0.9364909529685974,0.9360146522521973,0.9367528557777405,0.935276448726654,0.9376339316368103,0.936181366443634,0.9349192976951599,0.936633825302124,0.9370148181915283,0.9366099834442139,0.9371338486671448,0.9365862011909485,0.9357051253318787,0.9355860352516174,0.9366576075553894,0.9372767806053162,0.9367290735244751,0.9347764253616333,0.9378482699394226,0.9373244047164917,0.9373481869697571,0.9366576075553894,0.9378244280815125,0.9361099004745483,0.9382292628288269,0.9375863075256348,0.9368719458580017,0.936181366443634,0.9379673004150391,0.9375863075256348,0.9381340146064758,0.9367528557777405,0.9375863075256348,0.9364194869995117,0.9381102323532104,0.9373005628585815,0.9377530217170715,0.9375625252723694,0.9367290735244751,0.9375625252723694,0.9373958110809326,0.9372291564941406,0.9375625252723694,0.9376815557479858,0.9379911422729492,0.9384436011314392,0.9375386834144592,0.9377530217170715,0.9384912252426147,0.9373958110809326,0.938324511051178,0.9358241558074951,0.9375386834144592,0.937872052192688,0.9378244280815125,0.938324511051178,0.9370148181915283,0.938419759273529,0.9388007521629333,0.9380387663841248],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"variable=Train_Loss<br>Number of epochs=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"Train_Loss\",\"line\":{\"color\":\"rgb(117,112,179)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Train_Loss\",\"showlegend\":true,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350],\"xaxis\":\"x\",\"y\":[3.4703426361083984,2.8878307342529297,2.457624673843384,2.2557356357574463,2.1096601486206055,1.9768871068954468,1.8455946445465088,1.716345191001892,1.5954954624176025,1.4874811172485352,1.3927708864212036,1.3097187280654907,1.2362667322158813,1.1706230640411377,1.1118011474609375,1.0590741634368896,1.0116647481918335,0.9686728715896606,0.9298692345619202,0.8950104117393494,0.8631702661514282,0.8342262506484985,0.8081048130989075,0.7841328978538513,0.762031614780426,0.7418618202209473,0.7227592468261719,0.7056122422218323,0.6895970106124878,0.6740935444831848,0.6602308750152588,0.6469296813011169,0.6340413689613342,0.6219314336776733,0.6101243495941162,0.5990549325942993,0.589129626750946,0.5795255303382874,0.5703701376914978,0.5610503554344177,0.5513206124305725,0.5436660647392273,0.5353488326072693,0.5276885628700256,0.5194635391235352,0.5122232437133789,0.5053175687789917,0.49790260195732117,0.49103933572769165,0.4843868613243103,0.47850269079208374,0.4719886779785156,0.46555405855178833,0.4599701166152954,0.4537300169467926,0.4486578404903412,0.44286325573921204,0.43759965896606445,0.43199944496154785,0.4271853268146515,0.4213695228099823,0.4163435697555542,0.4107331335544586,0.40648195147514343,0.4024570882320404,0.3961670398712158,0.39227500557899475,0.387545108795166,0.38300344347953796,0.38007742166519165,0.3751259744167328,0.37107130885124207,0.3672555088996887,0.3637513518333435,0.35906553268432617,0.3553794026374817,0.3524525463581085,0.3480248749256134,0.34579789638519287,0.3422354459762573,0.33781588077545166,0.33505338430404663,0.3311694264411926,0.3288455307483673,0.32537686824798584,0.32346293330192566,0.3197605013847351,0.31674906611442566,0.31445345282554626,0.3121053874492645,0.30843088030815125,0.30681878328323364,0.30440831184387207,0.3014209568500519,0.2986783981323242,0.2962524890899658,0.293946236371994,0.2912437915802002,0.28883111476898193,0.28723499178886414,0.28574660420417786,0.2827986776828766,0.28090929985046387,0.2781614065170288,0.2767547070980072,0.2739916145801544,0.2730032205581665,0.2711278796195984,0.26886144280433655,0.2671295404434204,0.2655566930770874,0.26279252767562866,0.26129579544067383,0.259568989276886,0.25719308853149414,0.25581294298171997,0.2546967566013336,0.25344446301460266,0.2514925003051758,0.24988336861133575,0.24852339923381805,0.24685657024383545,0.24524551630020142,0.2433730810880661,0.24249373376369476,0.24052949249744415,0.23959608376026154,0.23884114623069763,0.2362406849861145,0.23538944125175476,0.2338901311159134,0.2327588051557541,0.23140203952789307,0.23043954372406006,0.2289109230041504,0.2274802327156067,0.22606873512268066,0.22519871592521667,0.22401387989521027,0.22275742888450623,0.2217143177986145,0.22044959664344788,0.2194776087999344,0.21804027259349823,0.2174522876739502,0.21554048359394073,0.21449603140354156,0.21392443776130676,0.21272507309913635,0.21174059808254242,0.21071134507656097,0.20895177125930786,0.20854073762893677,0.20729245245456696,0.2059738039970398,0.20583297312259674,0.2037842720746994,0.20299218595027924,0.20287160575389862,0.20117290318012238,0.20066328346729279,0.20021529495716095,0.1990198940038681,0.1977282613515854,0.1967516541481018,0.19682875275611877,0.1952853798866272,0.1943918615579605,0.19365867972373962,0.1924435943365097,0.191330224275589,0.19123928248882294,0.1902225911617279,0.1891351342201233,0.18825863301753998,0.18758514523506165,0.18718251585960388,0.18602098524570465,0.18571363389492035,0.18436439335346222,0.1839560866355896,0.18367019295692444,0.18283629417419434,0.18115995824337006,0.18091782927513123,0.17980705201625824,0.1792854219675064,0.17879219353199005,0.17789040505886078,0.17724451422691345,0.17644380033016205,0.17626093327999115,0.17486818134784698,0.17465190589427948,0.1745772808790207,0.17316552996635437,0.17275787889957428,0.17163389921188354,0.1713961809873581,0.17049844563007355,0.16977831721305847,0.16936099529266357,0.168425053358078,0.1678178310394287,0.16755259037017822,0.16711702942848206,0.16601485013961792,0.16555289924144745,0.16527771949768066,0.16442394256591797,0.16333484649658203,0.16327103972434998,0.16286538541316986,0.16196991503238678,0.16079486906528473,0.16113024950027466,0.16031338274478912,0.15962305665016174,0.15938174724578857,0.1585875004529953,0.15843886137008667,0.15753090381622314,0.1568775177001953,0.15601269900798798,0.15572743117809296,0.15520192682743073,0.15482769906520844,0.1541990488767624,0.15393202006816864,0.15341609716415405,0.15270882844924927,0.15250055491924286,0.15150579810142517,0.15161839127540588,0.15053685009479523,0.150451198220253,0.1496991664171219,0.14962422847747803,0.14864473044872284,0.1482917070388794,0.14786969125270844,0.14736980199813843,0.1468007117509842,0.14628532528877258,0.14580489695072174,0.14581981301307678,0.1451234221458435,0.14431357383728027,0.14423570036888123,0.1435430645942688,0.14361701905727386,0.14273011684417725,0.14234523475170135,0.14161361753940582,0.14091260731220245,0.1405860036611557,0.1407867670059204,0.1399698406457901,0.13949905335903168,0.13924306631088257,0.13868512213230133,0.13842494785785675,0.1380012333393097,0.1372271031141281,0.13712982833385468,0.13642756640911102,0.1363036036491394,0.1360393613576889,0.13536593317985535,0.13511846959590912,0.1347380429506302,0.1343824416399002,0.13380268216133118,0.13321293890476227,0.13307249546051025,0.1325368732213974,0.13274608552455902,0.13204535841941833,0.13169004023075104,0.13083001971244812,0.13070325553417206,0.1304396390914917,0.12977401912212372,0.12970171868801117,0.1293892115354538,0.12851254642009735,0.12849119305610657,0.12819308042526245,0.12768425047397614,0.127371683716774,0.12684109807014465,0.12646198272705078,0.12625856697559357,0.12581174075603485,0.12530730664730072,0.12533855438232422,0.12476709485054016,0.12425444275140762,0.12405367195606232,0.12351533025503159,0.12324974685907364,0.12312446534633636,0.12261395156383514,0.12244465202093124,0.12214792519807816,0.12193033844232559,0.12146841734647751,0.12103258818387985,0.12090300768613815,0.12018521130084991,0.12005428969860077,0.11936728656291962,0.11931141465902328,0.11856108158826828,0.11900301277637482,0.11849414557218552,0.1182057335972786,0.11779086291790009,0.11743757873773575,0.11695291101932526,0.11695396900177002,0.11655930429697037,0.11645106971263885,0.11583231389522552,0.11545585095882416,0.11542012542486191,0.11482878029346466,0.11474496871232986,0.11454629898071289,0.11397732049226761,0.11376406252384186,0.11379177868366241,0.11317794024944305,0.1126864030957222,0.11223716288805008,0.11231622099876404,0.11202576011419296,0.11129933595657349,0.11142012476921082,0.11092886328697205,0.11123741418123245,0.11035583913326263,0.11015722900629044,0.10976944863796234,0.10932696610689163,0.10935023427009583,0.10910943895578384,0.10860658437013626,0.10836880654096603,0.10805795341730118],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"variable=Validation_loss<br>Number of epochs=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"Validation_loss\",\"line\":{\"color\":\"rgb(231,41,138)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Validation_loss\",\"showlegend\":true,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350],\"xaxis\":\"x\",\"y\":[3.2633426189422607,2.600216865539551,2.335538625717163,2.174294948577881,2.037991523742676,1.9069969654083252,1.7758760452270508,1.650233507156372,1.5359795093536377,1.4350093603134155,1.3461942672729492,1.268165111541748,1.2005678415298462,1.1376750469207764,1.0837785005569458,1.0335679054260254,0.988894522190094,0.9492751955986023,0.9151712656021118,0.8811183571815491,0.8530488610267639,0.823388934135437,0.8016342520713806,0.7771672010421753,0.757160484790802,0.7394341826438904,0.7222990989685059,0.7068681120872498,0.689858615398407,0.6762564182281494,0.6638378500938416,0.6488730907440186,0.6360043287277222,0.6271462440490723,0.6143273115158081,0.6123170852661133,0.5979743003845215,0.5839459896087646,0.5823299288749695,0.5671233534812927,0.5589125156402588,0.5526798963546753,0.5456172227859497,0.5354422330856323,0.5333609580993652,0.5216667056083679,0.5159744620323181,0.5085121989250183,0.5023886561393738,0.4949282705783844,0.48964858055114746,0.4866145849227905,0.47926148772239685,0.4781084656715393,0.4677855372428894,0.4627111554145813,0.4636121392250061,0.451445996761322,0.4516627788543701,0.44126033782958984,0.43584850430488586,0.4337666630744934,0.4351935386657715,0.42261090874671936,0.41923072934150696,0.4142285883426666,0.4086248278617859,0.40624329447746277,0.4051479995250702,0.40071532130241394,0.39453595876693726,0.3918152153491974,0.385763019323349,0.38672930002212524,0.3800794780254364,0.3793303966522217,0.37390536069869995,0.3720739781856537,0.3670787513256073,0.37204599380493164,0.3673691749572754,0.37189802527427673,0.3758186399936676,0.35940849781036377,0.35218527913093567,0.34879767894744873,0.3452375531196594,0.3484516143798828,0.3394833505153656,0.33823341131210327,0.3380015790462494,0.33168360590934753,0.3336303234100342,0.32824623584747314,0.3404909372329712,0.32641109824180603,0.3221675455570221,0.3288486897945404,0.31790900230407715,0.31747883558273315,0.31357505917549133,0.31181076169013977,0.3165504038333893,0.3086080253124237,0.3111473619937897,0.3067426085472107,0.30230453610420227,0.30708402395248413,0.30867934226989746,0.2998124957084656,0.2955939471721649,0.29593804478645325,0.29775387048721313,0.30516842007637024,0.29085129499435425,0.29009711742401123,0.2905358672142029,0.29187139868736267,0.2854427695274353,0.28685107827186584,0.2840442657470703,0.28888818621635437,0.282156765460968,0.28035080432891846,0.28199756145477295,0.2770686149597168,0.2840105891227722,0.27911248803138733,0.2724848687648773,0.2800312042236328,0.2698758542537689,0.27103498578071594,0.26963964104652405,0.2678753733634949,0.27516722679138184,0.2686917185783386,0.2729519307613373,0.26411664485931396,0.27146458625793457,0.26185932755470276,0.2704011797904968,0.2625465393066406,0.26457494497299194,0.26601600646972656,0.2568378150463104,0.254995197057724,0.2563948631286621,0.25980356335639954,0.25564509630203247,0.25468334555625916,0.25247564911842346,0.25523388385772705,0.24989433586597443,0.25850820541381836,0.24832592904567719,0.2489527463912964,0.24773581326007843,0.25033119320869446,0.24704404175281525,0.24632638692855835,0.24874672293663025,0.24359667301177979,0.2453552633523941,0.24413509666919708,0.24266189336776733,0.2426595687866211,0.24042008817195892,0.2396996021270752,0.23924267292022705,0.24253490567207336,0.23778575658798218,0.23942901194095612,0.2449275255203247,0.25151193141937256,0.23767311871051788,0.23506878316402435,0.23496578633785248,0.23275385797023773,0.234956294298172,0.23408901691436768,0.23222151398658752,0.23963865637779236,0.2317635715007782,0.23583751916885376,0.231106698513031,0.23400887846946716,0.2390103042125702,0.22786518931388855,0.23019173741340637,0.23060153424739838,0.2268799990415573,0.2264801561832428,0.23040267825126648,0.2266920953989029,0.22596654295921326,0.22565928101539612,0.22820612788200378,0.2248227447271347,0.22408856451511383,0.22725661098957062,0.22661294043064117,0.22506392002105713,0.22241602838039398,0.22370345890522003,0.22362767159938812,0.22290721535682678,0.22024355828762054,0.22187843918800354,0.2209651619195938,0.22425583004951477,0.2235262095928192,0.22057798504829407,0.21839508414268494,0.21945352852344513,0.21973825991153717,0.22150367498397827,0.2238992601633072,0.2170979380607605,0.22096943855285645,0.21685203909873962,0.21617646515369415,0.21836219727993011,0.2187494933605194,0.2147844433784485,0.21969282627105713,0.21570147573947906,0.21583843231201172,0.21398569643497467,0.2226148396730423,0.2156727910041809,0.21307344734668732,0.2115446776151657,0.21514661610126495,0.21145278215408325,0.21272814273834229,0.21182085573673248,0.2115216851234436,0.2141103595495224,0.21205274760723114,0.2108447402715683,0.21038177609443665,0.21014602482318878,0.20907393097877502,0.21143515408039093,0.20848137140274048,0.2139512300491333,0.21031275391578674,0.21034903824329376,0.20903901755809784,0.2071121484041214,0.22784949839115143,0.21483035385608673,0.2080242782831192,0.20944760739803314,0.21115021407604218,0.20908775925636292,0.20634588599205017,0.20654284954071045,0.2081865519285202,0.20917917788028717,0.20922774076461792,0.20779480040073395,0.2138199806213379,0.20871517062187195,0.2053835541009903,0.20546866953372955,0.20372682809829712,0.20380699634552002,0.20610322058200836,0.20399130880832672,0.2024591714143753,0.20496663451194763,0.20287373661994934,0.20388159155845642,0.2083885222673416,0.20139899849891663,0.20386724174022675,0.2027149200439453,0.20526708662509918,0.20128971338272095,0.20266036689281464,0.20446908473968506,0.20366421341896057,0.20464523136615753,0.2014428675174713,0.1999436914920807,0.20329396426677704,0.2057424634695053,0.20097437500953674,0.20288898050785065,0.20123733580112457,0.20101769268512726,0.2014637440443039,0.2018376886844635,0.19932830333709717,0.20116835832595825,0.20189984142780304,0.19906684756278992,0.19924698770046234,0.1998450756072998,0.19895824790000916,0.1999184638261795,0.20183727145195007,0.20392921566963196,0.19886620342731476,0.19915087521076202,0.20067748427391052,0.2039177119731903,0.1972903460264206,0.197664275765419,0.19790154695510864,0.2000807821750641,0.19839003682136536,0.19981180131435394,0.19688433408737183,0.19764141738414764,0.1984276920557022,0.20066425204277039,0.1964191496372223,0.19748789072036743,0.19625400006771088,0.19732601940631866,0.19875262677669525,0.1984328329563141,0.19595670700073242,0.19683320820331573,0.19931653141975403,0.1966322362422943,0.19842268526554108,0.1973453015089035,0.19740881025791168,0.2013074904680252,0.19687607884407043,0.1981360763311386,0.19530291855335236,0.1963765174150467,0.19668173789978027,0.196573868393898,0.1964244693517685,0.1972021609544754,0.19553716480731964,0.2039797008037567,0.19643855094909668,0.19661465287208557,0.1962609440088272,0.19581907987594604,0.1997777223587036,0.19494421780109406,0.19452029466629028,0.1958526223897934],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Number of epochs\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Loss and Accuracy plots\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('5c2ed564-3e87-4c52-8bb2-b02df0d55ccf');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.line(history_details_df,x='Number of epochs',y=history_details_df.columns[0:],color_discrete_sequence=px.colors.qualitative.Dark2,title='Loss and Accuracy plots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGMc8q6LMpoM"
   },
   "source": [
    "## Modifying the LeNet model to get the best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T05:20:59.678145Z",
     "iopub.status.busy": "2023-04-02T05:20:59.677349Z",
     "iopub.status.idle": "2023-04-02T05:20:59.795766Z",
     "shell.execute_reply": "2023-04-02T05:20:59.794393Z",
     "shell.execute_reply.started": "2023-04-02T05:20:59.678109Z"
    },
    "id": "eNJ_VlGJMpoM"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=(32, 32, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(48, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(84, activation='relu'))\n",
    "model.add(Dense(35, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:20:59.801534Z",
     "iopub.status.busy": "2023-04-02T05:20:59.801099Z",
     "iopub.status.idle": "2023-04-02T05:20:59.834335Z",
     "shell.execute_reply": "2023-04-02T05:20:59.832796Z",
     "shell.execute_reply.started": "2023-04-02T05:20:59.801482Z"
    },
    "id": "-NvieMhKMpoM",
    "outputId": "c020b315-fddd-40d0-cc70-dd4291332434"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 32)        832       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 10, 10, 48)        38448     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5, 5, 48)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1200)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               307456    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 84)                21588     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 35)                2975      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 371,299\n",
      "Trainable params: 371,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z7m4OSXcYI-G",
    "outputId": "c114bae5-b425-4f9e-c9a4-14dcf6aa3c91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:20:59.837057Z",
     "iopub.status.busy": "2023-04-02T05:20:59.836529Z",
     "iopub.status.idle": "2023-04-02T05:34:57.947361Z",
     "shell.execute_reply": "2023-04-02T05:34:57.946012Z",
     "shell.execute_reply.started": "2023-04-02T05:20:59.837006Z"
    },
    "id": "S_Nw84E0MpoN",
    "outputId": "bd5ac469-a045-40dc-8817-0ddb27293db5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "500/500 [==============================] - 24s 21ms/step - loss: 1.0671 - accuracy: 0.6740 - val_loss: 0.4542 - val_accuracy: 0.8522\n",
      "Epoch 2/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3556 - accuracy: 0.8854 - val_loss: 0.3108 - val_accuracy: 0.8984\n",
      "Epoch 3/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.2401 - accuracy: 0.9221 - val_loss: 0.2242 - val_accuracy: 0.9280\n",
      "Epoch 4/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1837 - accuracy: 0.9410 - val_loss: 0.1986 - val_accuracy: 0.9376\n",
      "Epoch 5/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1512 - accuracy: 0.9510 - val_loss: 0.1755 - val_accuracy: 0.9448\n",
      "Epoch 6/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.1260 - accuracy: 0.9596 - val_loss: 0.1699 - val_accuracy: 0.9463\n",
      "Epoch 7/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.1066 - accuracy: 0.9654 - val_loss: 0.1532 - val_accuracy: 0.9536\n",
      "Epoch 8/350\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.0917 - accuracy: 0.9704 - val_loss: 0.2041 - val_accuracy: 0.9325\n",
      "Epoch 9/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0804 - accuracy: 0.9746 - val_loss: 0.1437 - val_accuracy: 0.9557\n",
      "Epoch 10/350\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.0674 - accuracy: 0.9785 - val_loss: 0.1290 - val_accuracy: 0.9616\n",
      "Epoch 11/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0604 - accuracy: 0.9803 - val_loss: 0.1350 - val_accuracy: 0.9609\n",
      "Epoch 12/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0530 - accuracy: 0.9824 - val_loss: 0.1435 - val_accuracy: 0.9608\n",
      "Epoch 13/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0464 - accuracy: 0.9848 - val_loss: 0.1393 - val_accuracy: 0.9612\n",
      "Epoch 14/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0440 - accuracy: 0.9855 - val_loss: 0.1393 - val_accuracy: 0.9636\n",
      "Epoch 15/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0410 - accuracy: 0.9865 - val_loss: 0.1333 - val_accuracy: 0.9628\n",
      "Epoch 16/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0333 - accuracy: 0.9890 - val_loss: 0.1329 - val_accuracy: 0.9652\n",
      "Epoch 17/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0335 - accuracy: 0.9886 - val_loss: 0.1402 - val_accuracy: 0.9643\n",
      "Epoch 18/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0305 - accuracy: 0.9899 - val_loss: 0.1362 - val_accuracy: 0.9674\n",
      "Epoch 19/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0273 - accuracy: 0.9906 - val_loss: 0.1686 - val_accuracy: 0.9617\n",
      "Epoch 20/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0266 - accuracy: 0.9907 - val_loss: 0.1427 - val_accuracy: 0.9661\n",
      "Epoch 21/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0228 - accuracy: 0.9922 - val_loss: 0.1482 - val_accuracy: 0.9659\n",
      "Epoch 22/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0208 - accuracy: 0.9929 - val_loss: 0.1539 - val_accuracy: 0.9674\n",
      "Epoch 23/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0232 - accuracy: 0.9921 - val_loss: 0.1643 - val_accuracy: 0.9639\n",
      "Epoch 24/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0194 - accuracy: 0.9932 - val_loss: 0.1605 - val_accuracy: 0.9659\n",
      "Epoch 25/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0173 - accuracy: 0.9940 - val_loss: 0.1655 - val_accuracy: 0.9648\n",
      "Epoch 26/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0160 - accuracy: 0.9944 - val_loss: 0.1673 - val_accuracy: 0.9672\n",
      "Epoch 27/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0203 - accuracy: 0.9930 - val_loss: 0.1668 - val_accuracy: 0.9655\n",
      "Epoch 28/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0150 - accuracy: 0.9950 - val_loss: 0.1694 - val_accuracy: 0.9676\n",
      "Epoch 29/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0154 - accuracy: 0.9947 - val_loss: 0.1661 - val_accuracy: 0.9658\n",
      "Epoch 30/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0166 - accuracy: 0.9943 - val_loss: 0.1802 - val_accuracy: 0.9671\n",
      "Epoch 31/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0153 - accuracy: 0.9950 - val_loss: 0.1623 - val_accuracy: 0.9693\n",
      "Epoch 32/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0117 - accuracy: 0.9960 - val_loss: 0.1863 - val_accuracy: 0.9664\n",
      "Epoch 33/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0133 - accuracy: 0.9956 - val_loss: 0.1894 - val_accuracy: 0.9648\n",
      "Epoch 34/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0138 - accuracy: 0.9952 - val_loss: 0.1757 - val_accuracy: 0.9677\n",
      "Epoch 35/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0122 - accuracy: 0.9960 - val_loss: 0.1909 - val_accuracy: 0.9667\n",
      "Epoch 36/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0135 - accuracy: 0.9955 - val_loss: 0.1867 - val_accuracy: 0.9685\n",
      "Epoch 37/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0142 - accuracy: 0.9951 - val_loss: 0.1981 - val_accuracy: 0.9670\n",
      "Epoch 38/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0088 - accuracy: 0.9970 - val_loss: 0.1877 - val_accuracy: 0.9677\n",
      "Epoch 39/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0118 - accuracy: 0.9960 - val_loss: 0.2114 - val_accuracy: 0.9643\n",
      "Epoch 40/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0110 - accuracy: 0.9964 - val_loss: 0.1886 - val_accuracy: 0.9688\n",
      "Epoch 41/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0109 - accuracy: 0.9965 - val_loss: 0.1789 - val_accuracy: 0.9683\n",
      "Epoch 42/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0112 - accuracy: 0.9963 - val_loss: 0.2101 - val_accuracy: 0.9631\n",
      "Epoch 43/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0104 - accuracy: 0.9964 - val_loss: 0.2248 - val_accuracy: 0.9662\n",
      "Epoch 44/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.1939 - val_accuracy: 0.9691\n",
      "Epoch 45/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0127 - accuracy: 0.9957 - val_loss: 0.1832 - val_accuracy: 0.9691\n",
      "Epoch 46/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0070 - accuracy: 0.9976 - val_loss: 0.2180 - val_accuracy: 0.9661\n",
      "Epoch 47/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0096 - accuracy: 0.9970 - val_loss: 0.2271 - val_accuracy: 0.9663\n",
      "Epoch 48/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0095 - accuracy: 0.9968 - val_loss: 0.2060 - val_accuracy: 0.9684\n",
      "Epoch 49/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0100 - accuracy: 0.9966 - val_loss: 0.2179 - val_accuracy: 0.9672\n",
      "Epoch 50/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0125 - accuracy: 0.9957 - val_loss: 0.2046 - val_accuracy: 0.9691\n",
      "Epoch 51/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0073 - accuracy: 0.9976 - val_loss: 0.2001 - val_accuracy: 0.9706\n",
      "Epoch 52/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0080 - accuracy: 0.9972 - val_loss: 0.2174 - val_accuracy: 0.9669\n",
      "Epoch 53/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0109 - accuracy: 0.9965 - val_loss: 0.2090 - val_accuracy: 0.9694\n",
      "Epoch 54/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0092 - accuracy: 0.9970 - val_loss: 0.1967 - val_accuracy: 0.9692\n",
      "Epoch 55/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0060 - accuracy: 0.9979 - val_loss: 0.2351 - val_accuracy: 0.9665\n",
      "Epoch 56/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0120 - accuracy: 0.9960 - val_loss: 0.2366 - val_accuracy: 0.9665\n",
      "Epoch 57/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0103 - accuracy: 0.9968 - val_loss: 0.2274 - val_accuracy: 0.9669\n",
      "Epoch 58/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.2016 - val_accuracy: 0.9707\n",
      "Epoch 59/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.2207 - val_accuracy: 0.9698\n",
      "Epoch 60/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0102 - accuracy: 0.9967 - val_loss: 0.2332 - val_accuracy: 0.9669\n",
      "Epoch 61/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.2212 - val_accuracy: 0.9699\n",
      "Epoch 62/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0102 - accuracy: 0.9967 - val_loss: 0.2263 - val_accuracy: 0.9683\n",
      "Epoch 63/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.2286 - val_accuracy: 0.9701\n",
      "Epoch 64/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0055 - accuracy: 0.9981 - val_loss: 0.2244 - val_accuracy: 0.9684\n",
      "Epoch 65/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.2268 - val_accuracy: 0.9686\n",
      "Epoch 66/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.2285 - val_accuracy: 0.9696\n",
      "Epoch 67/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.2206 - val_accuracy: 0.9704\n",
      "Epoch 68/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.2346 - val_accuracy: 0.9674\n",
      "Epoch 69/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0071 - accuracy: 0.9977 - val_loss: 0.2223 - val_accuracy: 0.9701\n",
      "Epoch 70/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0084 - accuracy: 0.9973 - val_loss: 0.2403 - val_accuracy: 0.9678\n",
      "Epoch 71/350\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.0058 - accuracy: 0.9981 - val_loss: 0.2451 - val_accuracy: 0.9674\n",
      "Epoch 72/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.2497 - val_accuracy: 0.9682\n",
      "Epoch 73/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.2335 - val_accuracy: 0.9700\n",
      "Epoch 74/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0069 - accuracy: 0.9979 - val_loss: 0.2503 - val_accuracy: 0.9666\n",
      "Epoch 75/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0069 - accuracy: 0.9979 - val_loss: 0.2336 - val_accuracy: 0.9691\n",
      "Epoch 76/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.2742 - val_accuracy: 0.9660\n",
      "Epoch 77/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.2551 - val_accuracy: 0.9689\n",
      "Epoch 78/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0068 - accuracy: 0.9979 - val_loss: 0.2464 - val_accuracy: 0.9679\n",
      "Epoch 79/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.2328 - val_accuracy: 0.9719\n",
      "Epoch 80/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.2442 - val_accuracy: 0.9695\n",
      "Epoch 81/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.2356 - val_accuracy: 0.9692\n",
      "Epoch 82/350\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.0056 - accuracy: 0.9981 - val_loss: 0.2391 - val_accuracy: 0.9712\n",
      "Epoch 83/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.2574 - val_accuracy: 0.9681\n",
      "Epoch 84/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.2597 - val_accuracy: 0.9666\n",
      "Epoch 85/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.2416 - val_accuracy: 0.9707\n",
      "Epoch 86/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.2916 - val_accuracy: 0.9667\n",
      "Epoch 87/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.2344 - val_accuracy: 0.9709\n",
      "Epoch 88/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.2758 - val_accuracy: 0.9664\n",
      "Epoch 89/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0092 - accuracy: 0.9972 - val_loss: 0.2601 - val_accuracy: 0.9681\n",
      "Epoch 90/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.2535 - val_accuracy: 0.9688\n",
      "Epoch 91/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.2615 - val_accuracy: 0.9706\n",
      "Epoch 92/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.2762 - val_accuracy: 0.9671\n",
      "Epoch 93/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.2632 - val_accuracy: 0.9685\n",
      "Epoch 94/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.2610 - val_accuracy: 0.9698\n",
      "Epoch 95/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.2623 - val_accuracy: 0.9681\n",
      "Epoch 96/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.2697 - val_accuracy: 0.9693\n",
      "Epoch 97/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0069 - accuracy: 0.9980 - val_loss: 0.2667 - val_accuracy: 0.9679\n",
      "Epoch 98/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.2632 - val_accuracy: 0.9707\n",
      "Epoch 99/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.2908 - val_accuracy: 0.9708\n",
      "Epoch 100/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.2745 - val_accuracy: 0.9710\n",
      "Epoch 101/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 0.2850 - val_accuracy: 0.9714\n",
      "Epoch 102/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0067 - accuracy: 0.9981 - val_loss: 0.2693 - val_accuracy: 0.9709\n",
      "Epoch 103/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 0.2999 - val_accuracy: 0.9693\n",
      "Epoch 104/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0056 - accuracy: 0.9984 - val_loss: 0.2969 - val_accuracy: 0.9678\n",
      "Epoch 105/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.2820 - val_accuracy: 0.9697\n",
      "Epoch 106/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 0.2956 - val_accuracy: 0.9681\n",
      "Epoch 107/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.2988 - val_accuracy: 0.9671\n",
      "Epoch 108/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0081 - accuracy: 0.9977 - val_loss: 0.2696 - val_accuracy: 0.9711\n",
      "Epoch 109/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 0.2720 - val_accuracy: 0.9702\n",
      "Epoch 110/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.2814 - val_accuracy: 0.9707\n",
      "Epoch 111/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.2934 - val_accuracy: 0.9699\n",
      "Epoch 112/350\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.0060 - accuracy: 0.9983 - val_loss: 0.3049 - val_accuracy: 0.9689\n",
      "Epoch 113/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0072 - accuracy: 0.9980 - val_loss: 0.2808 - val_accuracy: 0.9698\n",
      "Epoch 114/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.2880 - val_accuracy: 0.9687\n",
      "Epoch 115/350\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.3032 - val_accuracy: 0.9689\n",
      "Epoch 116/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.2997 - val_accuracy: 0.9676\n",
      "Epoch 117/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.2787 - val_accuracy: 0.9714\n",
      "Epoch 118/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.2948 - val_accuracy: 0.9706\n",
      "Epoch 119/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0063 - accuracy: 0.9983 - val_loss: 0.3085 - val_accuracy: 0.9676\n",
      "Epoch 120/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.3216 - val_accuracy: 0.9694\n",
      "Epoch 121/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.3115 - val_accuracy: 0.9695\n",
      "Epoch 122/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0060 - accuracy: 0.9983 - val_loss: 0.2960 - val_accuracy: 0.9699\n",
      "Epoch 123/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.3040 - val_accuracy: 0.9704\n",
      "Epoch 124/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.3059 - val_accuracy: 0.9707\n",
      "Epoch 125/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0045 - accuracy: 0.9989 - val_loss: 0.2883 - val_accuracy: 0.9719\n",
      "Epoch 126/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.3087 - val_accuracy: 0.9701\n",
      "Epoch 127/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.3006 - val_accuracy: 0.9709\n",
      "Epoch 128/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.3338 - val_accuracy: 0.9688\n",
      "Epoch 129/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.3319 - val_accuracy: 0.9688\n",
      "Epoch 130/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.3375 - val_accuracy: 0.9669\n",
      "Epoch 131/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.3369 - val_accuracy: 0.9692\n",
      "Epoch 132/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0060 - accuracy: 0.9984 - val_loss: 0.3348 - val_accuracy: 0.9696\n",
      "Epoch 133/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.3157 - val_accuracy: 0.9690\n",
      "Epoch 134/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.3279 - val_accuracy: 0.9687\n",
      "Epoch 135/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.3079 - val_accuracy: 0.9704\n",
      "Epoch 136/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.3409 - val_accuracy: 0.9695\n",
      "Epoch 137/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.3370 - val_accuracy: 0.9684\n",
      "Epoch 138/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.3401 - val_accuracy: 0.9701\n",
      "Epoch 139/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.3274 - val_accuracy: 0.9711\n",
      "Epoch 140/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.3309 - val_accuracy: 0.9704\n",
      "Epoch 141/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.3240 - val_accuracy: 0.9697\n",
      "Epoch 142/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.3303 - val_accuracy: 0.9705\n",
      "Epoch 143/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0059 - accuracy: 0.9986 - val_loss: 0.3410 - val_accuracy: 0.9706\n",
      "Epoch 144/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.3186 - val_accuracy: 0.9711\n",
      "Epoch 145/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.3206 - val_accuracy: 0.9716\n",
      "Epoch 146/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0077 - accuracy: 0.9980 - val_loss: 0.3183 - val_accuracy: 0.9713\n",
      "Epoch 147/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.3531 - val_accuracy: 0.9701\n",
      "Epoch 148/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.3622 - val_accuracy: 0.9673\n",
      "Epoch 149/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.3288 - val_accuracy: 0.9719\n",
      "Epoch 150/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.3238 - val_accuracy: 0.9713\n",
      "Epoch 151/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.3409 - val_accuracy: 0.9701\n",
      "Epoch 152/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.3523 - val_accuracy: 0.9670\n",
      "Epoch 153/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.3462 - val_accuracy: 0.9699\n",
      "Epoch 154/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.3244 - val_accuracy: 0.9730\n",
      "Epoch 155/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0067 - accuracy: 0.9983 - val_loss: 0.3590 - val_accuracy: 0.9700\n",
      "Epoch 156/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.3520 - val_accuracy: 0.9708\n",
      "Epoch 157/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0052 - accuracy: 0.9986 - val_loss: 0.3639 - val_accuracy: 0.9709\n",
      "Epoch 158/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.3763 - val_accuracy: 0.9702\n",
      "Epoch 159/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.3631 - val_accuracy: 0.9704\n",
      "Epoch 160/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 0.3715 - val_accuracy: 0.9708\n",
      "Epoch 161/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.3491 - val_accuracy: 0.9719\n",
      "Epoch 162/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.3494 - val_accuracy: 0.9709\n",
      "Epoch 163/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.3510 - val_accuracy: 0.9710\n",
      "Epoch 164/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.3739 - val_accuracy: 0.9679\n",
      "Epoch 165/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.3539 - val_accuracy: 0.9717\n",
      "Epoch 166/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.3484 - val_accuracy: 0.9716\n",
      "Epoch 167/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.3624 - val_accuracy: 0.9698\n",
      "Epoch 168/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.3611 - val_accuracy: 0.9709\n",
      "Epoch 169/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.3674 - val_accuracy: 0.9706\n",
      "Epoch 170/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.3526 - val_accuracy: 0.9718\n",
      "Epoch 171/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0056 - accuracy: 0.9986 - val_loss: 0.3839 - val_accuracy: 0.9683\n",
      "Epoch 172/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.3590 - val_accuracy: 0.9697\n",
      "Epoch 173/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 0.3889 - val_accuracy: 0.9710\n",
      "Epoch 174/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0078 - accuracy: 0.9982 - val_loss: 0.3684 - val_accuracy: 0.9710\n",
      "Epoch 175/350\n",
      "500/500 [==============================] - 12s 24ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.3711 - val_accuracy: 0.9704\n",
      "Epoch 176/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.4076 - val_accuracy: 0.9699\n",
      "Epoch 177/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.3877 - val_accuracy: 0.9698\n",
      "Epoch 178/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0058 - accuracy: 0.9986 - val_loss: 0.3781 - val_accuracy: 0.9717\n",
      "Epoch 179/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.3934 - val_accuracy: 0.9723\n",
      "Epoch 180/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0064 - accuracy: 0.9986 - val_loss: 0.4146 - val_accuracy: 0.9702\n",
      "Epoch 181/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0065 - accuracy: 0.9986 - val_loss: 0.4160 - val_accuracy: 0.9704\n",
      "Epoch 182/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0052 - accuracy: 0.9989 - val_loss: 0.3807 - val_accuracy: 0.9709\n",
      "Epoch 183/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.4112 - val_accuracy: 0.9695\n",
      "Epoch 184/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0058 - accuracy: 0.9987 - val_loss: 0.4888 - val_accuracy: 0.9662\n",
      "Epoch 185/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.3647 - val_accuracy: 0.9703\n",
      "Epoch 186/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0045 - accuracy: 0.9989 - val_loss: 0.4002 - val_accuracy: 0.9706\n",
      "Epoch 187/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.4333 - val_accuracy: 0.9702\n",
      "Epoch 188/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 0.4196 - val_accuracy: 0.9708\n",
      "Epoch 189/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.4116 - val_accuracy: 0.9717\n",
      "Epoch 190/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.3998 - val_accuracy: 0.9717\n",
      "Epoch 191/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.4065 - val_accuracy: 0.9711\n",
      "Epoch 192/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.4293 - val_accuracy: 0.9710\n",
      "Epoch 193/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.4044 - val_accuracy: 0.9717\n",
      "Epoch 194/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0054 - accuracy: 0.9988 - val_loss: 0.4155 - val_accuracy: 0.9709\n",
      "Epoch 195/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0062 - accuracy: 0.9986 - val_loss: 0.4019 - val_accuracy: 0.9714\n",
      "Epoch 196/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.4153 - val_accuracy: 0.9702\n",
      "Epoch 197/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.4281 - val_accuracy: 0.9704\n",
      "Epoch 198/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0066 - accuracy: 0.9986 - val_loss: 0.4084 - val_accuracy: 0.9716\n",
      "Epoch 199/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0051 - accuracy: 0.9989 - val_loss: 0.4073 - val_accuracy: 0.9710\n",
      "Epoch 200/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.4236 - val_accuracy: 0.9709\n",
      "Epoch 201/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.4193 - val_accuracy: 0.9707\n",
      "Epoch 202/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.4298 - val_accuracy: 0.9714\n",
      "Epoch 203/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.4624 - val_accuracy: 0.9696\n",
      "Epoch 204/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.4331 - val_accuracy: 0.9673\n",
      "Epoch 205/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.4388 - val_accuracy: 0.9704\n",
      "Epoch 206/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.4381 - val_accuracy: 0.9714\n",
      "Epoch 207/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0065 - accuracy: 0.9986 - val_loss: 0.4548 - val_accuracy: 0.9687\n",
      "Epoch 208/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0043 - accuracy: 0.9991 - val_loss: 0.4081 - val_accuracy: 0.9719\n",
      "Epoch 209/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.4714 - val_accuracy: 0.9689\n",
      "Epoch 210/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0063 - accuracy: 0.9987 - val_loss: 0.4222 - val_accuracy: 0.9715\n",
      "Epoch 211/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.4434 - val_accuracy: 0.9707\n",
      "Epoch 212/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0063 - accuracy: 0.9987 - val_loss: 0.4171 - val_accuracy: 0.9731\n",
      "Epoch 213/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0061 - accuracy: 0.9986 - val_loss: 0.4131 - val_accuracy: 0.9721\n",
      "Epoch 214/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.4401 - val_accuracy: 0.9716\n",
      "Epoch 215/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.4339 - val_accuracy: 0.9719\n",
      "Epoch 216/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 0.4295 - val_accuracy: 0.9724\n",
      "Epoch 217/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0044 - accuracy: 0.9991 - val_loss: 0.4926 - val_accuracy: 0.9694\n",
      "Epoch 218/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.4273 - val_accuracy: 0.9723\n",
      "Epoch 219/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0061 - accuracy: 0.9988 - val_loss: 0.4589 - val_accuracy: 0.9716\n",
      "Epoch 220/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 0.4705 - val_accuracy: 0.9698\n",
      "Epoch 221/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0057 - accuracy: 0.9988 - val_loss: 0.4229 - val_accuracy: 0.9726\n",
      "Epoch 222/350\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.4506 - val_accuracy: 0.9707\n",
      "Epoch 223/350\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.5183 - val_accuracy: 0.9685\n",
      "Epoch 224/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0067 - accuracy: 0.9986 - val_loss: 0.5524 - val_accuracy: 0.9678\n",
      "Epoch 225/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.4661 - val_accuracy: 0.9705\n",
      "Epoch 226/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0046 - accuracy: 0.9990 - val_loss: 0.4716 - val_accuracy: 0.9705\n",
      "Epoch 227/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 0.5462 - val_accuracy: 0.9694\n",
      "Epoch 228/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0059 - accuracy: 0.9987 - val_loss: 0.5091 - val_accuracy: 0.9704\n",
      "Epoch 229/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.4643 - val_accuracy: 0.9728\n",
      "Epoch 230/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.4847 - val_accuracy: 0.9715\n",
      "Epoch 231/350\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.0061 - accuracy: 0.9989 - val_loss: 0.5196 - val_accuracy: 0.9698\n",
      "Epoch 232/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0066 - accuracy: 0.9988 - val_loss: 0.5066 - val_accuracy: 0.9701\n",
      "Epoch 233/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0052 - accuracy: 0.9990 - val_loss: 0.5064 - val_accuracy: 0.9690\n",
      "Epoch 234/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.4811 - val_accuracy: 0.9704\n",
      "Epoch 235/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.4911 - val_accuracy: 0.9709\n",
      "Epoch 236/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.5114 - val_accuracy: 0.9705\n",
      "Epoch 237/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0053 - accuracy: 0.9989 - val_loss: 0.5308 - val_accuracy: 0.9695\n",
      "Epoch 238/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.5236 - val_accuracy: 0.9705\n",
      "Epoch 239/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 0.4848 - val_accuracy: 0.9720\n",
      "Epoch 240/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.5072 - val_accuracy: 0.9701\n",
      "Epoch 241/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0063 - accuracy: 0.9987 - val_loss: 0.4927 - val_accuracy: 0.9709\n",
      "Epoch 242/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.5139 - val_accuracy: 0.9700\n",
      "Epoch 243/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0065 - accuracy: 0.9987 - val_loss: 0.5215 - val_accuracy: 0.9712\n",
      "Epoch 244/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.5178 - val_accuracy: 0.9711\n",
      "Epoch 245/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0050 - accuracy: 0.9990 - val_loss: 0.4936 - val_accuracy: 0.9715\n",
      "Epoch 246/350\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.0057 - accuracy: 0.9989 - val_loss: 0.5240 - val_accuracy: 0.9695\n",
      "Epoch 247/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.5210 - val_accuracy: 0.9705\n",
      "Epoch 248/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.5206 - val_accuracy: 0.9712\n",
      "Epoch 249/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.4696 - val_accuracy: 0.9714\n",
      "Epoch 250/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0061 - accuracy: 0.9989 - val_loss: 0.5443 - val_accuracy: 0.9691\n",
      "Epoch 251/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.4955 - val_accuracy: 0.9706\n",
      "Epoch 252/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.5487 - val_accuracy: 0.9715\n",
      "Epoch 253/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.5430 - val_accuracy: 0.9704\n",
      "Epoch 254/350\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.0061 - accuracy: 0.9989 - val_loss: 0.5279 - val_accuracy: 0.9699\n",
      "Epoch 255/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.5534 - val_accuracy: 0.9694\n",
      "Epoch 256/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0064 - accuracy: 0.9988 - val_loss: 0.5383 - val_accuracy: 0.9701\n",
      "Epoch 257/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.5083 - val_accuracy: 0.9728\n",
      "Epoch 258/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.5624 - val_accuracy: 0.9699\n",
      "Epoch 259/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0071 - accuracy: 0.9987 - val_loss: 0.5233 - val_accuracy: 0.9710\n",
      "Epoch 260/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0065 - accuracy: 0.9988 - val_loss: 0.5538 - val_accuracy: 0.9699\n",
      "Epoch 261/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.5249 - val_accuracy: 0.9733\n",
      "Epoch 262/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 0.4938 - val_accuracy: 0.9717\n",
      "Epoch 263/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.5880 - val_accuracy: 0.9699\n",
      "Epoch 264/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0079 - accuracy: 0.9987 - val_loss: 0.5250 - val_accuracy: 0.9694\n",
      "Epoch 265/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 0.5403 - val_accuracy: 0.9707\n",
      "Epoch 266/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0041 - accuracy: 0.9993 - val_loss: 0.5660 - val_accuracy: 0.9707\n",
      "Epoch 267/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0052 - accuracy: 0.9990 - val_loss: 0.5614 - val_accuracy: 0.9710\n",
      "Epoch 268/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.5444 - val_accuracy: 0.9691\n",
      "Epoch 269/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0063 - accuracy: 0.9988 - val_loss: 0.5689 - val_accuracy: 0.9691\n",
      "Epoch 270/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.5472 - val_accuracy: 0.9711\n",
      "Epoch 271/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.5175 - val_accuracy: 0.9713\n",
      "Epoch 272/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.5475 - val_accuracy: 0.9698\n",
      "Epoch 273/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0044 - accuracy: 0.9992 - val_loss: 0.5629 - val_accuracy: 0.9702\n",
      "Epoch 274/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0054 - accuracy: 0.9990 - val_loss: 0.5516 - val_accuracy: 0.9719\n",
      "Epoch 275/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.5794 - val_accuracy: 0.9703\n",
      "Epoch 276/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0043 - accuracy: 0.9992 - val_loss: 0.5761 - val_accuracy: 0.9704\n",
      "Epoch 277/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 0.5971 - val_accuracy: 0.9708\n",
      "Epoch 278/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 0.5862 - val_accuracy: 0.9707\n",
      "Epoch 279/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0042 - accuracy: 0.9993 - val_loss: 0.5520 - val_accuracy: 0.9712\n",
      "Epoch 280/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.5289 - val_accuracy: 0.9718\n",
      "Epoch 281/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0055 - accuracy: 0.9990 - val_loss: 0.6148 - val_accuracy: 0.9695\n",
      "Epoch 282/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.5610 - val_accuracy: 0.9713\n",
      "Epoch 283/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0053 - accuracy: 0.9991 - val_loss: 0.6061 - val_accuracy: 0.9696\n",
      "Epoch 284/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0068 - accuracy: 0.9989 - val_loss: 0.5806 - val_accuracy: 0.9705\n",
      "Epoch 285/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 0.5590 - val_accuracy: 0.9721\n",
      "Epoch 286/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0043 - accuracy: 0.9992 - val_loss: 0.5964 - val_accuracy: 0.9715\n",
      "Epoch 287/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0042 - accuracy: 0.9992 - val_loss: 0.5724 - val_accuracy: 0.9714\n",
      "Epoch 288/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0053 - accuracy: 0.9991 - val_loss: 0.5665 - val_accuracy: 0.9709\n",
      "Epoch 289/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.5799 - val_accuracy: 0.9728\n",
      "Epoch 290/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0061 - accuracy: 0.9990 - val_loss: 0.5996 - val_accuracy: 0.9705\n",
      "Epoch 291/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 0.5975 - val_accuracy: 0.9707\n",
      "Epoch 292/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0052 - accuracy: 0.9992 - val_loss: 0.6163 - val_accuracy: 0.9699\n",
      "Epoch 293/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0061 - accuracy: 0.9991 - val_loss: 0.5977 - val_accuracy: 0.9715\n",
      "Epoch 294/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0067 - accuracy: 0.9989 - val_loss: 0.6078 - val_accuracy: 0.9715\n",
      "Epoch 295/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.5963 - val_accuracy: 0.9723\n",
      "Epoch 296/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0068 - accuracy: 0.9988 - val_loss: 0.6043 - val_accuracy: 0.9712\n",
      "Epoch 297/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0041 - accuracy: 0.9993 - val_loss: 0.6235 - val_accuracy: 0.9700\n",
      "Epoch 298/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.6026 - val_accuracy: 0.9713\n",
      "Epoch 299/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0079 - accuracy: 0.9987 - val_loss: 0.5978 - val_accuracy: 0.9703\n",
      "Epoch 300/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0048 - accuracy: 0.9992 - val_loss: 0.6322 - val_accuracy: 0.9711\n",
      "Epoch 301/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0058 - accuracy: 0.9990 - val_loss: 0.6376 - val_accuracy: 0.9705\n",
      "Epoch 302/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.6492 - val_accuracy: 0.9708\n",
      "Epoch 303/350\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.6668 - val_accuracy: 0.9715\n",
      "Epoch 304/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.0054 - accuracy: 0.9991 - val_loss: 0.6373 - val_accuracy: 0.9708\n",
      "Epoch 305/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0047 - accuracy: 0.9991 - val_loss: 0.5972 - val_accuracy: 0.9726\n",
      "Epoch 306/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0070 - accuracy: 0.9989 - val_loss: 0.6452 - val_accuracy: 0.9702\n",
      "Epoch 307/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.6096 - val_accuracy: 0.9716\n",
      "Epoch 308/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.6227 - val_accuracy: 0.9710\n",
      "Epoch 309/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0062 - accuracy: 0.9990 - val_loss: 0.6099 - val_accuracy: 0.9703\n",
      "Epoch 310/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.6953 - val_accuracy: 0.9691\n",
      "Epoch 311/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0042 - accuracy: 0.9993 - val_loss: 0.6624 - val_accuracy: 0.9694\n",
      "Epoch 312/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0059 - accuracy: 0.9989 - val_loss: 0.5991 - val_accuracy: 0.9710\n",
      "Epoch 313/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.5944 - val_accuracy: 0.9727\n",
      "Epoch 314/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0058 - accuracy: 0.9990 - val_loss: 0.6242 - val_accuracy: 0.9709\n",
      "Epoch 315/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.6578 - val_accuracy: 0.9695\n",
      "Epoch 316/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0061 - accuracy: 0.9990 - val_loss: 0.6626 - val_accuracy: 0.9708\n",
      "Epoch 317/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 0.6479 - val_accuracy: 0.9721\n",
      "Epoch 318/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.6247 - val_accuracy: 0.9721\n",
      "Epoch 319/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0040 - accuracy: 0.9993 - val_loss: 0.6638 - val_accuracy: 0.9718\n",
      "Epoch 320/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0069 - accuracy: 0.9989 - val_loss: 0.6711 - val_accuracy: 0.9715\n",
      "Epoch 321/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0049 - accuracy: 0.9992 - val_loss: 0.6634 - val_accuracy: 0.9710\n",
      "Epoch 322/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.6984 - val_accuracy: 0.9710\n",
      "Epoch 323/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0049 - accuracy: 0.9992 - val_loss: 0.6540 - val_accuracy: 0.9719\n",
      "Epoch 324/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0058 - accuracy: 0.9991 - val_loss: 0.6406 - val_accuracy: 0.9719\n",
      "Epoch 325/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.7033 - val_accuracy: 0.9718\n",
      "Epoch 326/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0066 - accuracy: 0.9990 - val_loss: 0.7007 - val_accuracy: 0.9700\n",
      "Epoch 327/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0050 - accuracy: 0.9991 - val_loss: 0.6753 - val_accuracy: 0.9714\n",
      "Epoch 328/350\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.0050 - accuracy: 0.9992 - val_loss: 0.6706 - val_accuracy: 0.9732\n",
      "Epoch 329/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.6671 - val_accuracy: 0.9727\n",
      "Epoch 330/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0053 - accuracy: 0.9992 - val_loss: 0.6919 - val_accuracy: 0.9711\n",
      "Epoch 331/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0050 - accuracy: 0.9993 - val_loss: 0.6785 - val_accuracy: 0.9703\n",
      "Epoch 332/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0071 - accuracy: 0.9990 - val_loss: 0.7213 - val_accuracy: 0.9705\n",
      "Epoch 333/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0067 - accuracy: 0.9991 - val_loss: 0.6888 - val_accuracy: 0.9720\n",
      "Epoch 334/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0045 - accuracy: 0.9993 - val_loss: 0.7040 - val_accuracy: 0.9711\n",
      "Epoch 335/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0048 - accuracy: 0.9993 - val_loss: 0.7570 - val_accuracy: 0.9709\n",
      "Epoch 336/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0044 - accuracy: 0.9993 - val_loss: 0.7564 - val_accuracy: 0.9708\n",
      "Epoch 337/350\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.0049 - accuracy: 0.9992 - val_loss: 0.7311 - val_accuracy: 0.9715\n",
      "Epoch 338/350\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.7757 - val_accuracy: 0.9695\n",
      "Epoch 339/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0053 - accuracy: 0.9992 - val_loss: 0.7379 - val_accuracy: 0.9708\n",
      "Epoch 340/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0066 - accuracy: 0.9991 - val_loss: 0.7392 - val_accuracy: 0.9713\n",
      "Epoch 341/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0044 - accuracy: 0.9993 - val_loss: 0.7509 - val_accuracy: 0.9712\n",
      "Epoch 342/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0061 - accuracy: 0.9991 - val_loss: 0.6678 - val_accuracy: 0.9724\n",
      "Epoch 343/350\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0050 - accuracy: 0.9993 - val_loss: 0.7037 - val_accuracy: 0.9728\n",
      "Epoch 344/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0048 - accuracy: 0.9993 - val_loss: 0.7159 - val_accuracy: 0.9711\n",
      "Epoch 345/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0052 - accuracy: 0.9993 - val_loss: 0.7552 - val_accuracy: 0.9704\n",
      "Epoch 346/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0060 - accuracy: 0.9991 - val_loss: 0.7104 - val_accuracy: 0.9713\n",
      "Epoch 347/350\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.0057 - accuracy: 0.9991 - val_loss: 0.7717 - val_accuracy: 0.9718\n",
      "Epoch 348/350\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.7239 - val_accuracy: 0.9724\n",
      "Epoch 349/350\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.0057 - accuracy: 0.9992 - val_loss: 0.7497 - val_accuracy: 0.9712\n",
      "Epoch 350/350\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.0065 - accuracy: 0.9990 - val_loss: 0.7782 - val_accuracy: 0.9703\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "number_of_epoch = 350\n",
    "history = model.fit(X_train_converted, Y_train, steps_per_epoch=500, epochs=number_of_epoch, verbose=1, validation_data=(X_val_converted, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "V3CkTRPOkdFc"
   },
   "outputs": [],
   "source": [
    "model.save(\"model_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T05:34:57.951414Z",
     "iopub.status.busy": "2023-04-02T05:34:57.951003Z",
     "iopub.status.idle": "2023-04-02T05:34:57.961043Z",
     "shell.execute_reply": "2023-04-02T05:34:57.95953Z",
     "shell.execute_reply.started": "2023-04-02T05:34:57.951376Z"
    },
    "id": "38UvHdt4MpoN"
   },
   "outputs": [],
   "source": [
    "#creating a list for no of epochs\n",
    "n_epochs = list(range(1,number_of_epoch+1))\n",
    "\n",
    "#creating a dataframe of loss and accuracy values\n",
    "history_details_df = pd.DataFrame({\"Number of epochs\":n_epochs,\"Train_Accuracy\":history.history['accuracy'],\"Validation_accuracy\":history.history['val_accuracy'],\"Train_Loss\":history.history['loss'],\"Validation_loss\":history.history['val_loss']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:34:57.962828Z",
     "iopub.status.busy": "2023-04-02T05:34:57.962464Z",
     "iopub.status.idle": "2023-04-02T05:34:58.052189Z",
     "shell.execute_reply": "2023-04-02T05:34:58.050789Z",
     "shell.execute_reply.started": "2023-04-02T05:34:57.962796Z"
    },
    "id": "YeEoEI0OMpoN",
    "outputId": "fa527889-104c-413d-d6f4-db8d1da28ca2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"b3e3846f-cc99-4394-9437-a3326067b53b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b3e3846f-cc99-4394-9437-a3326067b53b\")) {                    Plotly.newPlot(                        \"b3e3846f-cc99-4394-9437-a3326067b53b\",                        [{\"hovertemplate\":\"variable=Train_Accuracy<br>Number of epochs=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"Train_Accuracy\",\"line\":{\"color\":\"rgb(27,158,119)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Train_Accuracy\",\"showlegend\":true,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350],\"xaxis\":\"x\",\"y\":[0.6739575862884521,0.8854213953018188,0.9221477508544922,0.9409544467926025,0.9509977698326111,0.9595885276794434,0.9653871059417725,0.9704474806785583,0.9746267199516296,0.9785082936286926,0.980312168598175,0.9824137091636658,0.9848248362541199,0.9854618906974792,0.9864560961723328,0.9890457987785339,0.988605260848999,0.9899328351020813,0.990623414516449,0.9907008409500122,0.9922070503234863,0.9929095506668091,0.9921236634254456,0.9932369589805603,0.9940466284751892,0.994433581829071,0.993022620677948,0.9950467944145203,0.994707465171814,0.9942728281021118,0.9950348734855652,0.9959517121315002,0.9955528378486633,0.995159924030304,0.9959874153137207,0.9955230355262756,0.9951420426368713,0.9970352053642273,0.9960350394248962,0.9964220523834229,0.9964696764945984,0.9963386654853821,0.9963625073432922,0.9964160919189453,0.995665967464447,0.9976364970207214,0.9970054626464844,0.9968327879905701,0.9966006278991699,0.9956718683242798,0.9976305365562439,0.9971840381622314,0.9964934587478638,0.9969935417175293,0.9979460835456848,0.9960052967071533,0.9968149662017822,0.9976364970207214,0.997969925403595,0.9967435002326965,0.9985652565956116,0.9967197179794312,0.9973983764648438,0.9981306195259094,0.9972674250602722,0.9977317452430725,0.9969161748886108,0.9982497096061707,0.997725784778595,0.9973090887069702,0.9981008768081665,0.9977555871009827,0.997231662273407,0.9978687167167664,0.9979341626167297,0.9982794523239136,0.9970887899398804,0.9979282021522522,0.9980056285858154,0.9979758262634277,0.9974638819694519,0.9981425404548645,0.9973924160003662,0.9983211755752563,0.9984402060508728,0.9983925819396973,0.9976484179496765,0.9985771179199219,0.997231662273407,0.9982020854949951,0.9984818696975708,0.9983270764350891,0.9981961250305176,0.9983092546463013,0.9979460835456848,0.9984997510910034,0.9979639649391174,0.9985116720199585,0.9981842041015625,0.997779369354248,0.9985771179199219,0.998065173625946,0.9983687996864319,0.9983747005462646,0.9978032112121582,0.9987438321113586,0.9985830783843994,0.997696042060852,0.9986247420310974,0.998231828212738,0.9989343285560608,0.9982913732528687,0.9979817867279053,0.9984937906265259,0.9983747005462646,0.9982556700706482,0.9982497096061707,0.9990474581718445,0.9982735514640808,0.9979996681213379,0.9986605048179626,0.9983330368995667,0.9988033771514893,0.9987617135047913,0.9988867044448853,0.9984342455863953,0.998231828212738,0.9987319111824036,0.9986783266067505,0.9987497925758362,0.9983152151107788,0.9983628392219543,0.9984104633331299,0.9985235929489136,0.998725950717926,0.998725950717926,0.9986545443534851,0.998053252696991,0.9986128807067871,0.9989759922027588,0.9984700083732605,0.9985414147377014,0.998600959777832,0.9987974166870117,0.9991010427474976,0.9980294108390808,0.9987021684646606,0.9986545443534851,0.9985652565956116,0.9986485838890076,0.9986069202423096,0.998600959777832,0.9988867044448853,0.999011754989624,0.9983270764350891,0.9988152980804443,0.9985949993133545,0.9987617135047913,0.9986366629600525,0.9987081289291382,0.9988271594047546,0.9986605048179626,0.9987319111824036,0.9984521269798279,0.9986128807067871,0.9993213415145874,0.9992379546165466,0.9984521269798279,0.9988569617271423,0.9987854957580566,0.9986128807067871,0.9987795352935791,0.9988986253738403,0.9982140064239502,0.998999834060669,0.9990355372428894,0.9988510012626648,0.9986188411712646,0.9987974166870117,0.998589038848877,0.998559296131134,0.9988867044448853,0.9990474581718445,0.9987081289291382,0.9987617135047913,0.9989283680915833,0.9988569617271423,0.998916506767273,0.9984580874443054,0.999053418636322,0.999011754989624,0.9990057945251465,0.9990474581718445,0.9988033771514893,0.9985830783843994,0.9990057945251465,0.9989759922027588,0.9985771179199219,0.9988807439804077,0.9990891218185425,0.9989105463027954,0.9991962909698486,0.9987557530403137,0.9985414147377014,0.9989819526672363,0.9991070032119751,0.9985771179199219,0.9990713000297546,0.9992617964744568,0.9986545443534851,0.9990295767784119,0.9986605048179626,0.9986247420310974,0.9993689656257629,0.9989522099494934,0.9990355372428894,0.9990593791007996,0.9987200498580933,0.9988450407981873,0.9988510012626648,0.9987795352935791,0.9991962909698486,0.9993332028388977,0.9985830783843994,0.9985235929489136,0.9990057945251465,0.9989641308784485,0.9987140893936157,0.9994344115257263,0.9992617964744568,0.9988510012626648,0.9987736344337463,0.9990414977073669,0.9991546273231506,0.9989641308784485,0.9991843700408936,0.9989283680915833,0.9987974166870117,0.9991725087165833,0.9990355372428894,0.9987319111824036,0.9990891218185425,0.9987438321113586,0.9993213415145874,0.9990414977073669,0.9989283680915833,0.9994999170303345,0.9988152980804443,0.9990653395652771,0.9988629221916199,0.9990295767784119,0.9991427063941956,0.9990295767784119,0.9988926649093628,0.99909508228302,0.9988212585449219,0.9994403719902039,0.9993808269500732,0.9986664652824402,0.9988390803337097,0.9993272423744202,0.9991725087165833,0.9990713000297546,0.9987200498580933,0.9991962909698486,0.9992796182632446,0.998999834060669,0.99909508228302,0.9988033771514893,0.998958170413971,0.9991665482521057,0.9992974996566772,0.9992022514343262,0.9990057945251465,0.9992498755455017,0.9991605877876282,0.9989641308784485,0.9989641308784485,0.9992915391921997,0.9992677569389343,0.9989759922027588,0.9991307854652405,0.99909508228302,0.9988867044448853,0.9991903305053711,0.999178409576416,0.9992201328277588,0.9991189241409302,0.9992141723632812,0.9990414977073669,0.9991725087165833,0.9991962909698486,0.9990831613540649,0.9989105463027954,0.9992022514343262,0.9988390803337097,0.9993034601211548,0.9993748664855957,0.9987081289291382,0.9992022514343262,0.9989522099494934,0.9993451237678528,0.9992796182632446,0.9990713000297546,0.9991010427474976,0.9988807439804077,0.9993927478790283,0.9995951652526855,0.9990295767784119,0.9990236759185791,0.9992677569389343,0.9989462494850159,0.9994463324546814,0.999011754989624,0.9993451237678528,0.998999834060669,0.9991486668586731,0.9993867874145508,0.9993391633033752,0.9989045858383179,0.9991962909698486,0.9994284510612488,0.999178409576416,0.999053418636322,0.999136745929718,0.9990057945251465,0.99909508228302,0.9992201328277588,0.9994463324546814,0.9992141723632812,0.9992558360099792,0.998958170413971,0.999136745929718,0.9993034601211548,0.9992737174034119,0.9992855787277222,0.999178409576416,0.9993867874145508,0.9991546273231506,0.9991129636764526,0.9992974996566772,0.9991427063941956,0.9992677569389343,0.9993034601211548,0.9992915391921997,0.999053418636322,0.9991189241409302,0.999464213848114,0.9991725087165833,0.999011754989624],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"variable=Validation_accuracy<br>Number of epochs=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"Validation_accuracy\",\"line\":{\"color\":\"rgb(217,95,2)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Validation_accuracy\",\"showlegend\":true,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350],\"xaxis\":\"x\",\"y\":[0.8521693348884583,0.8984140753746033,0.9279897212982178,0.9375863075256348,0.9448492527008057,0.9463495016098022,0.9536124467849731,0.9325141906738281,0.9556841254234314,0.9616373777389526,0.960875391960144,0.9607563018798828,0.9611611366271973,0.96363765001297,0.9627565741539001,0.9652331471443176,0.9642568230628967,0.9673524498939514,0.9617326259613037,0.9661380052566528,0.9658761024475098,0.9673762917518616,0.9638996124267578,0.9658522605895996,0.9647568464279175,0.967233419418335,0.9655188918113708,0.9675667881965637,0.9657570123672485,0.9671381711959839,0.969305157661438,0.9664475917816162,0.9647806882858276,0.9677335023880005,0.9667333364486694,0.9684717059135437,0.9670429229736328,0.967685878276825,0.9642568230628967,0.9687574505805969,0.9682573676109314,0.9631375670433044,0.9662094712257385,0.9691384434700012,0.9690908193588257,0.9661142230033875,0.9662808775901794,0.9684478640556335,0.967233419418335,0.9690670371055603,0.9705672264099121,0.9669476747512817,0.9693765640258789,0.9692099094390869,0.9664713740348816,0.9665189981460571,0.966876208782196,0.9707339406013489,0.9697575569152832,0.9668524265289307,0.96992427110672,0.9682812094688416,0.9701386094093323,0.968400239944458,0.9686383605003357,0.9696385264396667,0.97037672996521,0.9674477577209473,0.9701386094093323,0.9678049087524414,0.9674477577209473,0.9681859016418457,0.9700433611869812,0.9665904641151428,0.9691146612167358,0.9659951329231262,0.968852698802948,0.9679001569747925,0.9719483852386475,0.9695432782173157,0.9692099094390869,0.9712101817131042,0.9681144952774048,0.9665904641151428,0.9706624746322632,0.9666857123374939,0.9709005951881409,0.966423749923706,0.9681144952774048,0.9687574505805969,0.9705672264099121,0.9670667052268982,0.9684717059135437,0.9698290228843689,0.9680668711662292,0.9693289399147034,0.967947781085968,0.9707339406013489,0.9707577228546143,0.9709720611572266,0.9713530540466309,0.9709005951881409,0.9692575335502625,0.9678049087524414,0.9696623086929321,0.9680668711662292,0.9671143293380737,0.9710673093795776,0.9701623916625977,0.9706624746322632,0.9699004888534546,0.9688764810562134,0.9698290228843689,0.9686859846115112,0.9689003229141235,0.9675667881965637,0.9714483022689819,0.9705672264099121,0.9676144123077393,0.9694004058837891,0.9695194363594055,0.9699481129646301,0.9704243540763855,0.9706624746322632,0.9718769192695618,0.9701147675514221,0.9709005951881409,0.9688288569450378,0.9687812328338623,0.9669476747512817,0.9692336916923523,0.9696385264396667,0.9690194129943848,0.9687098264694214,0.9704243540763855,0.9695432782173157,0.9684478640556335,0.9701147675514221,0.9710673093795776,0.9704243540763855,0.9697099328041077,0.970471978187561,0.9706148505210876,0.9710673093795776,0.9715673923492432,0.9712815880775452,0.9700671434402466,0.967328667640686,0.9718769192695618,0.9712815880775452,0.9700909852981567,0.9669714570045471,0.9699481129646301,0.9729961156845093,0.9700433611869812,0.9707577228546143,0.970924437046051,0.9702100157737732,0.9704481363296509,0.9708291888237,0.9719007611274719,0.970924437046051,0.9709958434104919,0.9678763747215271,0.9716626405715942,0.9716387987136841,0.9697575569152832,0.9708768129348755,0.9706386923789978,0.9718292951583862,0.9683288335800171,0.9697099328041077,0.9709720611572266,0.9709958434104919,0.9704481363296509,0.9699481129646301,0.9698051810264587,0.9717340469360352,0.9723055958747864,0.9701623916625977,0.97037672996521,0.9708768129348755,0.9694956541061401,0.9661856293678284,0.9702814817428589,0.9706148505210876,0.9702338576316833,0.9707577228546143,0.9716864228248596,0.9717102646827698,0.9711387157440186,0.9710434675216675,0.9716626405715942,0.9708529710769653,0.9713768362998962,0.9702100157737732,0.97037672996521,0.9715673923492432,0.9710196852684021,0.9709005951881409,0.9707100987434387,0.9713768362998962,0.9696385264396667,0.9672810435295105,0.9704243540763855,0.9713530540466309,0.9686622023582458,0.9718769192695618,0.968852698802948,0.9715197682380676,0.9707100987434387,0.9731390476226807,0.9721388816833496,0.9716150164604187,0.9718531370162964,0.972448468208313,0.9693765640258789,0.9722579121589661,0.9715911746025085,0.9697575569152832,0.9725674986839294,0.9706624746322632,0.9685431122779846,0.9678049087524414,0.970543384552002,0.9704957604408264,0.9693527817726135,0.9704005122184753,0.9728056192398071,0.971543550491333,0.9698051810264587,0.9701147675514221,0.9689955711364746,0.9704005122184753,0.9708529710769653,0.970471978187561,0.96947181224823,0.970471978187561,0.9719721674919128,0.9701386094093323,0.9708768129348755,0.9699718952178955,0.9712339639663696,0.9710673093795776,0.971543550491333,0.96947181224823,0.9704957604408264,0.9712339639663696,0.9713530540466309,0.9690908193588257,0.9705910086631775,0.9714959263801575,0.9704005122184753,0.9699481129646301,0.9694480299949646,0.9700671434402466,0.9727818369865417,0.96992427110672,0.9710196852684021,0.96992427110672,0.9733295440673828,0.9716864228248596,0.9698766469955444,0.9693527817726135,0.9706624746322632,0.9706624746322632,0.9709720611572266,0.9691146612167358,0.9691146612167358,0.9711149334907532,0.9713054299354553,0.9698290228843689,0.9701623916625977,0.9718531370162964,0.9703291058540344,0.9704005122184753,0.9708053469657898,0.9707339406013489,0.9712339639663696,0.9717816710472107,0.9694956541061401,0.9713054299354553,0.969567060470581,0.9705196022987366,0.9720674157142639,0.9715197682380676,0.9713530540466309,0.970924437046051,0.9727818369865417,0.9705196022987366,0.9706863164901733,0.9698766469955444,0.9715197682380676,0.9714721441268921,0.9722579121589661,0.9712339639663696,0.9699718952178955,0.9712578058242798,0.9703291058540344,0.9710673093795776,0.9704957604408264,0.9707577228546143,0.9714959263801575,0.9708291888237,0.9725674986839294,0.9701623916625977,0.9715911746025085,0.9710196852684021,0.9702576398849487,0.9691384434700012,0.9693527817726135,0.9710196852684021,0.9727342128753662,0.9708529710769653,0.96947181224823,0.9708053469657898,0.9721150398254395,0.9721388816833496,0.9717816710472107,0.9715197682380676,0.9709720611572266,0.9710434675216675,0.9719007611274719,0.9719483852386475,0.9718055129051208,0.9700433611869812,0.9714483022689819,0.9732342958450317,0.9726627469062805,0.971091091632843,0.9702814817428589,0.9705196022987366,0.9720197916030884,0.971091091632843,0.9708529710769653,0.9708291888237,0.9715197682380676,0.9695194363594055,0.9707815647125244,0.9713292121887207,0.9711625576019287,0.9723770022392273,0.9728294610977173,0.9711387157440186,0.9704005122184753,0.9712815880775452,0.9718055129051208,0.9724246263504028,0.9712101817131042,0.9703291058540344],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"variable=Train_Loss<br>Number of epochs=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"Train_Loss\",\"line\":{\"color\":\"rgb(117,112,179)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Train_Loss\",\"showlegend\":true,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350],\"xaxis\":\"x\",\"y\":[1.0671377182006836,0.35556161403656006,0.24008022248744965,0.18367448449134827,0.15118862688541412,0.12604078650474548,0.10658752918243408,0.09168708324432373,0.08039885014295578,0.06738880276679993,0.060441695153713226,0.052971575409173965,0.04639797657728195,0.04397723078727722,0.04103687033057213,0.033327922224998474,0.03350486233830452,0.030540816485881805,0.02732638455927372,0.026610083878040314,0.022755246609449387,0.020819561555981636,0.023160023614764214,0.019386226311326027,0.017345763742923737,0.015980565920472145,0.020264439284801483,0.014953128062188625,0.015391901135444641,0.016619790345430374,0.015297580510377884,0.011745662428438663,0.01329624280333519,0.013769930228590965,0.012188971042633057,0.013509741052985191,0.014179100282490253,0.00876785907894373,0.011798065155744553,0.011049536056816578,0.010945804417133331,0.01117608230561018,0.010436086915433407,0.010807397775352001,0.012666722759604454,0.007041227072477341,0.009637420997023582,0.009456214495003223,0.01003703661262989,0.01252672541886568,0.007258916273713112,0.00799929816275835,0.010889996774494648,0.00923998560756445,0.0060362303629517555,0.012047925032675266,0.010258547961711884,0.007718080189079046,0.006247462704777718,0.010186838917434216,0.004474691115319729,0.010179844684898853,0.008489619009196758,0.005499016493558884,0.008729877881705761,0.007263061590492725,0.009656057693064213,0.005591069348156452,0.007094990462064743,0.008406645618379116,0.005756743252277374,0.007259114645421505,0.008737239055335522,0.006856030784547329,0.0068725463934242725,0.00526631623506546,0.0095510920509696,0.006773228757083416,0.006245629861950874,0.006835344713181257,0.008593238890171051,0.0055981893092393875,0.008472634479403496,0.005428703967481852,0.005105908960103989,0.005344619043171406,0.007668131496757269,0.004676848649978638,0.009241885505616665,0.005957712419331074,0.004629481118172407,0.005559595301747322,0.006083974614739418,0.005540275946259499,0.007148951757699251,0.004777946043759584,0.006901229731738567,0.004723221994936466,0.005935093387961388,0.007729147095233202,0.004294558893889189,0.006733432412147522,0.004770778585225344,0.005590763874351978,0.007344243582338095,0.00411944929510355,0.005119717679917812,0.00807928666472435,0.004271112848073244,0.00623764842748642,0.0032707422506064177,0.0060122390277683735,0.007192166056483984,0.005398552864789963,0.005664854776114225,0.006416989024728537,0.006483676843345165,0.003358045592904091,0.006299794651567936,0.007031677756458521,0.004845363087952137,0.006005564238876104,0.004265638068318367,0.004276508465409279,0.004484905861318111,0.005650483537465334,0.006301766261458397,0.004209612030535936,0.005006873980164528,0.004780260380357504,0.005759113002568483,0.006044034380465746,0.005808802787214518,0.005474322475492954,0.004819334018975496,0.004474113695323467,0.004831819795072079,0.007536521181464195,0.005031634122133255,0.003324375720694661,0.005898589733988047,0.005280236713588238,0.005858652293682098,0.004491038620471954,0.0032728640362620354,0.007720554247498512,0.005364811513572931,0.004673615097999573,0.005116682965308428,0.005119966808706522,0.0052728247828781605,0.005254492163658142,0.003876748029142618,0.003770701354369521,0.0066599235869944096,0.004162224475294352,0.00515957223251462,0.0046249073930084705,0.0047807455994188786,0.005101922899484634,0.004356109071522951,0.0051734559237957,0.0051793730817735195,0.006598995067179203,0.005330405198037624,0.0024810819886624813,0.0031678795348852873,0.005862026941031218,0.004592664074152708,0.004565074574202299,0.005648889113217592,0.004707084037363529,0.004787773825228214,0.007826360873878002,0.003867578227072954,0.0037821605801582336,0.004686451051384211,0.005759515333920717,0.0049440511502325535,0.006415048148483038,0.0064866854809224606,0.005194746423512697,0.003890763036906719,0.0058094109408557415,0.0049822162836790085,0.0044663334265351295,0.004407018423080444,0.00477788457646966,0.006921662483364344,0.004570205695927143,0.0038012044969946146,0.003980409353971481,0.003961460664868355,0.005383911542594433,0.00624189805239439,0.0036673014983534813,0.004190059844404459,0.006588894873857498,0.005120693240314722,0.0038630180060863495,0.004652072209864855,0.0030178201850503683,0.005562031641602516,0.006328921765089035,0.004411492962390184,0.003233624156564474,0.0064838300459086895,0.004294584039598703,0.0027375754434615374,0.006270974408835173,0.0038234645035117865,0.006282994523644447,0.006070888135582209,0.0030557094141840935,0.004697821103036404,0.004260302986949682,0.004378709942102432,0.005986461415886879,0.006071447394788265,0.005520337726920843,0.005655973684042692,0.003878505900502205,0.0027990504167973995,0.006663254927843809,0.007308890577405691,0.004628404974937439,0.004260056186467409,0.005941114854067564,0.0022330323699861765,0.0032726682256907225,0.00614564074203372,0.0066245440393686295,0.005188729614019394,0.00344928284175694,0.005584748927503824,0.0034061730839312077,0.005342490505427122,0.005172227509319782,0.004027389921247959,0.004487216006964445,0.006281026639044285,0.004580376669764519,0.00653832545503974,0.0030068999622017145,0.004983088001608849,0.005673196632415056,0.00239650160074234,0.0058099739253520966,0.004178480710834265,0.006106214597821236,0.004676295910030603,0.004201152361929417,0.004684035666286945,0.006140052806586027,0.003985181916505098,0.0063839019276201725,0.0027873485814779997,0.0037146126851439476,0.007132301572710276,0.00652326550334692,0.002860055770725012,0.003995249513536692,0.004480031318962574,0.007856342010200024,0.00403958186507225,0.004060789942741394,0.005190819501876831,0.0042224060744047165,0.006250813137739897,0.005628833081573248,0.003811625065281987,0.00302040483802557,0.004431750159710646,0.005380932241678238,0.003853279398754239,0.004290126729756594,0.0062842112965881824,0.006293250247836113,0.004155525006353855,0.0037253559567034245,0.0055013191886246204,0.004003684502094984,0.005341031588613987,0.006753493100404739,0.004011837765574455,0.004300152417272329,0.0041721840389072895,0.005276009440422058,0.00413924315944314,0.006064111366868019,0.004017762374132872,0.005187970586121082,0.0060613276436924934,0.006706829648464918,0.004055629950016737,0.006752979010343552,0.0041229915805161,0.0029060356318950653,0.007908274419605732,0.004769995808601379,0.0058101932518184185,0.0032851623836904764,0.0038781415205448866,0.00543703930452466,0.004722707439213991,0.006997270975261927,0.0033440222032368183,0.002583090215921402,0.006152131129056215,0.005617317743599415,0.004198848735541105,0.005910918582230806,0.0033733961172401905,0.0057919081300497055,0.003058012342080474,0.006128399632871151,0.005071834661066532,0.0032677343115210533,0.004008473828434944,0.00686572166159749,0.004881640430539846,0.0033580127637833357,0.004923617467284203,0.005789186339825392,0.004611408803611994,0.006637253798544407,0.005048207473009825,0.0050255311653018,0.003976189997047186,0.005339119583368301,0.004952318035066128,0.00705514382570982,0.006734451279044151,0.004483940079808235,0.0047752391546964645,0.004423577804118395,0.00486381258815527,0.0035767480731010437,0.005299585871398449,0.0065924967639148235,0.004440303426235914,0.006070799194276333,0.005017742980271578,0.004756357055157423,0.005192476790398359,0.0059748305939137936,0.00569967832416296,0.0029106945730745792,0.005727489944547415,0.006450841668993235],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"variable=Validation_loss<br>Number of epochs=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"Validation_loss\",\"line\":{\"color\":\"rgb(231,41,138)\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Validation_loss\",\"showlegend\":true,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350],\"xaxis\":\"x\",\"y\":[0.45415613055229187,0.3107820153236389,0.2242020219564438,0.19863447546958923,0.17548397183418274,0.16987857222557068,0.15319491922855377,0.20410828292369843,0.14370937645435333,0.12902328372001648,0.13498063385486603,0.14348256587982178,0.13932541012763977,0.13933470845222473,0.13328103721141815,0.13291575014591217,0.14023932814598083,0.13619829714298248,0.1686028689146042,0.14273764193058014,0.14820696413516998,0.15390780568122864,0.1643066257238388,0.16046683490276337,0.16552171111106873,0.16728277504444122,0.1668412834405899,0.16941402852535248,0.1661013513803482,0.18017743527889252,0.16226698458194733,0.18629935383796692,0.18938922882080078,0.17569275200366974,0.19094105064868927,0.18669819831848145,0.19808563590049744,0.18768756091594696,0.21141424775123596,0.18855245411396027,0.17886056005954742,0.21011121571063995,0.22484764456748962,0.19393835961818695,0.1831706166267395,0.21796748042106628,0.22706255316734314,0.20602290332317352,0.21792514622211456,0.20463314652442932,0.20008890330791473,0.21739868819713593,0.20899434387683868,0.19671280682086945,0.23506812751293182,0.23658592998981476,0.22735413908958435,0.20163311064243317,0.22071121633052826,0.23323048651218414,0.221216082572937,0.2262614220380783,0.22861357033252716,0.2243768572807312,0.22682379186153412,0.22850051522254944,0.22064703702926636,0.23458456993103027,0.22234338521957397,0.2403206080198288,0.24513594806194305,0.24972094595432281,0.2335033118724823,0.25028595328330994,0.2336062490940094,0.2741546630859375,0.2551177740097046,0.24636739492416382,0.23276986181735992,0.24419480562210083,0.23559215664863586,0.2390652447938919,0.25738587975502014,0.25966936349868774,0.24159318208694458,0.29164543747901917,0.2344440221786499,0.27579349279403687,0.26009857654571533,0.25350213050842285,0.26145079731941223,0.276176393032074,0.26315537095069885,0.26103174686431885,0.262289822101593,0.269749253988266,0.26667550206184387,0.2631676197052002,0.2907891869544983,0.274463027715683,0.2850338816642761,0.26926976442337036,0.29988011717796326,0.29687684774398804,0.28196972608566284,0.29556742310523987,0.29875028133392334,0.26958656311035156,0.27200159430503845,0.28142303228378296,0.2933688163757324,0.30494558811187744,0.2808411717414856,0.2879662811756134,0.30320340394973755,0.2997162640094757,0.2786964476108551,0.29477909207344055,0.30853521823883057,0.32163792848587036,0.3114704489707947,0.29599419236183167,0.3039571940898895,0.3058554530143738,0.28826838731765747,0.30867084860801697,0.3006287217140198,0.33376312255859375,0.33191829919815063,0.3374978303909302,0.33687472343444824,0.33479437232017517,0.315687894821167,0.3279215395450592,0.30792751908302307,0.34092846512794495,0.33703288435935974,0.3400919735431671,0.3273964524269104,0.3308904469013214,0.32402220368385315,0.33034706115722656,0.34102049469947815,0.318613201379776,0.32058289647102356,0.3183128833770752,0.3531319797039032,0.36219218373298645,0.3288435637950897,0.3237610161304474,0.34093132615089417,0.35227540135383606,0.346225380897522,0.3244461715221405,0.35900115966796875,0.3519977033138275,0.36389631032943726,0.37631627917289734,0.36311274766921997,0.37152907252311707,0.34914302825927734,0.3493504226207733,0.3510110080242157,0.37394434213638306,0.35386911034584045,0.34841248393058777,0.36235564947128296,0.3610692024230957,0.3674226403236389,0.3526404798030853,0.38394567370414734,0.3590051829814911,0.38885536789894104,0.3683984875679016,0.3710792064666748,0.4076022803783417,0.387743204832077,0.3780869245529175,0.3934197723865509,0.41457048058509827,0.415984570980072,0.38074156641960144,0.4111860692501068,0.48876723647117615,0.36466875672340393,0.4002029597759247,0.4332674741744995,0.4195763170719147,0.4115886390209198,0.3997590243816376,0.40647730231285095,0.4292967617511749,0.40439891815185547,0.4154713749885559,0.4019174873828888,0.4153042137622833,0.4281255304813385,0.40838438272476196,0.4072960615158081,0.42358827590942383,0.4192677140235901,0.42978647351264954,0.46235761046409607,0.4330865144729614,0.43877583742141724,0.43810582160949707,0.45483091473579407,0.40807637572288513,0.47138890624046326,0.4221870005130768,0.44340816140174866,0.41709116101264954,0.4130842685699463,0.4400578439235687,0.4338705241680145,0.42945680022239685,0.4926474690437317,0.4273412227630615,0.4588601887226105,0.4704953134059906,0.422938734292984,0.4506272077560425,0.5183265209197998,0.5524171590805054,0.4660664498806,0.47159233689308167,0.5462313890457153,0.5091125965118408,0.4643009901046753,0.4847205579280853,0.5195707678794861,0.5065581798553467,0.506415843963623,0.4810557961463928,0.4911152124404907,0.5114220380783081,0.5307915806770325,0.5235540866851807,0.48477694392204285,0.5072333812713623,0.49268725514411926,0.5138652920722961,0.5215329527854919,0.5177730321884155,0.49361926317214966,0.5239768028259277,0.5210076570510864,0.5205739736557007,0.46962377429008484,0.544289231300354,0.4954715073108673,0.5487499237060547,0.5430313944816589,0.5278697609901428,0.5533729791641235,0.5382950305938721,0.5083210468292236,0.5624168515205383,0.5233059525489807,0.5538139939308167,0.5248603820800781,0.493770033121109,0.5880150198936462,0.525016188621521,0.5402697324752808,0.5660337805747986,0.5614100694656372,0.5443535447120667,0.5688987970352173,0.5471660494804382,0.5175007581710815,0.547508716583252,0.5629101395606995,0.551572859287262,0.5794103145599365,0.5760655403137207,0.5971173048019409,0.5862303376197815,0.5520056486129761,0.5288507342338562,0.6147815585136414,0.5610195398330688,0.6060720086097717,0.5805911421775818,0.5590052008628845,0.5963531732559204,0.572360098361969,0.5664945244789124,0.5798828601837158,0.5995572209358215,0.5974605679512024,0.6163291931152344,0.5976507663726807,0.607831597328186,0.5963449478149414,0.6042841672897339,0.6234893798828125,0.6025983691215515,0.5977611541748047,0.6321538090705872,0.637570858001709,0.6492111682891846,0.6668330430984497,0.6373265981674194,0.597171425819397,0.6451994776725769,0.6095765829086304,0.622708261013031,0.6098586320877075,0.695257842540741,0.6623875498771667,0.5991249084472656,0.5943732261657715,0.624214231967926,0.6577836275100708,0.6626309156417847,0.6478617787361145,0.6246833205223083,0.6637529730796814,0.6710641980171204,0.6633667349815369,0.6983998417854309,0.6540340781211853,0.6405895948410034,0.7032588720321655,0.7007136344909668,0.675330400466919,0.6706400513648987,0.6671078205108643,0.6919044852256775,0.6784926652908325,0.7212591171264648,0.6887713670730591,0.7040436863899231,0.7569541335105896,0.7564287185668945,0.7311301827430725,0.7757203578948975,0.7379021644592285,0.7392429113388062,0.7508738040924072,0.6678141355514526,0.7036720514297485,0.7159402370452881,0.7551683783531189,0.7103624939918518,0.7716940641403198,0.7239065766334534,0.7496777176856995,0.7781548500061035],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Number of epochs\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Loss and Accuracy plots\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('b3e3846f-cc99-4394-9437-a3326067b53b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.line(history_details_df,x='Number of epochs',y=history_details_df.columns[0:],color_discrete_sequence=px.colors.qualitative.Dark2,title='Loss and Accuracy plots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:34:58.05405Z",
     "iopub.status.busy": "2023-04-02T05:34:58.053675Z",
     "iopub.status.idle": "2023-04-02T05:35:10.465465Z",
     "shell.execute_reply": "2023-04-02T05:35:10.464005Z",
     "shell.execute_reply.started": "2023-04-02T05:34:58.054017Z"
    },
    "id": "Y28aQ0fgMpoN",
    "outputId": "c298d9b5-604e-426c-fdb3-17031b454ab4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093/1093 [==============================] - 2s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"Test.csv\")\n",
    "\n",
    "#reshaping the data to (32x32)\n",
    "df_test = df_test.values.reshape(-1,28,28,1)\n",
    "df_test = np.pad(df_test,((0,0),(2,2),(2,2),(0,0)),'constant')\n",
    "\n",
    "# predect the labels of the test dataset, and return the exact number not an array (decode it)\n",
    "y_pred = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:35:10.467676Z",
     "iopub.status.busy": "2023-04-02T05:35:10.467322Z",
     "iopub.status.idle": "2023-04-02T05:35:10.47488Z",
     "shell.execute_reply": "2023-04-02T05:35:10.473559Z",
     "shell.execute_reply.started": "2023-04-02T05:35:10.467643Z"
    },
    "id": "OGfLwnkSMpoN",
    "outputId": "c2e8ade7-7557-4930-c369-97288825f22d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0 ... 15 15 34]\n"
     ]
    }
   ],
   "source": [
    "results = np.argmax(y_pred,axis = 1)\n",
    "print(results)\n",
    "results = pd.Series(results,name=\"Label\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-02T05:35:10.477791Z",
     "iopub.status.busy": "2023-04-02T05:35:10.477413Z",
     "iopub.status.idle": "2023-04-02T05:35:10.542225Z",
     "shell.execute_reply": "2023-04-02T05:35:10.540774Z",
     "shell.execute_reply.started": "2023-04-02T05:35:10.477757Z"
    },
    "id": "aLRah1m7MpoN",
    "outputId": "328113ab-0ce1-4355-df77-a17705c48c8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "# Generate a Submission File \n",
    "submission = pd.DataFrame({'ImageId':pd.Series(list(range(1, len(results)+1))),\n",
    "                           'Label':pd.Series(results)})\n",
    "# save the df\n",
    "submission.to_csv(\"submission.csv\", index = False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEhk6_NpMpoN"
   },
   "source": [
    "This notebook is intended to explain the CNN model parameters with LeNet model and why the CNN model instead of MLP----simple answer is --> the concept is driven from the biological inspiration of visual cortex, which is responsible for the image recognition in the brain and also trying to get the best score by tweaking the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": true,
    "id": "4-gg_XiSMpoO"
   },
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
